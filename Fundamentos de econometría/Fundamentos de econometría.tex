% !TEX program = lualatex

\documentclass[8pt,a4paper]{extarticle}
\input{../template_es.tex}
\input{../macros.tex}

\hfuzz=77pt

% Class info
\renewcommand{\csClass}{Fundamentos de econometría}
\renewcommand{\csClassCode}{EST - 21104}
\renewcommand{\csTerm}{Primavera 2021}
\renewcommand{\csKeywords}{ }

% PDF Metadata
\hypersetup{
    pdftitle={\csof \csClass},
    pdfsubject={\csClass},
    pdfauthor={\csAuthorName},
    pdfkeywords={}
}

% Begin document
\begin{document}

\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		\Huge
		\textbf{\csClass}
		\vspace{0.5cm} \\
		\Large
		\cs\ $\cdot$ \csTerm
		\vfill
		\csAuthorName
		\vspace{0.8cm}
		\csClassCode\\
		\csSchool
	\end{center}
\end{titlepage}

\begin{multicols}{3}
	\setcounter{page}{1}

	\section{Fundamentos estadísticos de econometría}

	\begin{boxdef}[Error cuadrático medio]
		Definimos el \textbf{error cuadrático medio} como sigue:
		\[
			\text{\normalfont ECM} (\hat{\theta}) = \mathbb{E}_{\theta} \left[ \left( \hat{\theta} - \theta \right)^2  \right]  = \text{\normalfont Var} (\hat{\theta}) + \text{\normalfont Bias}^{2} (\hat{\theta}, \theta)
			.\]
		Este mide la diferencia en media cuadrada entre nuestros valores estimados y el real. Asimismo, podemos definir el \textbf{error cuadrático medio} de una variable $X$ respecto a alguna constante $c$ como sigue:
		\[
			\mathbb{E} \left[ (X - c)^2 \right] = \sigma^2_{\mathrm{x}} + (c - \mu_\mathrm{x})^2
			.\]
	\end{boxdef}

	\begin{boxtheo}[Error cuadrático mínimo]
		El valor que minimiza $\mathbb{E}\left[(X-c)^2\right]$ es $\mu_{\mathrm{x}}$.
	\end{boxtheo}

	\begin{boxcor}[Desigualdad de Markov I]
		Sean $\varphi$ una función monótona creciente y no negativa para los reales no negativos, $X$ una variable aleatoria, $a \ge 0$ y $\varphi(a) > 0$, entonces:
		\[
			\mathbb{P}\left( \left| X \right| \ge a \right) \le \frac{\mathbb{E}\left[ \varphi \left( \left| X \right|  \right)  \right] }{\varphi(a)}
			.\]
	\end{boxcor}

	\begin{boxcor}[Desigualdad de Markov II]
		Sea $X$ una variable aleatoria y $a > 0$, entonces:
		\[
			\mathbb{P}\left( \left| X \right| \ge a  \right) \le \frac{\mathbb{E}\left[ \left| X \right|^r  \right] }{a^r}
			.\]
	\end{boxcor}

	\begin{boxtheo}[Desigualdad de Chebyshev]
		Sean $X$ una variable aleatoria con $\mathbb{E}(X) = \mu$ y $\text{\normalfont Var}(X) = \sigma^2$, y $k > 0$, entonces:
		\[
			\mathbb{P}\left( \|X - \mu\|_{\alpha} \ge k\sigma \right) \le \frac{1}{k^2}
			,\]
		o bien,
		\[
			\mathbb{P}\left( \|X - \mu\|_{\alpha} \ge k \right) \le \frac{\sigma^2}{k^2}
			;\]
		donde $\| \cdot \|_{\alpha}$ es la norma $\alpha$.
	\end{boxtheo}

	\begin{boxrmk}[]
		Sean $X_1, X_2, \ldots, X_n$ una colección de $n\in\mathbb{N}$ variables aleatorias, podemos expresar las funciones de densidad conjunta en términos de densidades condicionales de la siguiente manera:
		\begin{align*}
			f(x_1, x_2, \ldots, x_n) & = f(x_1  \mid x_2, x_3, \ldots, x_n)\cdot  f(x_2, x_3, \ldots, x_n)                                                         \\
			                         & =  f(x_1  \mid x_2, \ldots, x_n) \cdot f(x_2  \mid x_3, \ldots, x_n) \cdot \cdots \cdot f(x_{n-1}  \mid x_n) \cdot f(x_{n})
			.\end{align*}
	\end{boxrmk}

	\begin{boxtheo}[Ley de esperanzas iteradas]
		Sean $X$ y $Y$ dos variables aleatorias tales que $\mathbb{E}\left[ X \right] $ está definida y ambas están en el mismo espacio de probabilidad, entonces:
		\[
			\mathbb{E}\left[ X \right] = \mathbb{E}\left[ \mathbb{E}\left[ X  \mid Y \right]  \right]
			.\]
	\end{boxtheo}

	\begin{boxprop}[]
		Por la \textbf{ley de esperanzas iteradas}, tenemos:
		\[
			\text{\normalfont Var}(X) = \mathbb{E}\left[ \text{\normalfont Var}(X  \mid Y) \right] + \text{\normalfont Var} \left( \mathbb{E}[Y  \mid X] \right)
			.\]
	\end{boxprop}

	\begin{boxprop}[]
		\[
			\mathbb{E}[XY] = \mathbb{E}\left[ X \mathbb{E}[Y  \mid  X] \right]
			.\]
	\end{boxprop}

	\begin{boxprop}[]
		Sea $\varepsilon = Y - \mathbb{E}[Y \mid X]$, entonces:
		\begin{eqlist}
			\item $\mathbb{E}[\varepsilon  \mid X] = 0$.
			\item $\text{\normalfont Var}(\varepsilon  \mid X) = \sigma^2_{\mathrm{y \mid x}}$.
			\item $\mathbb{E}[\varepsilon] = 0$.
			\item $\text{\normalfont Var}(\varepsilon) = 0$.
			\item $\text{\normalfont Cov}(h(X), \varepsilon) = 0$.
		\end{eqlist}
	\end{boxprop}

	\begin{boxprop}[]
		\[
			\argmin_{h(X)} \mathbb{E}\left[ \left( Y - h(X) \right)^2  \right] = \mathbb{E}\left[ Y  \mid X \right]
			.\]
	\end{boxprop}

	\begin{boxdef}[Mejor predictor lineal]
		Dado que $\mathbb{E}[Y  \mid X]$ puede ser una función no lineal bastante complicada --- excepto en el caso normal bivariado, consideramos $\mathbb{E}^* [Y  \mid X]$ el \textbf{mejor predictor lineal} (BLP, por sus siglas en inglés) y lo definimos como sigue:
		\[
			\mathbb{E}^*\left[ Y  \mid X \right] = \alpha + \beta X
			,\]
		tal que
		\[
			\alpha = \mu_{\mathrm{y}} - \beta \mu_{\mathrm{x}} \qquad \beta = \frac{\sigma_{\mathrm{xy}}}{\sigma^2_{x}}
			.\]
	\end{boxdef}

	\begin{boxtheo}[]
		Si la esperanza condicional es lineal, esta coincide con el mejor predictor lineal.
	\end{boxtheo}

	\sectionbreak

	\begin{boxdef}[Independencia en distribución]
		Decimos que, en un conjunto finito de $n$ variables aleatorias $\left\{ X_1, \ldots, X_n \right\}$, $X_1, \ldots, X_n$ son \textbf{mutuamente independientes} si:
		\[
			F_{\mathrm{x_1,\ldots,x_n}} (x_1, \ldots, x_n) = \prod_{k=1}^{n} F_{\mathrm{x}_k} (x_k), \quad \forall x_1, \ldots, x_n
			.\]
	\end{boxdef}

	\begin{boxdef}[Independencia en media]
		Decimos que una variable aleatoria $Y$ es \textbf{independiente en media} respecto a otra $X$ si:
		\[
			\mathbb{E}[Y  \mid X] = \mathbb{E}[Y]
			.\]
	\end{boxdef}

	\begin{boxdef}[Independencia en covarianza]
		Decimos que dos variables aleatorias $X$ y $Y$ son \textbf{independientes en covarianza} si
		\[
			\text{\normalfont Cov}(X,Y) = 0
			.\]
	\end{boxdef}

	\begin{boxtheo}[]
		Independencia en distribución implica independencia en medias, y esta, a su vez, implica independencia en covarianzas; pero no en el orden inverso.
	\end{boxtheo}

	\begin{boxprop}[]
		Si $Y$ es independiente en media respecto a $X$, entonces:
		\[
			\mathbb{E}[X^r Y] = \mathbb{E}[X^r] \mathbb{E}[Y], \quad \forall r\in \mathbb{R}
			.\]
	\end{boxprop}

	\begin{boxprop}[]
		Si $Y$ es independiente en distribución respecto a $X$, entonces:
		\[
			\mathbb{E}[h(X) Y^s] = \mathbb{E}[h(X)] \mathbb{E}[Y^s]
		\]
		y, por lo tanto, también se cumple
		\[
			\mathbb{E}[X^r Y^s] = \mathbb{E}[X^r] \mathbb{E}[Y^s]
		\]
		para cualquier función $h(X)$ y todo $r$ y $s$.
	\end{boxprop}

	\begin{boxprop}
		Sean $\varepsilon = Y - \mathbb{E} [Y \mid X]$ y $U = Y - \mathbb{E}^* [Y \mid X]$, entonces $\varepsilon$ es independiente en media respecto a $X$ y $U$ no está correlacionado con $X$.
	\end{boxprop}

	\newpage

	\begin{boxtheo}[Desigualdad Cauchy-Schwartz]
		Sea $\displaystyle \rho = \frac{\Cov(X,Y)}{\sqrt{\Var(X)} \sqrt{\Var(Y)}}$, entonces $0 \leq \rho^{2} \leq 1$.
	\end{boxtheo}

	\begin{boxprop}
		Sea $X \sim \mathcal{N} (\mu,\sigma^{2})$, entonces, para toda $p$ entera no negativa, los momentos centrados son los siguientes:
		\[\EE [(X - \mu)^{p}] =
		\begin{cases}
			0, & \text{si $p$ es impar}; \\
			\sigma^{p}(p-1)!!, & \text{si $p$ es par.}
		\end{cases} \]
	\end{boxprop}

	\begin{boxprop}
		En el caso normal, la ausencia de correlación equivale a independencia estocástica.
	\end{boxprop}

	\begin{boxprop}
		Sea $g(Y \mid X) \sim \mathcal{N}(\EE[Y \mid X], \Var(Y \mid X))$, entonces:
		\[\EE [Y \mid X] = \mu_{\mathrm{y}} + \rho \frac{\sigma_{\mathrm{y}}}{\sigma_{\mathrm{x}}} (X - \mu_{\mathrm{x}}),\]
		y
		\[\Var(Y \mid X) = \sigma_{\mathrm{y}}^{2} (1 - \rho)^2.\]
		Nótese que la esperanza condicional es lineal y la varianza es constante.
	\end{boxprop}

	\begin{boxrmk}
		En el caso normal, la función de regresión coincide con el mejor predictor lineal tal que, si $Y = \EE[ Y \mid X ] + \varepsilon$ y $\EE[ \varepsilon \mid X ] = 0$, $Y = \alpha + \beta X + \varepsilon$ con $\alpha = \mu_{\mathrm{y}} - \beta \mu_{\mathrm{x}}$ y $\displaystyle \beta = \frac{\sigma_{\mathrm{xy}}}{\sigma^2_{\mathrm{x}}}$.
	\end{boxrmk}

	\begin{boxprop}
		La distribución condicional de $Y$ dado $X$ es normal.
		\[Y \mid X \sim \mathcal{N} (\alpha + \beta X, \sigma^2).\]
	\end{boxprop}

	\newpage
	\section{Estimación}

	\begin{boxtheo}
		Sea $X$ una muestra aleatoria finita con $n$ observaciones, entonces, $x_1, x_2, \dots, x_n$ son independientes e idénticamente distribuídas y, por lo tanto, la densidad conjunta de dicha muestra aleatoria es:
		\[F_{\mathrm{x_1,\ldots,x_n}}(x_1,\ldots,x_n) = \prod_{k = 1}^{n} F_{\mathrm{x}_k} (x_k).\]
	\end{boxtheo}

	\begin{boxdef}[Estadístico muestral]
		Sea $T_n = h(X)$ una función escalar de una muestra aleatoria, entonces, este es un \textbf{estadístico muestral}.
	\end{boxdef}

	\begin{boxrmk}
		Todo estadístico muestral $T_n$ es una variable aleatoria porque su valor es determinado por el resultado de un experimento. Asimismo, a la distribución de probabilidad de $T_n$ se le conoce como \emph{distribución muestral}, y está completamente determinada por $h(\cdot)$, $f(\mathrm{x})$ y $n$.
	\end{boxrmk}

	\begin{boxprop}
		La media muestral satisface las siguientes propiedades:
		\begin{bulletlist}
			\item Si $X \sim \Ber(p)$, entonces, $n\bar{X} \sim \Bin(n,p)$.
			\item Si $X \sim \Norm(\mu, \sigma^2)$, entonces, $\displaystyle \bar{X} \sim \Norm\left(\mu,\frac{\sigma^2}{n}\right)$.
			\item Si $X \sim \Exp(\lambda)$, entonces, $k\lambda\bar{X} \sim \csq (k)$ con $k = 2n$.
		\end{bulletlist}
	\end{boxprop}

	\begin{boxtheo}[de la media muestral]
		Dada un muestra aleatoria de tamaño $n$ y cualquier población con $\EE [X] = \mu$ y $\Var(X) = \sigma^2$, la media muestral $\bar{X}$ tiene esperanza $\EE [\bar{X}] = \mu$ y varianza $\displaystyle \Var(\bar{X}) = \frac{\sigma^2}{n}$.
	\end{boxtheo}

	\begin{boxdef}[Momento muestral centrado]
		\[M_r = \frac{1}{n}\sum_{i = 1}^n \left( x_i - \bar{X} \right)^r.\]
	\end{boxdef}

	\begin{boxdef}[Momento muestral no centrado]
		\[M'_r = \frac{1}{n}\sum_{i = 1}^n x_i^r.\]
	\end{boxdef}

	\begin{boxdef}[Momento muestral centrado en la media poblacional]
		\[M^*_r = \frac{1}{n}\sum_{i = 1}^n \left( x_i - \mu \right)^r.\]
		Nótese que este \textbf{no} es un estimador muestral, pues, se requiere un parámetro poblacional. Sin embargo, les será útil para algunas demostraciones.
	\end{boxdef}

	\newpage
	\section{Teoría asintótica}

	\newpage
	\section{Inferencia en el modelo lineal}

	\vfill\eject
	\columnbreak
\end{multicols}
\end{document}
