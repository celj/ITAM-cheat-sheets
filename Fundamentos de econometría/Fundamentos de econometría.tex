% !TEX program = lualatex

\documentclass[8pt,a4paper]{extarticle}
\input{../template_es.tex}
\input{../macros.tex}

% Class info
\renewcommand{\csClass}{Fundamentos de econometría}
\renewcommand{\csClassCode}{EST - 21104}
\renewcommand{\csTerm}{Primavera 2021}
\renewcommand{\csKeywords}{ }

% PDF Metadata
\hypersetup{
    pdftitle={\csof \csClass},      
    pdfsubject={\csClass},      
    pdfauthor={\csAuthorName},  
    pdfkeywords={}              
}

% Begin document
\begin{document}

\begin{titlepage}
    \begin{center}
	\vspace*{1cm}
	\Huge
        \textbf{\csClass}
	\vspace{0.5cm} \\
	\Large
        \cs\ $\cdot$ \csTerm
        \vfill
        \csAuthorName
	\vspace{0.8cm}
        \csClassCode\\
        \csSchool     
    \end{center}
\end{titlepage}

\begin{multicols}{3}
\setcounter{page}{1}

\section{Fundamentos estadísticos de econometría}

\begin{boxdef}[Error cuadrático medio]
	Definimos el \textbf{error cuadrático medio} como sigue:
	\[
		\text{\normalfont ECM} (\hat{\theta}) = \mathbb{E}_{\theta} \left[ \left( \hat{\theta} - \theta \right)^2  \right]  = \text{\normalfont Var} (\hat{\theta}) + \text{\normalfont Bias}^{2} (\hat{\theta}, \theta)
	.\] 
	Este mide la diferencia en media cuadrada entre nuestros valores estimados y el real. Asimismo, podemos definir el \textbf{error cuadrático medio} de una variable $X$ respecto a alguna constante $c$ como sigue:
	\[
		\mathbb{E} \left[ (X - c)^2 \right] = \sigma^2_{\mathbf{x}} + (c - \mu_\mathbf{x})^2
	.\] 
\end{boxdef} 

\begin{boxtheo}[Error cuadrático mínimo]
	El valor que minimiza $\mathbb{E}\left[(X-c)^2\right]$ es $\mu_{\mathbf{x}}$.
\end{boxtheo}

\begin{boxcor}[Desigualdad de Markov I]
	Sean $\varphi$ una función monótona creciente y no negativa para los reales no negativos, $X$ una variable aleatoria, $a \ge 0$ y $\varphi(a) > 0$, entonces:
	\[
		\mathbb{P}\left( \left| X \right| \ge a \right) \le \frac{\mathbb{E}\left[ \varphi \left( \left| X \right|  \right)  \right] }{\varphi(a)}
	.\] 
\end{boxcor}

\begin{boxcor}[Desigualdad de Markov II]
	Sea $X$ una variable aleatoria y $a > 0$, entonces:
	\[
		\mathbb{P}\left( \left| X \right| \ge a  \right) \le \frac{\mathbb{E}\left[ \left| X \right|^r  \right] }{a^r}
	.\] 
\end{boxcor}

\begin{boxtheo}[Desigualdad de Chebyshev]
	Sean $X$ una variable aleatoria con $\mathbb{E}(X) = \mu$ y $\text{\normalfont Var}(X) = \sigma^2$, y $k > 0$, entonces:
	\[
		\mathbb{P}\left( \|X - \mu\|_{\alpha} \ge k\sigma \right) \le \frac{1}{k^2}
	,\] 
	o bien,
	\[
		\mathbb{P}\left( \|X - \mu\|_{\alpha} \ge k \right) \le \frac{\sigma^2}{k^2}
	;\] 
	donde $\| \cdot \|_{\alpha}$ es la norma $\alpha$.
\end{boxtheo}

\begin{boxrmk}[]
	Sean $X_1, X_2, \ldots, X_n$ una colección de $n\in\mathbb{N}$ variables aleatorias, podemos expresar las funciones de densidad conjunta en términos de densidades condicionales de la siguiente manera:
	\begin{align*}
		f(x_1, x_2, \ldots, x_n) &= f(x_1  \mid x_2, x_3, \ldots, x_n)\cdot  f(x_2, x_3, \ldots, x_n) \\
								 &=  f(x_1  \mid x_2, \ldots, x_n) \cdot f(x_2  \mid x_3, \ldots, x_n) \cdot \cdots \cdot f(x_{n-1}  \mid x_n) \cdot f(x_{n})
	.\end{align*}
\end{boxrmk}

\begin{boxtheo}[Ley de esperanzas iteradas]
	Sean $X$ y $Y$ dos variables aleatorias tales que $\mathbb{E}\left[ X \right] $ está definida y ambas están en el mismo espacio de probabilidad, entonces:
	\[
		\mathbb{E}\left[ X \right] = \mathbb{E}\left[ \mathbb{E}\left[ X  \mid Y \right]  \right] 
	.\] 
\end{boxtheo}

\begin{boxprop}[]
	Por la \textbf{ley de esperanzas iteradas}, tenemos:
	\[
		\text{\normalfont Var}(X) = \mathbb{E}\left[ \text{\normalfont Var}(X  \mid Y) \right] + \text{\normalfont Var} \left( \mathbb{E}[Y  \mid X] \right)
	.\] 
\end{boxprop}

\begin{boxprop}[]
	\[
		\mathbb{E}[XY] = \mathbb{E}\left[ X \mathbb{E}[Y  \mid  X] \right] 
	.\] 
\end{boxprop}

\begin{boxprop}[]
	Sea $\varepsilon = Y - \mathbb{E}[Y \mid X]$, entonces:
	\begin{eqlist}
	\item $\mathbb{E}[\varepsilon  \mid X] = 0$.
	\item $\text{\normalfont Var}(\varepsilon  \mid X) = \sigma^2_{\mathbf{y \mid x}}$.
	\item $\mathbb{E}[\varepsilon] = 0$.
	\item $\text{\normalfont Var}(\varepsilon) = 0$.
	\item $\text{\normalfont Cov}(h(X), \varepsilon) = 0$.
	\end{eqlist}
\end{boxprop}

\begin{boxprop}[]
	\[
		\arg\min_{h(X)} \mathbb{E}\left[ \left( Y - h(X) \right)^2  \right] = \mathbb{E}\left[ Y  \mid X \right] 
	.\] 
\end{boxprop}

\begin{boxdef}[Mejor predictor lineal]
	Dado que $\mathbb{E}[Y  \mid X]$ puede ser una función no lineal bastante complicada---excepto en el caso normal bivariado---, consideramos $\mathbb{E}^* [Y  \mid X]$ el \textbf{mejor predictor lineal} (BLP, por sus siglas en inglés) y lo definimos como sigue:
	\[
		\mathbb{E}^*\left[ Y  \mid X \right] = \alpha + \beta X
	,\] 
	tal que
	\[
		\alpha = \mu_{\mathbf{y}} - \beta \mu_{\mathbf{x}} \qquad \beta = \frac{\sigma_{\mathbf{xy}}}{\sigma^2_{x}}
	.\] 
\end{boxdef}

\begin{boxtheo}[]
	Si la esperanza condicional es lineal, esta coincide con el mejor predictor lineal.
\end{boxtheo}

\sectionbreak

\begin{boxdef}[Independencia en distribución]
	Decimos que, en un conjunto finito de $n$ variables aleatorias $\left\{ X_1, \ldots, X_n \right\}$, $X_1, \ldots, X_n$ son \textbf{mutuamente independientes} si:
	\[
		F_{\mathbf{x_1,\ldots,x_n}} (x_1, \ldots, x_n) = \prod_{k=1}^{n} F_{\mathbf{x_k}} (x_k), \quad \forall x_1, \ldots, x_n
	.\] 
\end{boxdef}

\begin{boxdef}[Independencia en media]
	Decimos que una variable aleatoria $Y$ es \textbf{independiente en media} respecto a otra $X$ si:
	\[
		\mathbb{E}[Y  \mid X] = \mathbb{E}[Y]
	.\] 
\end{boxdef}

\begin{boxdef}[Independencia en covarianza]
	Decimos que dos variables aleatorias $X$ y $Y$ son \textbf{independientes en covarianza} si
	\[
		\text{\normalfont Cov}(X,Y) = 0
	.\] 
\end{boxdef}

\begin{boxtheo}[]
	Independencia en distribución implica independencia en medias, y esta, a su vez, implica independencia en covarianzas; pero no en el orden inverso.
\end{boxtheo}

\begin{boxprop}[]
	Si $Y$ es independiente en media respecto a $X$, entonces:
	\[
		\mathbb{E}[X^r Y] = \mathbb{E}[X^r] \mathbb{E}[Y], \quad \forall r\in \mathbb{R}
	.\] 
\end{boxprop}

\begin{boxprop}[]
	Si $Y$ es independiente en distribución respecto a $X$, entonces:
	\[
		\mathbb{E}[h(X) Y^s] = \mathbb{E}[h(X)] \mathbb{E}[Y^s]
	\] 
	y, por lo tanto, también se cumple
	\[
		\mathbb{E}[X^r Y^s] = \mathbb{E}[X^r] \mathbb{E}[Y^s]
	\] 
	para cualquier función $h(X)$ y todo $r$ y $s$.
\end{boxprop}

\newpage
\section{Econometría, modelos y datos}

\newpage
\section{Modelo de regresión lineal simple}

\newpage
\section{Análisis de los supuestos del modelo}

\newpage
\section{Modelo de regresión lineal múltiple}

\newpage
\section{Otros temas sobre el modelo de regresión lineal}

\vfill\eject
\columnbreak
\end{multicols}
\end{document}
