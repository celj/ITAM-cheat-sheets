% !TEX program = lualatex

\documentclass[8pt,a4paper]{extarticle}
\input{../template_es.tex}
\input{../macros.tex}

\hfuzz=77pt

% Class info
\renewcommand{\csClass}{Fundamentos de econometría}
\renewcommand{\csClassCode}{EST $\cdot$ 21104}
\renewcommand{\csTerm}{Primavera 2021}
\renewcommand{\csKeywords}{ }

% PDF Metadata
\hypersetup{
    pdftitle={\csof \csClass},
    pdfsubject={\csClass},
    pdfauthor={\csAuthorName},
    pdfkeywords={}
}

% Begin document
\begin{document}

\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		\Huge
		\textbf{\csClass}
		\vspace{0.5cm} \\
		\Large
		\cs\ $\cdot$ \csTerm
		\vfill
		\csAuthorName\\
		\vspace{0.8cm}
		\csClassCode\\
		\csSchool
	\end{center}
\end{titlepage}

\begin{multicols}{3}
	\setcounter{page}{1}

	\section{Fundamentos estadísticos de econometría}

	\begin{boxdef}[Error cuadrático medio]
		Definimos el \textbf{error cuadrático medio} como sigue:
		\[
			\text{\normalfont ECM} (\hat{\theta}) = \mathbb{E}_{\theta} \left[ \left( \hat{\theta} - \theta \right)^2  \right]  = \text{\normalfont Var} (\hat{\theta}) + \text{\normalfont Bias}^{2} (\hat{\theta}, \theta)
			.\]
		Este mide la diferencia en media cuadrada entre nuestros valores estimados y el real. Asimismo, podemos definir el \textbf{error cuadrático medio} de una variable $X$ respecto a alguna constante $c$ como sigue:
		\[
			\mathbb{E} \left[ (X - c)^2 \right] = \sigma^2_{\mathrm{x}} + (c - \mu_\mathrm{x})^2
			.\]
	\end{boxdef}

	\begin{boxtheo}[Error cuadrático mínimo]
		El valor que minimiza $\mathbb{E}\left[(X-c)^2\right]$ es $\mu_{\mathrm{x}}$.
	\end{boxtheo}

	\begin{boxcor}[Desigualdad de Markov I]
		Sean $\varphi$ una función monótona creciente y no negativa para los reales no negativos, $X$ una variable aleatoria, $a \ge 0$ y $\varphi(a) > 0$, entonces:
		\[
			\mathbb{P}\left( \left| X \right| \ge a \right) \le \frac{\mathbb{E}\left[ \varphi \left( \left| X \right|  \right)  \right] }{\varphi(a)}
			.\]
	\end{boxcor}

	\begin{boxcor}[Desigualdad de Markov II]
		Sea $X$ una variable aleatoria y $a > 0$, entonces:
		\[
			\mathbb{P}\left( \left| X \right| \ge a  \right) \le \frac{\mathbb{E}\left[ \left| X \right|^r  \right] }{a^r}
			.\]
	\end{boxcor}

	\begin{boxtheo}[Desigualdad de Chebyshev]
		Sean $X$ una variable aleatoria con $\mathbb{E}(X) = \mu$ y $\text{\normalfont Var}(X) = \sigma^2$, y $k > 0$, entonces:
		\[
			\mathbb{P}\left( \|X - \mu\|_{\alpha} \ge k\sigma \right) \le \frac{1}{k^2}
			,\]
		o bien,
		\[
			\mathbb{P}\left( \|X - \mu\|_{\alpha} \ge k \right) \le \frac{\sigma^2}{k^2}
			;\]
		donde $\| \cdot \|_{\alpha}$ es la norma $\alpha$.
	\end{boxtheo}

	\begin{boxrmk}[]
		Sean $X_1, X_2, \ldots, X_n$ una colección de $n\in\mathbb{N}$ variables aleatorias, podemos expresar las funciones de densidad conjunta en términos de densidades condicionales de la siguiente manera:
		\begin{align*}
			f(x_1, x_2, \ldots, x_n) & = f(x_1  \mid x_2, x_3, \ldots, x_n)\cdot  f(x_2, x_3, \ldots, x_n)                                                         \\
			                         & =  f(x_1  \mid x_2, \ldots, x_n) \cdot f(x_2  \mid x_3, \ldots, x_n) \cdot \cdots \cdot f(x_{n-1}  \mid x_n) \cdot f(x_{n})
			.\end{align*}
	\end{boxrmk}

	\begin{boxtheo}[Ley de esperanzas iteradas]
		Sean $X$ y $Y$ dos variables aleatorias tales que $\mathbb{E}\left[ X \right] $ está definida y ambas están en el mismo espacio de probabilidad, entonces:
		\[
			\mathbb{E}\left[ X \right] = \mathbb{E}\left[ \mathbb{E}\left[ X  \mid Y \right]  \right]
			.\]
	\end{boxtheo}

	\begin{boxprop}[]
		Por la \textbf{ley de esperanzas iteradas}, tenemos:
		\[
			\text{\normalfont Var}(X) = \mathbb{E}\left[ \text{\normalfont Var}(X  \mid Y) \right] + \text{\normalfont Var} \left( \mathbb{E}[Y  \mid X] \right)
			.\]
	\end{boxprop}

	\begin{boxprop}[]
		\[
			\mathbb{E}[XY] = \mathbb{E}\left[ X \mathbb{E}[Y  \mid  X] \right]
			.\]
	\end{boxprop}

	\begin{boxprop}[]
		Sea $\varepsilon = Y - \mathbb{E}[Y \mid X]$, entonces:
		\begin{eqlist}
			\item $\mathbb{E}[\varepsilon  \mid X] = 0$.
			\item $\text{\normalfont Var}(\varepsilon  \mid X) = \sigma^2_{\mathrm{y \mid x}}$.
			\item $\mathbb{E}[\varepsilon] = 0$.
			\item $\text{\normalfont Var}(\varepsilon) = 0$.
			\item $\text{\normalfont Cov}(h(X), \varepsilon) = 0$.
		\end{eqlist}
	\end{boxprop}

	\begin{boxprop}[]
		\[
			\argmin_{h(X)} \mathbb{E}\left[ \left( Y - h(X) \right)^2  \right] = \mathbb{E}\left[ Y  \mid X \right]
			.\]
	\end{boxprop}

	\begin{boxdef}[Mejor predictor lineal]
		Dado que $\mathbb{E}[Y  \mid X]$ puede ser una función no lineal bastante complicada --- excepto en el caso normal bivariado, consideramos $\mathbb{E}^* [Y  \mid X]$ el \textbf{mejor predictor lineal} (BLP, por sus siglas en inglés) y lo definimos como sigue:
		\[
			\mathbb{E}^*\left[ Y  \mid X \right] = \alpha + \beta X
			,\]
		tal que
		\[
			\alpha = \mu_{\mathrm{y}} - \beta \mu_{\mathrm{x}} \qquad \beta = \frac{\sigma_{\mathrm{xy}}}{\sigma^2_{x}}
			.\]
	\end{boxdef}

	\begin{boxtheo}[]
		Si la esperanza condicional es lineal, esta coincide con el mejor predictor lineal.
	\end{boxtheo}

	\sectionbreak

	\begin{boxdef}[Independencia en distribución]
		Decimos que, en un conjunto finito de $n$ variables aleatorias $\left\{ X_1, \ldots, X_n \right\}$, $X_1, \ldots, X_n$ son \textbf{mutuamente independientes} si:
		\[
			F_{\mathrm{x_1,\ldots,x_n}} (x_1, \ldots, x_n) = \prod_{k=1}^{n} F_{\mathrm{x}_k} (x_k), \quad \forall x_1, \ldots, x_n
			.\]
	\end{boxdef}

	\begin{boxdef}[Independencia en media]
		Decimos que una variable aleatoria $Y$ es \textbf{independiente en media} respecto a otra $X$ si:
		\[
			\mathbb{E}[Y  \mid X] = \mathbb{E}[Y]
			.\]
	\end{boxdef}

	\begin{boxdef}[Independencia en covarianza]
		Decimos que dos variables aleatorias $X$ y $Y$ son \textbf{independientes en covarianza} si
		\[
			\text{\normalfont Cov}(X,Y) = 0
			.\]
	\end{boxdef}

	\begin{boxtheo}[]
		Independencia en distribución implica independencia en medias, y esta, a su vez, implica independencia en covarianzas; pero no en el orden inverso.
	\end{boxtheo}

	\begin{boxprop}[]
		Si $Y$ es independiente en media respecto a $X$, entonces:
		\[
			\mathbb{E}[X^r Y] = \mathbb{E}[X^r] \mathbb{E}[Y], \quad \forall r\in \mathbb{R}
			.\]
	\end{boxprop}

	\begin{boxprop}[]
		Si $Y$ es independiente en distribución respecto a $X$, entonces:
		\[
			\mathbb{E}[h(X) Y^s] = \mathbb{E}[h(X)] \mathbb{E}[Y^s]
		\]
		y, por lo tanto, también se cumple
		\[
			\mathbb{E}[X^r Y^s] = \mathbb{E}[X^r] \mathbb{E}[Y^s]
		\]
		para cualquier función $h(X)$ y todo $r$ y $s$.
	\end{boxprop}

	\begin{boxprop}
		Sean $\varepsilon = Y - \mathbb{E} [Y \mid X]$ y $U = Y - \mathbb{E}^* [Y \mid X]$, entonces $\varepsilon$ es independiente en media respecto a $X$ y $U$ no está correlacionado con $X$.
	\end{boxprop}

	\newpage

	\begin{boxtheo}[Desigualdad Cauchy-Schwartz]
		Sea $\displaystyle \rho = \frac{\Cov(X,Y)}{\sqrt{\Var(X)} \sqrt{\Var(Y)}}$, entonces $0 \leq \rho^{2} \leq 1$.
	\end{boxtheo}

	\begin{boxprop}
		Sea $X \sim \mathcal{N} (\mu,\sigma^{2})$, entonces, para toda $p$ entera no negativa, los momentos centrados son los siguientes:
		\[\EE [(X - \mu)^{p}] =
			\begin{cases}
				0,                 & \text{si $p$ es impar}; \\
				\sigma^{p}(p-1)!!, & \text{si $p$ es par.}
			\end{cases} \]
		\emph{Nota:} $\displaystyle n!! = \prod_{k = 0}^{\left\lceil \frac{n}{2} \right\rceil - 1} (n - 2k)$. \[\ie \quad (n -1)!! = (n -1)(n - 3)(n - 5) \cdots 3 \cdot 1,\] para toda $n$ par.
	\end{boxprop}

	\begin{boxprop}
		En el caso normal, la ausencia de correlación equivale a independencia estocástica.
	\end{boxprop}

	\begin{boxprop}
		Sea $g(Y \mid X) \sim \mathcal{N}(\EE[Y \mid X], \Var(Y \mid X))$, entonces:
		\[\EE [Y \mid X] = \mu_{\mathrm{y}} + \rho \frac{\sigma_{\mathrm{y}}}{\sigma_{\mathrm{x}}} (X - \mu_{\mathrm{x}}),\]
		y
		\[\Var(Y \mid X) = \sigma_{\mathrm{y}}^{2} (1 - \rho)^2.\]
		Nótese que la esperanza condicional es lineal y la varianza es constante.
	\end{boxprop}

	\begin{boxrmk}
		En el caso normal, la función de regresión coincide con el mejor predictor lineal tal que, si $Y = \EE[ Y \mid X ] + \varepsilon$ y $\EE[ \varepsilon \mid X ] = 0$, $Y = \alpha + \beta X + \varepsilon$ con $\alpha = \mu_{\mathrm{y}} - \beta \mu_{\mathrm{x}}$ y $\displaystyle \beta = \frac{\sigma_{\mathrm{xy}}}{\sigma^2_{\mathrm{x}}}$.
	\end{boxrmk}

	\begin{boxprop}
		La distribución condicional de $Y$ dado $X$ es normal.
		\[Y \mid X \sim \mathcal{N} (\alpha + \beta X, \sigma^2).\]
	\end{boxprop}

	\newpage
	\section{Estimación}

	\begin{boxtheo}
		Sea $X$ una muestra aleatoria finita con $n$ observaciones, entonces, $x_1, x_2, \dots, x_n$ son independientes e idénticamente distribuídas y, por lo tanto, la densidad conjunta de dicha muestra aleatoria es:
		\[F_{\mathrm{x_1,\ldots,x_n}}(x_1,\ldots,x_n) = \prod_{k = 1}^{n} F_{\mathrm{x}_k} (x_k).\]
	\end{boxtheo}

	\begin{boxdef}[Estadístico muestral]
		Sea $T_n = h(X)$ una función escalar de una muestra aleatoria, entonces, este es un \textbf{estadístico muestral}.
	\end{boxdef}

	\begin{boxrmk}
		Todo estadístico muestral $T_n$ es una variable aleatoria porque su valor es determinado por el resultado de un experimento. Asimismo, a la distribución de probabilidad de $T_n$ se le conoce como \emph{distribución muestral}, y está completamente determinada por $h(\cdot)$, $f(\mathrm{x})$ y $n$.
	\end{boxrmk}

	\begin{boxprop}
		La media muestral satisface las siguientes propiedades:
		\begin{bulletlist}
			\item Si $X \sim \Ber(p)$, entonces, $n\bar{X} \sim \Bin(n,p)$.
			\item Si $X \sim \Norm(\mu, \sigma^2)$, entonces, $\displaystyle \bar{X} \sim \Norm\left(\mu,\frac{\sigma^2}{n}\right)$.
			\item Si $X \sim \Exp(\lambda)$, entonces, $k\lambda\bar{X} \sim \csq (k)$ con $k = 2n$.
		\end{bulletlist}
	\end{boxprop}

	\begin{boxtheo}[de la media muestral]
		Dada un muestra aleatoria de tamaño $n$ y cualquier población con $\EE [X] = \mu$ y $\Var(X) = \sigma^2$, la media muestral $\bar{X}$ tiene esperanza $\mu$ y varianza $\displaystyle \frac{\sigma^2}{n}$.
	\end{boxtheo}

	\begin{boxdef}[Momento muestral centrado]
		\[M_r = \frac{1}{n}\sum_{i = 1}^n \left( X_i - \bar{X} \right)^r.\]
	\end{boxdef}

	\begin{boxdef}[Momento muestral no centrado]
		\[M'_r = \frac{1}{n}\sum_{i = 1}^n X_i^r.\]
	\end{boxdef}

	\begin{boxdef}[Momento muestral centrado en la media poblacional]
		\[M^*_r = \frac{1}{n}\sum_{i = 1}^n \left( X_i - \mu \right)^r.\]
		Nótese que este \textbf{no} es un estimador muestral, pues, se requiere un parámetro poblacional. Sin embargo, les será útil para algunas demostraciones.
	\end{boxdef}

	\begin{boxrmk}
		El \emph{teorema de la media muestral} no puede aplicarse a momentos centrados alrededor de la media muestral.
	\end{boxrmk}

	\begin{boxprop}
		Sean $X$ una muestra aleatoria finita con $n$ observaciones y $\bar{X}$ la media muestral, entonces:
		\[\Cov(X_i,\bar{X}) = \Cov(\bar{X},\bar{X}) = \frac{\Var(X)}{n}, \quad \forall X_i \in X.\]
	\end{boxprop}

	\begin{boxprop}
		$M_2 \leq M^*_2$ en cualquier muestra, donde $M_2$ es la varianza muestral. Asimismo, $\EE[M_2] \approx \mu_2$ y $\Var(M_2) \approx \Var(M^*_2)$ si $n$ es grande.
	\end{boxprop}

	\subsection*{Distribuciones derivadas de la normal}

	\begin{bulletlist}
		\item Sean $Z_i \sim \Norm(0,1)$ variables aleatorias independientes, entonces: \[ \sum_{i =1}^{k} Z_i^2 \sim \csq (k).\]
		\item Sean $Z \sim \Norm(0,1)$ y $W \sim \csq(k)$ dos variables aleatorias independientes, entonces: \[\frac{Z}{\sqrt{\left(\displaystyle \frac{W}{k}\right)}} \sim t(k).\]
	\end{bulletlist}

	\begin{boxprop}[Población normal estándar]
		Dadas una variable aleatoria $X \sim \Norm(0,1)$ y una muestra aleatoria $X_1, X_2, \dots, X_n$ de tamaño $n$ con media muestral $\bar{X}$ y varianza muestral $S^2$, se cumple:
		\begin{eqlist}
			\item $\displaystyle \sqrt{n}\bar{X} \sim \Norm(0,1).$
			\item $\displaystyle nS^2 \sim \csq(n -1).$
			\item $\bar{X}$ y $S^2$ son independientes.
			\item $\displaystyle \frac{\sqrt{(n -1)} \bar{X}}{S} \sim t(n - 1)$
		\end{eqlist}
	\end{boxprop}

	\begin{boxprop}[Población normal general]
		Dadas una variable aleatoria $X \sim \Norm(\mu,\sigma^2)$ y una muestra aleatoria $X_1, X_2, \dots, X_n$ de tamaño $n$ con media muestral $\bar{X}$ y varianza muestral $S^2$, se cumple:
		\begin{eqlist}
			\item $\displaystyle \bar{X} \sim \Norm(\mu,\nicefrac{\sigma^2}{n}).$
			\item $\displaystyle \frac{nS^2}{\sigma^2} \sim \csq(n -1).$
			\item $\bar{X}$ y $S^2$ son independientes.
			\item $\displaystyle \frac{\sqrt{(n -1)} (\bar{X} - \mu)}{S} \sim t(n - 1)$
		\end{eqlist}
	\end{boxprop}

	\begin{boxtheo}
		Dada una muestra aleatoria de tamaño $n$, $\displaystyle \left\{ \left( X_i, Y_i \right) \right\}_{i=1}^n$ son observaciones independientes e idénticamente distribuídas tales que $(X_i,Y_i) \in \mathcal{X} \times \mathcal{Y}$. \par
		\emph{Nota:} esto no implica independencia entre vectores.
	\end{boxtheo}

	\begin{boxrmk}
		Decimos que $T$ es un estimador \emph{insesgado} si y solo si $\EE[T - \theta] = 0$ para todo $\theta$.
	\end{boxrmk}

	\begin{boxdef}[Estimador insesgado de varianza mínima]
		Decimos que $T$ es un \textbf{estimador insesgado de varianza mínima} si y solo si:
		\begin{eqlist}
			\item $\EE[T - \theta] = 0$ para todo $\theta$.
			\item $\Var(T) \leq \Var(T^*)$, para todo $T^*$, tal que $\EE[T^* - \theta] = 0$.
		\end{eqlist}
	\end{boxdef}

	\begin{boxtheo}
		En una muestra aleatoria de tamaño $n$, de cualquier población, la media muestral es el estimador lineal insesgado de menor varianza respecto a la media poblacional.
	\end{boxtheo}

	\newpage
	\section{Teoría asintótica}

	Sea $T_n$ una sucesión de variables aleatorias con distribuciones acumuladas $G_n(t) = \P(T_n \leq t)$ con esperanzas $\EE[T_n]$ y varianzas $\Var(T_n)$:

	\begin{boxdef}[Convergencia en probabilidad]
		Decimos que $T_n$ \textbf{converge en probabilidad}\footnote{$\displaystyle T_n \overset{\displaystyle p}{\longrightarrow} c$.} a una constante $c$ si y solo si:
		\begin{align*}
			\lim_{n \to \infty} G_n(t) = 0, \quad & \forall t < c,    \\
			\lim_{n \to \infty} G_n(t) = 1, \quad & \forall t \geq c.
		\end{align*}
		O bien,
		\[\lim_{n \to \infty} \P( \left| T_n - c \right| \geq \varepsilon ) = 0, \quad \forall \varepsilon > 0.\]
	\end{boxdef}

	\begin{boxdef}[Convergencia en media $r$]
		Decimos que $T_n$ \textbf{converge en media} $\bm r$\footnote{$\displaystyle T_n \overset{\displaystyle L^r}{\longrightarrow} c$.} a una constante $c$ si y solo si:
		\[\lim_{n \to \infty} \EE \left[ (T_n - c)^r \right] = 0.\]
	\end{boxdef}

	\begin{boxdef}[Convergencia en distribución]
		Decimos que $T_n$ \textbf{converge en distribución}\footnote{$\displaystyle T_n \overset{\displaystyle d}{\longrightarrow} G(\cdot)$.} a $G(\cdot)$ si y sólo si:
		\[\lim_{n \to \infty} G_n(t) = G(t), \quad \forall t.\]
	\end{boxdef}

	\begin{boxcor}
		Sea $T_n$ una sucesión de variables aleatorias tal que $\displaystyle \lim_{n \to \infty} \EE[T_n] = c$ y $\displaystyle \lim_{n \to \infty} \Var(T_n) = 0$, entonces, $T_n$ converge en \emph{media cuadrática}\footnote{$T_n \overset{L^2}{\longrightarrow} c$.} a $c$.
	\end{boxcor}

	\begin{boxcor}
		Si $T_n \overset{L^r}{\longrightarrow} c$, entonces, $T_n \overset{p}{\longrightarrow} c$.
	\end{boxcor}

	\begin{boxtheo}[Ley de los grandes números]
		En una muestra aleatoria de cualquier población con esperanza $\mu$ y varianza $\sigma^2$, la media muestral converge en probabilidad a la media poblacional.
		\[\ie \quad \bar{X} \overset{p}{\longrightarrow} \mu.\]
		\textbf{Caso multivariado:}
		\[\left( \bar{X}_1, \bar{X}_2, \dots, \bar{X}_n \right) \plim \left( \mu_{\mathrm{x}_1}, \mu_{\mathrm{x}_2}, \dots, \mu_{\mathrm{x}_n} \right).\]
	\end{boxtheo}

	\begin{boxtheo}[Teorema central del límite]
		En una muestra aleatoria de cualquier población con esperanza $\mu$ y varianza $\sigma^2$, la media muestral estandarizada tiende en distribución a una normal estándar.
		\[\ie \quad \frac{\sqrt{n}\left( \bar{X} - \mu \right)}{\sigma} \overset{d}{\longrightarrow} \Norm(0,1).\]
		Equivalentemente,
		\[\sqrt{n}\left( \bar{X} - \mu \right) \overset{d}{\longrightarrow} \Norm(0,\sigma^2).\]
		\textbf{Caso multivariado:}
		\[\sqrt{n} \left( \bar{\bm X} - {\bm \mu} \right) \dlim \Norm(0,{\bm \Sigma}) .\]
	\end{boxtheo}

	\begin{boxprop}
		Decimos que la \emph{distribución asintótica} de la media muestral es $\Norm(\mu,\nicefrac{\displaystyle\sigma^2}{\displaystyle n})$.
		\[\ie \quad \bar{X} \overset{A}{\sim} \Norm(\mu,\nicefrac{\displaystyle\sigma^2}{\displaystyle n}).\]
	\end{boxprop}

	\begin{boxprop}
		El análogo muestral al \textbf{mejor predictor lineal} es:
		\[
			\mathbb{E}^*\left[ Y  \mid X \right] = A + B X
			,\]
		donde
		\[
			A = \bar{Y} - B \bar{X} \qquad B = \frac{ S_{\mathrm{xy}} }{ S_{\mathrm{x}^2} }
			.\]
	\end{boxprop}

	\begin{boxdef}[Consistencia]
		Decimos que $T_n$ es un estimador consistente de $\theta$ si y solo si:
		\[T_n \plim \theta.\]
	\end{boxdef}

	\sectionbreak

	\begin{boxtheo}[de Slutsky]
		Sean $T_n$, $V_n$ y $W_n$ sucesiones variables aleatorias, $h(\cdot)$ una función y $c$ una constante --- tal que estas últimas dos no dependen de $n$; lo siguiente se cumple:
		\begin{eqlist}
			\item Si $T_n \plim c$ y $h(T_n)$ es continua en $c$, entonces, $h(T_n) \plim h(c)$.
			\item Si $V_n \plim c_1$, $W_n \plim c_2$ y $h(V_n,W_n)$ es continua en $(c_1,c_2)$, entonces, $h(V_n,W_n) \plim h(c_1,c_2)$.
			\item Si $V_n \plim c$ y $W_n$ tiene una distribución límite, entonces, la distribución límite de $V_n + W_n$ es la misma que la correspondiente a $c + W_n$.
			\item Si $V_n \plim c$ y $W_n$ tiene una distribución límite, entonces, la distribución límite de $V_n W_n$ es la misma que la correspondiente a $c W_n$.
			\item Si $\sqrt{n}\left[ T_n - \theta \right] \dlim \Norm(0, \phi^2)$ y $h(T_n)$ es continuamente diferenciable en $\theta$, entonces: \[\sqrt{n} \left[ h(T_n) - h(\theta) \right] \dlim \Norm\left(0, \phi^2 \cdot \left[ h'(\theta) \right]^2 \right),\] tal que \[T_n \overset{A}{\sim} \Norm \left( h(\theta), \left[ h'(\theta) \right]^2 \nicefrac{\displaystyle \phi^2}{\displaystyle n} \right).\] \textbf{Caso multivariado:} si $\sqrt{n} \left[ {\bm T_n} - {\bm\theta} \right] \dlim \Norm(0, {\bm \Sigma})$ y $h({\bm T_n})$ es continuamente diferenciable en ${\bm\theta}$, entonces: \[\sqrt{n} \left[ h({\bm T_n}) - h({\bm\theta}) \right] \dlim \Norm\left(0, \nabla h({\bm\theta})^T \cdot \text{\Large $\bm \Sigma$} \cdot \nabla h({\bm\theta}) \right),\] tal que \[{\bm T_n} \overset{A}{\sim} \Norm \left( h({\bm \theta}, ), \nabla h({\bm\theta})^T \cdot \frac{1}{n} \text{\Large $\bm \Sigma$} \cdot \nabla h({\bm\theta}) \right) .\]
		\end{eqlist}
	\end{boxtheo}

	\begin{boxdef}[Mejor estimador asintóticamente normal]
		Decimos que $T_n$ es el \textbf{mejor estimador asintóticamente normal} si y solo si:
		\begin{eqlist}
			\item $T_n \overset{A}{\sim} \Norm(\theta,\nicefrac{\displaystyle \sigma^2}{\displaystyle n})$.
			\item $\sigma^2 \leq \sigma^{*2}$, para todo $T^*_n$, tal que $T^*_n \dlim \Norm(\theta,\nicefrac{\displaystyle \sigma^{*2}}{\displaystyle n})$.
		\end{eqlist}
		A este criterio también se le conoce como \textbf{eficiencia asintótica}.
	\end{boxdef}

	\begin{boxdef}[Intervalo de confianza]
		Dado un conjunto de observaciones $x_1, \dots, x_n$ de las variables aleatorias $X_1, \dots, X_n$, y sean $\theta$ nuestro parámetro de interés y $\gamma\in(0,1)$. Si existen estadísticos muestrales $L_n = g(X_1, \dots, X_n)$ y $U_n = h(X_1, \dots, X_n)$ tales que: $$\P(L_n < \theta < U_n) = \gamma, \quad \forall \theta;$$ entonces, decimos que $(l_n,u_n)$ es un \textbf{intervalo de confianza} $\gamma \times 100 \%$ del parámetro $\theta$, donde $l_n = g(x_1, \dots, x_n)$ y $u_n = h(x_1, \dots, x_n)$.
	\end{boxdef}

	\newpage

	\subsubsection*{Algunas distribuciones asintóticas}

	\begin{eqlist}
		\item $\displaystyle S^2 \overset{A}{\sim} \Norm\left(\sigma^2, \frac{\mu_4 - \mu_2^2}{n}\right)$.
		\item $\displaystyle S_{\mathrm{xy}} \overset{A}{\sim} \Norm \left( \sigma_{\mathrm{xy}}, \frac{\mu_{22} - \mu_{11}^2}{n} \right)$.
		\item $\left( \bar{X}, \bar{Y} \right) \overset{A}{\sim} \Norm \left( \mu_{\mathrm{x}}, \mu_{\mathrm{y}}; \nicefrac{ \displaystyle \sigma^2_{\mathrm{x}}}{ \displaystyle n}, \nicefrac{ \displaystyle \sigma^2_{\mathrm{y}}}{ \displaystyle n}, \nicefrac{ \displaystyle \sigma_{\mathrm{xy}}}{ \displaystyle n} \right)$.
		\item $\displaystyle \frac{\bar{X}}{\bar{Y}} \overset{A}{\sim} \Norm \left( \theta, \frac{\phi^2}{n} \right) $ tal que $\displaystyle \theta = \frac{\mu_{\mathrm{x}}}{\mu_{\mathrm{y}}}$ y \par $\displaystyle \phi^2 = \left( \frac{1}{\mu_{\mathrm{y}}} \right)^2 \left( \sigma^2_\mathrm{x} + \theta^2 \sigma^2_\mathrm{y} - 2\theta\sigma_\mathrm{xy} \right) $.
		\item $\displaystyle \frac{S_{\mathrm{xy}}}{S^2_\mathrm{x}} \overset{A}{\sim} \Norm \left( \beta, \frac{\phi^2}{n} \right)$ donde $\displaystyle \beta = \frac{\sigma_\mathrm{xy}}{\sigma_\mathrm{x}^2}$ y \par $\displaystyle \phi^2 = \left( \frac{1}{\mu^2_{20}} \right) \left( \mu_{22} + \beta^2 \mu_{40} - 2\beta\mu_{31} \right)$.
	\end{eqlist}

	\newpage
	\section{Inferencia en el modelo lineal}

	\vfill\eject
	\columnbreak
\end{multicols}
\end{document}
