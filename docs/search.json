[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"econ student ITAM, given task compiling notes easy--study cheat sheets. hope find useful creative.\nEnjoy!Carlos Lezama\nLast update: Tuesday, 12 October 2021","code":""},{"path":"consumer-and-producer-theory.html","id":"consumer-and-producer-theory","chapter":"1 Consumer and Producer Theory","heading":"1 Consumer and Producer Theory","text":"section still development.definitions theorems can also used courses :calculus,linear algebra,optimization, andreal analysis.","code":""},{"path":"consumer-and-producer-theory.html","id":"linear-algebra","chapter":"1 Consumer and Producer Theory","heading":"1.1 Linear Algebra","text":"","code":""},{"path":"consumer-and-producer-theory.html","id":"basic-notions-of-linear-algebra","chapter":"1 Consumer and Producer Theory","heading":"Basic Notions of Linear Algebra","text":"Vectors always columns, customary write rows save space.Definition 1.1  (Homogeneous System) \\(b = 0_{m\\times 1}\\), system said homogeneous least one solution, given \\(x = 0_{n\\times 1}\\).\n\\[\n\\text{.e.}\\quad Ax = 0.\n\\]Proposition 1.1  \\(b \\neq 0\\), need exist \\(x\\) \\(Ax = b\\).Definition 1.2  (Linear Combination) \\(y \\\\mathbb{R}^m\\) linear combination \\(x_1, \\dots, x_n \\\\mathbb{R}^m\\) exists \\(\\alpha_1, \\dots, \\alpha_n \\\\mathbb{R}\\) \\(\\displaystyle y = \\sum_{= 1}^n \\alpha_i x_i\\).Example 1.1  Consider three matrices \\(A_{m\\times n}\\), \\(B_{k\\times m}\\) \\(C_{k\\times n}\\) \\(BA = C\\).Observe :rows \\(C\\) linear combinations rows \\(\\), andcolumns \\(C\\) linear combinations columns \\(B\\).statements fact equivalent since \\(BA = C\\) \\(^T B^T = C^T\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"elementary-row-operations-and-rref","chapter":"1 Consumer and Producer Theory","heading":"Elementary Row Operations and RREF","text":"Definition 1.3  (Elementary Row Operations) Elementary row operations simple operations allow transform system linear equations equivalent system.three elementary row operations:switching rows,multiplying row non-zero constant, andreplacing row sum row another row.Theorem 1.1  Let \\(E_{n\\times n}\\) elementary matrix. \\(E\\) invertible.exist number \\(k\\) elementary matrices \\(E_1, \\dots, E_k\\) \\(E^{-1} = E_1 \\cdots E_k\\).Definition 1.4  (Row Reduced Echelon Form) Applying three elementary row operations sequentially given matrix, can find row reduced echelon form (RREF).RREF matrix following properties:zero rows bottom,first non-zero entry non-zero row 1,first non-zero entry row occurs column \\(j\\) first non-zero entry next row occurs column \\(j'\\), \\(j < j'\\), andif first non-zero entry row occurs column \\(j\\), earlier rows zeros column \\(j\\).Theorem 1.2  Every matrix unique RREF.","code":""},{"path":"consumer-and-producer-theory.html","id":"rank","chapter":"1 Consumer and Producer Theory","heading":"Rank","text":"Definition 1.5  (Rank) rank \\(A_{m\\times n}\\) number non-zero rows RREF \\('\\).Definition 1.6  (Pivotal Column) pivotal column one contains first non-zero entry row.Corollary 1.1  \\(\\) \\(m\\times n\\), \\(\\text{rk}= \\text{rk}' \\leq \\min\\{m, n\\}\\).Also, given \\(A_{m\\times n}\\):\\(\\text{rk}= m\\) \\('\\) zero rows.\\(\\text{rk}= n\\) columns \\('\\) pivotal.Theorem 1.3  Let \\(\\) \\(m \\times n\\). \\(\\text{rk}= m\\) , \\(\\forall b \\\\mathbb{R}^m\\), \\(\\exists x \\\\mathbb{R}^n\\) \\(Ax = b\\). Namely, least one solution system.Theorem 1.4  Let \\(\\) \\(m \\times n\\). \\(\\text{rk}= n\\) , \\(\\forall b \\\\mathbb{R}^m\\), \\(Ax = Ay = b\\), \\(x = y\\). Namely, one solution system.Corollary 1.2  Let \\(\\) \\(m\\times n\\). \\(b \\\\mathbb{R}^m\\), \\(Ax = b\\) unique solution \\(\\mathbb{R}^n\\) \\(\\text{rk}= m = n\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"subspaces","chapter":"1 Consumer and Producer Theory","heading":"Subspaces","text":"Definition 1.7  (Subspace) non-empty set \\(S \\subseteq \\mathbb{R}^m\\) subspace \\(R^m\\) \\(\\alpha x + \\beta y \\S\\) whenever \\(x, y \\S\\) \\(\\alpha, \\beta \\\\mathbb{R}\\).Proposition 1.2  intersection members arbitrary family subspaces subspace. union subspaces need subspace.Definition 1.8  (Range) range \\(A_{m\\times n}\\) set\n\\[\nR() = \\left\\{ b \\\\mathbb{R}^m : \\exists x \\\\mathbb{R}^n,\\ Ax = b \\right\\}.\n\\]Definition 1.9  (Null Space) null space \\(A_{m\\times n}\\) set\n\\[\nN() = \\left\\{ x \\\\mathbb{R}^n : Ax = 0 \\right\\}.\n\\]Proposition 1.3  \\(A_{m\\times n}\\), following propositions hold:\\(N() = N(')\\),\\(R() \\neq R(')\\),\\(N() \\subseteq N(BA)\\),\\(R(BA) \\subseteq R(B)\\),\\(R() = \\mathbb{R}^m \\iff \\text{rk}= m\\), \\(N() = \\{0\\} \\iff \\text{rk}= n\\).Definition 1.10  (Linear Span) Vectors \\(a_1, \\dots, a_n \\S\\) span \\(S\\) , every \\(x \\S\\), exist \\(\\alpha_1, \\dots, \\alpha_n \\\\mathbb{R}\\) \\(\\displaystyle x = \\sum_{= 1}^n \\alpha_i a_i\\).Namely, \\(x\\) linear combination \\(a_1, \\dots, a_n\\).Definition 1.11  (Linear Independence) Vectors \\(a_1, \\dots, a_n\\) \\(\\mathbb{R}^m\\) linearly independent , \\(\\alpha_1, \\dots, \\alpha_n \\\\mathbb{R}\\),\n\\[\n\\sum_{=1}^n \\alpha_i a_i = 0 \\implies \\alpha_1 = \\cdots = \\alpha_n = 0.\n\\]linearly independent set contain \\(0\\).Proposition 1.4  given matrix, following propositions hold:non-zero rows matrix RREF linearly independent,subsets linearly independent set also linearly independent, anda set \\(\\{a_1, \\dots, a_n\\}\\) \\(\\mathbb{R}^m\\) linearly independent \\(N(A_{m\\times n}) = \\{0_{n\\times 1}\\}\\), \\(\\displaystyle = \\left[ a_1 \\mid \\cdots \\mid a_n \\right]_{m\\times n}\\) formed merging members \\(\\{a_1, \\dots, a_n\\}\\) columns.Lemma 1.1  \\(\\{a_1, \\dots, a_n\\} \\subset \\mathbb{R}^m\\) linearly dependent , \\(k \\\\{2, \\dots, n\\}\\), \\(a_k\\) linear combination \\(a_1, \\dots, a_{k-1}\\).Definition 1.12  (Basis) Let \\(S\\) subspace \\(\\mathbb{R}^m\\). set \\(\\{a_1, \\dots, a_n\\} \\subseteq S\\) basis \\(S\\) spans \\(S\\) linearly independent.Theorem 1.5  Let \\(S\\) subspace \\(\\mathbb{R}^m\\) basis \\(\\{a_1, \\dots, a_n\\}\\). \\(\\{b_1, \\dots, b_r\\} \\subset S\\) linearly independent, \\(r \\leq n\\).Corollary 1.3  \\(m + 1\\) vectors \\(\\mathbb{R}^m\\) linearly dependent.Corollary 1.4  \\(\\{a_1, \\dots, a_n\\}\\) \\(\\{b_1, \\dots, b_r\\}\\) two bases \\(S\\), \\(n = r\\).Definition 1.13  (Dimension) dimension subspace \\(S\\) number elements basis \\(S\\).Theorem 1.6  (Fundamental Theorem Linear Algebra ) Let \\(\\) \\(m\\times n\\). \\(\\dim R(^T) = \\text{rk}\\),\\(\\text{rk}= \\text{rk}^T\\), \\(\\dim N() = n - \\text{rk}\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"orthogonality","chapter":"1 Consumer and Producer Theory","heading":"Orthogonality","text":"Definition 1.14  (Orthogonal Complement) Let \\(S\\) subspace \\(\\mathbb{R}^m\\). orthogonal complement \\(S\\) set\n\\[\nS^\\perp = \\left\\{ y \\\\mathbb{R}^m : y \\cdot x = 0,\\ \\forall x \\S \\right\\}.\n\\]Lemma 1.2  subspace \\(S\\) \\(\\mathbb{R}^m\\), \\(S \\cap S^\\perp = \\{0\\}\\).Theorem 1.7  (Fundamental Theorem Linear Algebra II) Let \\(\\) \\(m\\times n\\). \\(R() = N(^T)^\\perp\\), \\(R()^\\perp = N(^T)\\).Corollary 1.5  subspace \\(S\\) \\(\\mathbb{R}^m\\), \\((S^\\perp)^\\perp = S\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"inverses","chapter":"1 Consumer and Producer Theory","heading":"Inverses","text":"Theorem 1.8  Let \\(\\) \\(C\\) \\(n\\times n\\). \\(AC = \\), \\(\\text{rk}= \\text{rk}C = n\\).Theorem 1.9  Let \\(\\) \\(C\\) \\(n\\times n\\). \\(AC = \\), \\(CA = \\).Definition 1.15  (Invertible Matrix) \\(A_{n\\times n}\\) invertible exists \\(C_{n\\times n}\\) \\(AC =CA = \\).Theorem 1.10  \\(AC = CA = \\) \\(AB = BA =\\), \\(C = B\\).Remark. \\(\\) \\(B\\) invertible, \\(AB\\) invertible well.Remark. \\(\\) invertible, \\(^T\\) \\((^T)^{-1} = (^{-1})^T\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"real-analysis","chapter":"1 Consumer and Producer Theory","heading":"1.2 Real Analysis","text":"","code":""},{"path":"consumer-and-producer-theory.html","id":"basic-notions-of-topology","chapter":"1 Consumer and Producer Theory","heading":"Basic Notions of Topology","text":"Definition 1.16  (Metric) Let \\(X\\) set. function \\(d : X \\times X \\\\mathbb{R}\\) metric \\(X\\) , \\(x, y, z \\X\\), following conditions hold:\\(d(x,y) \\geq 0\\),\\(d(x,y) = 0 \\iff x = y\\),\\(d(x, y) = d((y, x)\\), \\(d(x, y) \\leq d(x, z) + d(z, y)\\).Definition 1.17  (Metric Space) metric space pair \\((X, d)\\) \\(X\\) set \\(d\\) metric \\(X\\).Definition 1.18  (Open Ball) Let \\((X, d)\\) metric space. \\(x \\X\\) \\(\\varepsilon > 0\\), \\(B(x, \\varepsilon) = \\{ y \\X : d(x, y) < \\varepsilon \\}\\) open ball centered \\(x\\) radius \\(\\varepsilon\\).Definition 1.19  (Interior) Let \\(S \\subseteq X\\). \\(x\\) interior point \\(S\\) \\(B(x, \\varepsilon) \\subset S\\) \\(\\varepsilon > 0\\). set interior points \\(S\\) interior \\(S\\).Remark. \\[\n\\text{int}(S) \\subseteq S\n\\]Definition 1.20  (Open Set) \\(S\\) open \\(S = \\text{int}(S)\\).Remark. \\(\\varnothing\\), \\(\\text{int}(S)\\) \\(B(x, \\varepsilon)\\) open.Definition 1.21  (Closure) Let \\(S \\subseteq X\\). \\(x\\) closure point \\(S\\) \\(B(x, \\varepsilon) \\cap S \\neq \\varnothing\\) every \\(\\varepsilon > 0\\). set closure points \\(S\\) closure \\(S\\).Remark. \\[\nS \\subseteq \\text{cl}(S)\n\\]Definition 1.22  (Closed Set) \\(S\\) closed \\(S = \\text{cl}(S)\\).Remark. \\(\\varnothing\\) \\(\\text{cl}(S)\\) closed.Theorem 1.11  \\(S\\) closed \\(X \\setminus S\\) open.Definition 1.23  (Boundary) Let \\(S \\subseteq X\\). \\(x\\) boundary point \\(S\\) \\(B(x, \\varepsilon) \\cap S \\neq \\varnothing\\) \\(B(x, \\varepsilon) \\cap (X \\setminus S) \\neq \\varnothing\\) every \\(\\varepsilon > 0\\). set boundary points \\(S\\) boundary \\(S\\).Remark. \\(\\partial S = \\partial(X \\setminus S)\\).\\(\\partial S \\subseteq \\text{cl}(S)\\).\\(S\\) closed \\(\\partial S \\subseteq S\\).\\(\\partial S\\) closed set.Definition 1.24  (Metric Subspace) Let \\((X, d)\\) metric space \\(S \\subseteq X\\). \\(d\\) metric \\(S\\) therefore \\((S, d)\\) metric space well, usually referred metric subspace \\((X, d)\\).Theorem 1.12  Let \\((S, d)\\) metric subspace \\((X, d)\\).\\(\\subseteq S\\) open \\((S, d)\\) exists open set \\(U\\) \\((X, d)\\) \\(= S \\cap U\\).\\(\\subseteq S\\) closed \\((S, d)\\) exists closed set \\(U\\) \\((X, d)\\) \\(= S \\cap U\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"sequences","chapter":"1 Consumer and Producer Theory","heading":"Sequences","text":"Definition 1.25  (Sequence) Let \\((X, d)\\) metric space. sequence \\((X, d)\\) map \\(f : \\mathbb{N}\\X\\).Definition 1.26  (Convergent Sequence) sequence \\(\\{x_k\\}\\) said convergent following property holds: exists \\(x \\X\\) , every \\(\\varepsilon > 0\\), exists \\(l \\\\mathbb{N}\\) , every \\(k > l\\), \\(x_k \\B(x, \\varepsilon)\\). \\(x\\) called limit \\(\\{x_k\\}\\).Theorem 1.13  \\(\\lim x_k\\) exists, unique.Definition 1.27  (Subsequence) Let \\(\\{x_k\\}\\) sequence. subsequence \\(\\{x_k\\}\\) sequence obtained deleting (possibly none, possibly infinitely many) members \\(\\{x_k\\}\\). Namely, let \\(\\{k_n\\}\\) non-decreasing sequence integers. \\(\\{x_{k_{n}}\\}\\) subsequence \\(\\{x_k\\}\\).Theorem 1.14  \\(\\{x_k\\}\\) convergent limit \\(x\\) every subsequence \\(\\{x_k\\}\\) convergent limit \\(x\\).Theorem 1.15  Let \\((X, d)\\) metric space \\(S \\subseteq X\\). \\(x \\\\text{cl}(S)\\) exists sequence \\(\\{x_k\\}\\) \\(x_k \\S\\) \\(k\\), \\(\\lim x_k = x\\).Theorem 1.16  \\(S\\) closed every convergent sequence \\(S\\) converges element \\(S\\).Definition 1.28  (Bounded Sequence) Let \\((X, d)\\) metric space. sequence \\(\\{x_k\\}\\) \\(X\\) bounded contained ball finite radius, .e., \\(\\varepsilon > 0\\) \\(y \\X\\), \\(x_k \\B(y, \\varepsilon)\\) every \\(k\\).Theorem 1.17  (Bolzano–Weierstrass) Every bounded sequence \\(\\mathbb{R}^m\\) convergent subsequence.Definition 1.29  (Cauchy Sequence) Let \\((X, d)\\) metric space. sequence \\(\\{x_k\\}\\) \\(X\\) Cauchy sequence , every \\(\\varepsilon > 0\\), exists positive integer \\(k_\\varepsilon\\) , whenever \\(k, l > k_\\varepsilon\\), \\(d(x_k, x_l) < \\varepsilon\\).Theorem 1.18  \\(\\{x_k\\}\\) convergent, \\(\\{x_k\\}\\) Cauchy. \\(\\{x_k\\}\\) Cauchy, \\(\\{x_k\\}\\) bounded.Definition 1.30  (Completeness) Let \\((X, d)\\) metric space. \\((X, d)\\) complete every Cauchy sequence \\(X\\) converges element \\(X\\).Theorem 1.19  Let \\((X, d)\\) complete \\(S \\subseteq X\\). \\((S, d)\\) complete \\(S\\) closed subset \\(X\\).Definition 1.31  (Open Cover) Let \\((X, d)\\) metric space \\(S \\subseteq X\\). collection open sets \\(O_i\\), \\(\\\\), open cover \\(S\\) \\(\\displaystyle S \\subset \\bigcup_{\\} O_i\\).Definition 1.32  (Finite Subcover) Let \\(O_i\\), \\(\\\\), open cover \\(S\\). \\(I_0\\) finite subset \\(\\) \\(\\displaystyle S \\subset \\bigcup_{\\I_0} O_i\\), collection \\(O_i\\), \\(\\I_0\\), finite subcover \\(S\\).Definition 1.33  (Compactness) \\(S\\) compact every open cover \\(S\\) finite subcover.Theorem 1.20  (Compactness) compact set bounded closed.Theorem 1.21  (Sequential Compactness) \\(S\\) compact every sequence \\(S\\) subsequence converges point \\(S\\).Theorem 1.22  (Heine-Borel) subset \\(\\mathbb{R}^m\\) compact closed bounded.","code":""},{"path":"consumer-and-producer-theory.html","id":"continuity","chapter":"1 Consumer and Producer Theory","heading":"Continuity","text":"Definition 1.34  (Continuity) Let \\((X, d)\\) \\((Y, \\sigma)\\) metric spaces. function \\(f : X \\Y\\) continuous \\(x \\X\\) , every \\(\\varepsilon > 0\\), exists \\(\\delta > 0\\) , whenever \\(x' \\B(x, \\delta)\\), \\(f(x') \\B(f(x), \\varepsilon)\\).Definition 1.35  (Global Continuity) \\(f\\) continuous continuous \\(x\\) every \\(x \\X\\).Theorem 1.23  \\(f\\) continuous \\(x\\) every sequence \\(\\{x_k\\}\\) \\(X\\), whenever \\(\\lim x_k = x\\), \\(\\lim f(x_k) = f(x)\\).Definition 1.36  (Image) Let \\((X, d)\\) \\((Y, \\sigma)\\) metric spaces \\(f : X \\Y\\). \\(S \\subseteq X\\), \\(f(S) = \\{ f(x) \\Y : x \\S \\}\\).Definition 1.37  (Inverse Image) Let \\((X, d)\\) \\((Y, \\sigma)\\) metric spaces \\(f : X \\Y\\). \\(S \\subseteq Y\\), \\(f^{-1}(S) = \\{ x \\X : f(x) \\S \\}\\).Theorem 1.24  following equivalent:\\(f\\) continuous.\\(f^{-1}(S)\\) open \\(S\\) open.\\(f^{-1}(S)\\) closed \\(S\\) closed.Definition 1.38  (Upper Semicontinuous) Let \\((X, d)\\) metric space. function \\(f : X \\\\mathbb{R}\\) upper semicontinuous \\(\\{x \\X : f(x) \\geq \\alpha \\}\\) closed every \\(\\alpha \\\\mathbb{R}\\).Definition 1.39  (Lower Semicontinuous) Let \\((X, d)\\) metric space. function \\(f : X \\\\mathbb{R}\\) lower semicontinuous \\(\\{x \\X : f(x) \\leq \\alpha \\}\\) closed every \\(\\alpha \\\\mathbb{R}\\).Theorem 1.25  \\(f : X \\\\mathbb{R}\\) continuous \\(f\\) upper semicontinuous lower semicontinuous.Definition 1.40  (Maximizer / Minimizer) Let \\((X, d)\\) metric space \\(S \\subseteq X\\). function \\(f : S \\\\mathbb{R}\\) maximizer \\(x^* \\S\\), \\(f(x) \\leq f(x^*)\\) every \\(x \\S\\). Similarly, \\(f\\) minimizer \\(x_* \\S\\), \\(f(x) \\geq f(x_*)\\) every \\(x \\S\\).Theorem 1.26  (Weierstrass) Let \\(S\\) compact \\(f : S \\\\mathbb{R}\\) continuous. \\(f\\) maximizer minimizer.Theorem 1.27  Let \\(S\\) compact \\(f : S \\\\mathbb{R}\\) upper semicontinuous. \\(f\\) maximizer.Theorem 1.28  Let \\(S\\) compact \\(f : S \\\\mathbb{R}\\) lower semicontinuous. \\(f\\) minimizer.Theorem 1.29  (Separation) Let \\((X, d)\\) metric space. \\(S\\) \\(T\\) disjoint closed subsets \\(X\\), exists disjoint open sets \\(U\\) \\(V\\) \\(U \\supset S\\) \\(V \\supset T\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"differentiable-functions","chapter":"1 Consumer and Producer Theory","heading":"1.3 Differentiable Functions","text":"Definition 1.41  (Differentiability) Let \\(S \\subseteq \\mathbb{R}\\), \\(x \\S\\) \\(f : S \\\\mathbb{R}\\). \\(f\\) differentiable \\(x\\) \n\\[\n\\lim_{u \\0} \\frac{f(x + u) - f(x)}{u}\n\\]\nexists.Namely, \\(f\\) differentiable \\(x\\) \\(y \\\\mathbb{R}\\) , every \\(\\varepsilon > 0\\), exists \\(\\delta > 0\\) \n\\[\n\\left\\{ x + u \\B_\\delta(x) \\cap S : u \\neq 0 \\right\\} \\implies \\left\\lvert \\frac{f(x + u) - f(x)}{u} - y \\right\\rvert < \\varepsilon\n\\]\n\\(y = f'(x)\\).Definition 1.42  (Partial Derivative) Let \\(S \\subseteq \\mathbb{R}^n\\), \\(x \\S\\) \\(f : S \\\\mathbb{R}\\). \\(j\\)th partial derivative \\(f\\) \\(x\\) \n\\[\nD_j f(x) := \\lim_{u \\0} \\frac{f(x + ue_j) - f(x)}{u}\n\\]\nlimit exists.Definition 1.43  (Gradient) \\(D_j f(x)\\) exists \\(j = 1, \\dots, n\\), gradient \\(f\\) \\(x\\) vector\n\\[\n\\nabla f(x) :=\n\\begin{bmatrix}\nD_1 f(x) \\\\\n\\vdots \\\\\nD_n f(x)\n\\end{bmatrix}\n\\\\mathbb{R}^n.\n\\]Definition 1.44  (Jacobian) Let \\(S \\subseteq \\mathbb{R}^n\\), \\(x \\S\\) \\(f : S \\\\mathbb{R}^m\\). Suppose \\(D_j f_i(x)\\) exists \\(\\) \\(j\\), Jacobian \\(f\\) \\(x\\) \\(m\\times n\\) matrix\n\\[\nJ_f(x) :=\n\\begin{bmatrix}\nD_1 f_1(x) & \\cdots & D_n f_1(x) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nD_1 f_m(x) & \\cdots & D_n f_m(x)\n\\end{bmatrix}_{m\\times n}\n=\n\\begin{bmatrix}\n\\nabla f_1(x)^T \\\\\n\\vdots \\\\\n\\nabla f_m(x)^T\n\\end{bmatrix}.\n\\]Definition 1.45  (Differentiability) Let \\(S \\subseteq \\mathbb{R}^n\\). \\(f : S \\\\mathbb{R}^m\\) differentiable \\(x \\S\\) following two conditions hold:\\(D_j f_i (x)\\) exists \\(\\) \\(j\\),\n\\[\n\\lim_{u \\0} \\frac{\\lVert f(x + u) - f(x) - J_f(x)u \\rVert}{\\lVert u \\rVert} = 0.\n\\]second condition holds , every \\(\\varepsilon > 0\\), \\(\\delta > 0\\) , whenever \\(x + u \\B_\\delta(x) \\cap S\\) \\(u \\neq 0_{n\\times 1}\\), \n\\[\n\\frac{\\lVert f(x + u) - f(x) - J_f(x)u \\rVert}{\\lVert u \\rVert} < \\varepsilon.\n\\]Theorem 1.30  Suppose \\(S\\subseteq\\mathbb{R}^n\\), \\(x \\S\\) \\(f : S \\\\mathbb{R}^m\\). Suppose , \\(\\) \\(j\\), \\(D_j f_i(x)\\) exists \\(x \\mapsto D_j f_i(x)\\) continuous \\(S\\). \\(f\\) differentiable \\(x\\).Theorem 1.31  (Implicit Function) Let \\(F : \\mathbb{R}^2 \\\\mathbb{R}\\). Suppose \\(D_1F(x,y)\\) \\(D_2F(x,y)\\) exist continuous open \\(U\\) containing \\((, b) \\\\mathbb{R}^2\\). Suppose \\(F(, b) = 0\\) \\(D_2F(, b) \\neq 0\\). exists \\(\\varepsilon > 0\\) \\(g : B_\\varepsilon \\\\mathbb{R}\\) \\(g() = b\\), \\(F(x, g(x)) = 0\\) \\(x \\B_\\varepsilon ()\\) \n\\[\ng'() = -\\frac{D_1F(,b)}{D_2F(,b)}.\n\\]Theorem 1.32  (Implicit Function) Suppose \\(F : \\mathbb{R}^{n + m} \\\\mathbb{R}^m\\) differentiable open set \\(U\\) containing \\((, b) \\\\mathbb{R}^{n + m}\\) suppose entries \\(J_F (x,y)\\) continuous \\((x,y) \\U\\). Let \\(A_{m\\times n}\\) \\(B_{m\\times m}\\) defined \\(J_F (, b)\\) \n\\[\n=\n\\begin{bmatrix}\nD_1 F_1(,b) & \\cdots & D_n F_1(,b) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nD_1 F_m(,b) & \\cdots & D_n F_m(,b)\n\\end{bmatrix}_{m\\times n} \\\\ \\\\\nB =\n\\begin{bmatrix}\nD_{n+1} F_1(,b) & \\cdots & D_{n+m} F_1(,b) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nD_{n+1} F_m(,b) & \\cdots & D_{n+m} F_m(,b)\n\\end{bmatrix}_{m\\times m}\n\\]\nsuppose \\(F(,b) = 0\\) \\(B\\) non-singular. exists \\(\\varepsilon > 0\\) \\(g : B_\\varepsilon () \\\\mathbb{R}^m\\) \\(g() = b\\), \\(F(x, g(x)) = 0\\) \\(x \\B_\\varepsilon ()\\) \\(J_g () = -B^{-1}_{m\\times m} A_{m\\times n}\\).Theorem 1.33  (Inverse Function) Suppose \\(f : \\mathbb{R}^m \\\\mathbb{R}^m\\) differentiable open set \\(U\\) containing \\(b \\\\mathbb{R}^m\\). Suppose partials \\(f\\) continuous \\(U\\). Suppose \\(f(b) = \\) \\(J_f(b)\\) non-singular. exists \\(\\varepsilon > 0\\) \\(g : B_\\varepsilon () \\\\mathbb{R}^m\\) \\(g() = b\\), \\(f(g(x)) = x\\) \\(x \\B_\\varepsilon ()\\), \\(g\\) differentiable \\(\\) \\(J_g () = J_f (b)^{-1}\\).Theorem 1.34  (Weierstrass) Let \\(S \\subseteq \\mathbb{R}^n\\) compact let \\(f : S \\\\mathbb{R}\\) continuous. exists \\(x'\\) \\(x''\\) \\(S\\) , \\(x \\S\\), \\(f(x') \\leq f(x) \\leq f(x'')\\).Theorem 1.35  Suppose \\(f : \\mathbb{R}^n \\\\mathbb{R}\\) attains maximum minimum open set \\(U\\) \\(x \\U\\). \\(D_i f(x)\\) exists, \\(D_i f(x) = 0\\).Lemma 1.3  (\"Rolle's) Suppose \\(f : [, b] \\\\mathbb{R}\\) continuous \\(x \\[, b]\\) \\(f\\) differentiable \\(x \\(, b)\\). Furthermore, suppose \\(f() = 0 = f(b)\\). exists \\(c \\(, b)\\) \\(f'(c) = 0\\).Theorem 1.36  (Mean Value) Suppose \\(f : [, b] \\\\mathbb{R}\\) continuous \\(x \\[, b]\\) \\(f\\) differentiable \\(x \\(, b)\\). exists \\(c \\(, b)\\) \n\\[\nf'(c) = \\frac{f(b) - f()}{b - }.\n\\]Theorem 1.37  (Mean Value) Let \\(U\\) open set \\(\\mathbb{R}^n\\) containing \\(\\) \\(b\\). Suppose \\((1 - t)+ tb \\U\\) whenever \\(t \\[0, 1]\\). Suppose \\(f : U \\\\mathbb{R}\\) differentiable \\(x \\U\\). exists \\(t' \\(0,1)\\) \\(\\nabla f(c') \\cdot (b - ) = f(b) - f()\\) \\(c' = (1 - t')+ t'b\\).Definition 1.46  (Hessian Matrix) Let \\(U\\) open set \\(\\mathbb{R}^n\\) let \\(f : U \\\\mathbb{R}\\). Hessian \\(f\\) \\(x \\U\\) matrix\n\\[\nH_f (x) :=\n\\begin{bmatrix}\nD_1 D_1 f(x) & \\cdots & D_n D_1 f(x) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nD_1 D_n f(x) & \\cdots & D_n D_n f(x)\n\\end{bmatrix}_{n \\times n}\n\\]Theorem 1.38  Suppose \\(U\\) open \\(\\mathbb{R}^n\\), \\(f : U \\\\mathbb{R}^n\\) \\(x \\mapsto D_i D_j f(x)\\) continuous \\(x \\U\\) every \\(\\) \\(j\\). \\(H_f (x)\\) symmetric \\(x \\U\\).Theorem 1.39  Suppose \\(U \\subseteq \\mathbb{R}^n\\) open set containing \\(\\) \\(b\\) \\(f : U \\\\mathbb{R}\\). Furthermore, suppose \\(U\\) contains \\((1 - t) + tb\\) \\(t \\[0, 1]\\). Suppose \\(f\\) \\(\\nabla f\\) differentiable \\(U\\). \\(t' \\(0, 1)\\) \n\\[\nf(b) = f() + \\nabla f() \\cdot (b - ) + \\frac{1}{2} (b - )^T H_f (c') (b - )\n\\]\n\\(c' = (1 - t')+ t'b\\).Definition 1.47  (Convex Function) Let \\(S \\subseteq \\mathbb{R}^n\\) convex. function \\(f : S \\\\mathbb{R}\\) convex , \\(t \\[0,1]\\) \\(x, x' \\S\\), \\(f((1 - t)x + tx') \\leq (1 - t) f(x) + tf(x')\\). function \\(f : S \\\\mathbb{R}\\) strictly convex , \\(t \\(0,1)\\) \\(x, x' \\S\\) \\(x \\neq x'\\), \\(f((1 - t)x + tx') < (1 - t) f(x) + tf(x')\\).Theorem 1.40  Suppose \\(U \\subseteq \\mathbb{R}^n\\) open convex \\(f : U \\\\mathbb{R}\\) differentiable. \\(f\\) convex \\(f(x') \\geq f(x) + \\nabla f(x) \\cdot (x' - x)\\) \\(x, x' \\U\\).\\(f\\) strictly convex \\(f(x') > f(x) + \\nabla f(x) \\cdot (x' - x)\\) \\(x, x' \\U\\) \\(x \\neq x'\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"correspondences","chapter":"1 Consumer and Producer Theory","heading":"1.4 Correspondences","text":"Definition 1.48  (Correspondence) correspondence \\(X\\) \\(Y\\) (Euclidean) map \\(\\varphi : X \\rightrightarrows Y\\) associates every \\(x \\X\\) non-empty set \\(\\varphi (x) \\subseteq Y\\). Namely, correspondence \\(X\\) \\(Y\\) function \\(X\\) \\(\\{\\subseteq Y : \\neq \\varnothing\\}\\).Definition 1.49  (Closed-Valued / Open-Valued / Compact-Valued / ... ) correspondence \\(\\varphi\\) closed-valued \\(\\varphi(x)\\) closed subset \\(Y\\) every \\(x \\X\\). similarly define correspondences open-valued, compact-valued, etcetera.Definition 1.50  (Graph) graph \\(\\varphi\\) set\n\\[\nG_\\varphi := \\{ (x, y) \\X \\times Y : y \\\\varphi(x) \\} .\n\\]Proposition 1.5  \\(\\varphi\\) closed / open \\(G_\\varphi\\) closed / open subset \\(X \\times Y\\).Definition 1.51  (Inverses) Let \\(U \\subseteq Y\\). strong weak inverses \\(U\\) \\(\\varphi\\) defined respectively \n\\[\n\\varphi_s^{-1} (U) := \\{x \\X : \\varphi(x) \\subseteq U\\} \\\\\n\\varphi_w^{-1} (U) := \\{x \\X : \\varphi(x) \\cap U \\neq \\varnothing\\}\n\\]Proposition 1.6  \\(\\varphi_s^{-1} (U) \\subseteq \\varphi_w^{-1} (U)\\).\\(\\varphi\\) \\(\\mid \\varphi (x) \\mid = 1\\) every \\(x\\), \\(\\varphi_s^{-1} (U) = \\varphi_w^{-1} (U)\\).general, \\(\\varphi_w^{-1} (U) = \\left(\\varphi_s^{-1} \\left(U^C\\right)\\right)^C\\).Definition 1.52  (Upper Hemi-Continuity) correspondence \\(\\varphi : X \\rightrightarrows Y\\) upper hemi-continuous (UHC) \\(x \\X\\) , whenever \\(U\\) open subset \\(Y\\) \\(x \\\\varphi_s^{-1} (U)\\), \\(B(x, \\delta) \\subseteq \\varphi_s^{-1} (U)\\) \\(\\delta > 0\\). \\(\\varphi\\) UHC UHC every \\(x \\X\\).Proposition 1.7  \\(\\varphi\\) UHC strong inverse open set \\(Y\\) \\(\\varphi\\) open \\(X\\).Definition 1.53  (Lower Hemi-Continuity) correspondence \\(\\varphi : X \\rightrightarrows Y\\) lower hemi-continuous (LHC) \\(x \\X\\) , whenever \\(U\\) open subset \\(Y\\) \\(x \\\\varphi_w^{-1} (U)\\), \\(B(x, \\delta) \\subseteq \\varphi_w^{-1} (U)\\) \\(\\delta > 0\\). \\(\\varphi\\) LHC LHC every \\(x \\X\\).Proposition 1.8  \\(\\varphi\\) LHC weak inverse open set \\(Y\\) \\(\\varphi\\) open \\(X\\).Definition 1.54  (Continuity) correspondence continuous LHC UHC.Theorem 1.41  Let \\(f : X \\Y\\) function let \\(\\varphi : X \\rightrightarrows Y\\) defined \\(\\varphi (x) = \\{f(x)\\}\\) every \\(x\\). following equivalent:\\(\\varphi\\) UHC.\\(\\varphi\\) LHC.\\(f\\) continuous.Proposition 1.9  function \\(f : X \\Y\\) upper / lower semicontinuous, correspondence \\(\\varphi : X \\rightrightarrows Y\\) defined \\(\\varphi (x) = \\{f(x)\\}\\) need upper / lower hemicontinuous.Remark. Closedness UHC nested.Theorem 1.42  \\(\\varphi : X \\rightrightarrows Y\\) UHC closed-valued, also closed.Theorem 1.43  \\(\\varphi : X \\rightrightarrows Y\\) closed \\(Y\\) compact, \\(\\varphi\\) UHC.Theorem 1.44  \\(\\varphi\\) open, LHC.Proposition 1.10  every \\(x \\X\\), every sequence \\(\\{x_n\\}\\) converging \\(x\\) every sequence \\(\\{y_n\\}\\) \\(y_n \\\\varphi(x_n)\\) every \\(n\\), exists convergent subsequence \\(\\{y_n\\}\\) limit \\(\\varphi(x)\\).Proposition 1.11  every \\(x \\X\\), every sequence \\(\\{x_n\\}\\) converging \\(x\\) every \\(y \\\\varphi(x)\\), exists sequence \\(\\{y_n\\}\\) converging \\(y\\) \\(y_n \\\\varphi(x_n)\\) every \\(n\\).Theorem 1.45  \\(\\varphi\\) satisfies proposition 1.10, \\(\\varphi\\) UHC.\\(\\varphi\\) UHC compact-valued, \\(\\varphi\\) satisfies proposition 1.10.Theorem 1.46  \\(\\varphi\\) satisfies proposition 1.11 \\(\\varphi\\) satisfies LHC.Corollary 1.6  \\(\\varphi : X \\rightrightarrows Y\\) UHC compact-valued \\(K \\subseteq X\\) compact, \\(\\displaystyle \\varphi(K) := \\bigcup_{x \\K} \\varphi (x)\\) compact.Remark. subset \\(S\\) metric space compact every sequence \\(S\\) subsequence converges point \\(S\\).Theorem 1.47  (Maximum) Let \\(T\\) \\(X\\) Euclidean sets. Let \\(\\varphi : T \\rightrightarrows X\\) \\(f : X \\times T \\\\mathbb{R}\\). every \\(t \\T\\), consider problem\n\\[\n\\max_{x \\\\varphi(t)} f(x, t).\n\\]\nDefine \\(\\displaystyle \\mu (t) = \\mathop{\\mathrm{\\arg\\!\\max}}_{x \\\\varphi(t)} f(x, t)\\). Suppose \\(\\mu(t) \\neq \\varnothing\\) every \\(t\\).can define function \\(g : T \\\\mathbb{R}\\) \\(g(t) = f(x, t)\\) \\(x \\\\mu(t)\\).\\(\\varphi\\) compact-valued, UHC LHC \\(f\\) continuous, \\(\\mu : T \\rightrightarrows X\\) compact-valued UHC, \\(g : T \\\\mathbb{R}\\) continuous.","code":""},{"path":"consumer-and-producer-theory.html","id":"convexity","chapter":"1 Consumer and Producer Theory","heading":"1.5 Convexity","text":"Definition 1.55  (Convex Set) non-empty set \\(S \\subseteq \\mathbb{R}^m\\) convex , every \\(x, x' \\S\\) every \\(t \\[0, 1]\\), \\(tx + (1 - t)x' \\S\\).Definition 1.56  (Convex Combination) convex combination vectors \\(x_1, \\dots, x_n\\) vector form \\(\\displaystyle \\sum_{= 1}^n \\alpha_i x_i\\) \\(\\alpha_1, \\dots, \\alpha_n\\) non-negative numbers add 1.Theorem 1.48  \\(S \\subseteq \\mathbb{R}^m\\) convex \\(S = \\tilde{S}\\) \n\\[\n\\tilde{S} := \\left\\{ \\sum_{= 1}^n \\alpha_i x_i : n \\\\mathbb{N},\\ x_i \\S,\\ \\alpha_i \\geq 0,\\ \\forall ,\\ \\sum_{= 1}^n \\alpha_i = 1 \\right\\};\n\\]\n.e., contains convex combinations elements.Proposition 1.12  Arbitrary intersections convex sets convex.\\(S + T := \\{ s + t : s \\S, t \\T\\}\\) convex \\(S\\) \\(T\\) convex.every scalar \\(\\lambda \\geq 0\\), \\(\\lambda S := \\{\\lambda s : s \\S\\}\\) convex \\(S\\) convex.closure interior convex set convex using Euclidean metric.Definition 1.57  (Convex Hull) convex hull set \\(S \\subseteq \\mathbb{R}^m\\) “smallest” convex superset \\(S\\). Namely,\n\\[\n\\text{co}S := \\bigcap \\left\\{ G \\subseteq \\mathbb{R}^m : S \\subseteq G \\wedge G\\ \\text{convex} \\right\\}.\n\\]Proposition 1.13  \\(\\text{co}S\\) convex \\(S \\subseteq \\text{co}S\\).Theorem 1.49  \\[\n\\text{co}S = \\tilde{S}\n\\]Proposition 1.14  \\(S\\) convex \\(\\text{co}S \\subseteq S\\).Theorem 1.50  (Carathéodory) Let \\(S \\subseteq \\mathbb{R}^m\\) non-empty. \\(x \\\\text{co}S\\), \\(x\\) can written convex combination \\(m + 1\\) members \\(S\\), .e., exists \\(x_1, x_2, \\dots, x_{m+1} \\S\\) \\(\\alpha_1, \\alpha_2, \\dots, \\alpha_{m+1} \\geq 0\\) \\(\\displaystyle \\sum_{= 1}^{m + 1} \\alpha_i = 1\\) \\(\\displaystyle x = \\sum_{= 1}^{m + 1} \\alpha_i x_i\\).Proposition 1.15  \\(\\displaystyle \\text{co}\\left( \\sum_{= 1}^n S_i \\right) = \\sum_{= 1}^n \\text{co}S_i\\).\\(\\subset \\mathbb{R}^m\\) open, \\(\\text{co}\\) open.convex hull closed set \\(\\mathbb{R}^m\\) need closed. \\(K \\subset \\mathbb{R}^m\\) compact, \\(\\text{co}K\\) compact.Theorem 1.51  (Shapley-Folkman) Let \\(S_i \\subseteq \\mathbb{R}^m\\) every \\(= 1, \\dots, n\\), let \\(\\displaystyle x \\\\text{co}\\sum_{= 1}^n S_i\\). exists \\(x_1, \\dots, x_n\\) \\(x_i \\\\text{co}S_i\\) every \\(\\),\\(\\displaystyle x = \\sum_{= 1}^n x_i\\), \\(\\# \\{: x_i \\notin S_i\\} \\leq m\\).Remark. \\(x \\\\mathbb{R}^m\\) can written \\(\\displaystyle x = \\sum_{= 1}^k \\alpha_i x_i\\) \\(\\alpha_1, \\dots, \\alpha_k \\\\mathbb{R}_+\\), \\(x_1, \\dots, x_k \\\\mathbb{R}^m\\) , \\(k > m\\), exist \\(\\beta_1, \\dots, \\beta_k \\\\mathbb{R}_+\\) \\(\\# \\{: \\beta_i > 0\\} \\leq m\\) \\(\\displaystyle x = \\sum_{= 1}^k \\beta_i x_i\\).Scalars \\(\\alpha_i\\) \\(\\beta_i\\) need add 1.Definition 1.58  (Hyperplane) Fix \\(p \\\\mathbb{R}^m\\) \\(\\alpha \\\\mathbb{R}\\). hyperplane formed \\(p\\) \\(\\alpha\\) \n\\[\nH(p; \\alpha) := \\left\\{ x \\\\mathbb{R}^m : p \\cdot x = \\alpha \\right\\}\n\\]\n\\(p\\) called normal vector \\(H(p; \\alpha)\\).Theorem 1.52  (Minkowski) Let \\(S \\subseteq \\mathbb{R}^m\\) non-empty, convex closed, let \\(\\bar{x} \\notin S\\). exists \\(p \\\\mathbb{R}^m \\setminus \\{0\\}\\) \\(x_0 \\S\\) \\(p \\cdot \\bar{x} > p \\cdot x_0 \\geq p \\cdot x\\) every \\(x \\S\\).Theorem 1.53  Suppose \\(S \\subseteq \\mathbb{R}^m\\) non-empty convex, \\(x_n\\) sequence \\(\\mathbb{R}^m\\setminus \\text{cl}S\\). \\(x_n \\\\bar{x}\\), exists \\(p \\\\mathbb{R}^m \\setminus \\{0\\}\\) , every \\(x \\S\\), \\(p \\cdot x \\leq p \\cdot \\bar{x}\\).Theorem 1.54  (Supporting Hyperplane) Suppose \\(S \\subseteq \\mathbb{R}^m\\) non-empty convex. \\(\\bar{x} \\\\partial S\\), exists \\(p \\\\mathbb{R}^m \\setminus \\{0\\}\\) \\(p \\cdot \\bar{x} \\geq p \\cdot x\\) every \\(x \\S\\).Theorem 1.55  Suppose \\(S \\subseteq \\mathbb{R}^m\\) non-empty convex set, let \\(\\bar{x} \\notin S\\). exists \\(p \\\\mathbb{R}^m \\setminus \\{0\\}\\) \\(p \\cdot x \\leq p \\cdot \\bar{x}\\) every \\(x \\S\\).Theorem 1.56  (Separating Hyperplane) Let \\(S\\) \\(T\\) disjoint convex subsets \\(\\mathbb{R}^m\\). exists \\(p \\\\mathbb{R}^m \\setminus \\{0\\}\\) \\(p \\cdot s \\leq p \\cdot t\\) every \\((s, t) \\S \\times T\\).Definition 1.59  (Convex Function) Let \\(S \\subseteq \\mathbb{R}^n\\) convex. function \\(f : S \\\\mathbb{R}\\) convex \\(f(tx + (1 - t)y) \\leq tf(x) + (1 - t)f(y)\\), \\(\\forall x, y \\S\\), \\(t \\[0, 1]\\). Similarly, \\(f\\) said strictly convex \\(f(tx + (1 - t)y) < tf(x) + (1 - t)f(y)\\), \\(\\forall x, y \\S\\), \\(t \\(0, 1)\\).Definition 1.60  (Concave Function) Let \\(S \\subseteq \\mathbb{R}^n\\) convex. function \\(f : S \\\\mathbb{R}\\) concave \\(f(tx + (1 - t)y) \\geq tf(x) + (1 - t)f(y)\\), \\(\\forall x, y \\S\\), \\(t \\[0, 1]\\). Similarly, \\(f\\) said strictly concave \\(f(tx + (1 - t)y) > tf(x) + (1 - t)f(y)\\), \\(\\forall x, y \\S\\), \\(t \\(0, 1)\\).Definition 1.61  (Subgradient) Suppose \\(S \\subseteq \\mathbb{R}^n\\) convex \\(f : S \\\\mathbb{R}\\). vector \\(p \\\\mathbb{R}^n\\) subgradient \\(f\\) \\(x \\S\\) \n\\[\nf(y) \\geq f(x) + p \\cdot (x - y),\n\\]\n\\(y \\S\\).Definition 1.62  (Supergradient) Suppose \\(S \\subseteq \\mathbb{R}^n\\) convex \\(f : S \\\\mathbb{R}\\). vector \\(p \\\\mathbb{R}^n\\) supergradient \\(f\\) \\(x \\S\\) \n\\[\nf(y) \\leq f(x) + p \\cdot (x - y),\n\\]\n\\(y \\S\\).Theorem 1.57  Suppose \\(S \\subseteq \\mathbb{R}^n\\) convex \\(f : S \\\\mathbb{R}\\). \\(f\\) subgradient every \\(x \\S\\), \\(f\\) convex.Theorem 1.58  Suppose \\(S \\subseteq \\mathbb{R}^n\\) convex, \\(f : S \\\\mathbb{R}\\) convex, \\(x \\\\text{int}(S)\\). \\(f\\) continuous \\(x\\).Theorem 1.59  Suppose \\(S \\subseteq \\mathbb{R}^n\\) convex, \\(f : S \\\\mathbb{R}\\) convex, \\(x \\\\text{int}(S)\\). \\(f\\) subgradient \\(x\\).Theorem 1.60  Suppose \\(S \\subseteq \\mathbb{R}^n\\) convex, \\(f : S \\\\mathbb{R}\\) convex, \\(x \\\\text{int}(S)\\), \\(f\\) differentiable \\(x\\). \\(\\nabla f(x)\\) unique subgradient \\(f\\) \\(x\\).Theorem 1.61  Suppose \\(S \\subseteq \\mathbb{R}^n\\) convex \\(f : S \\\\mathbb{R}\\). \\(p_1\\) subgradient \\(f\\) \\(x_1\\) \\(p_2\\) subgradient \\(f\\) \\(x_2\\), \\((p_1 - p_2) \\cdot (x_1 - x_2) \\geq 0\\).Theorem 1.62  Suppose \\(S \\subseteq \\mathbb{R}^n\\) open convex, \\(f : S \\\\mathbb{R}\\) differentiable convex. \\(\\left(\\nabla f(x_1) - \\nabla f(x_2)\\right) \\cdot (x_1 - x_2) \\geq 0\\).","code":""},{"path":"consumer-and-producer-theory.html","id":"support-functions","chapter":"1 Consumer and Producer Theory","heading":"1.6 Support Functions","text":"","code":""},{"path":"consumer-and-producer-theory.html","id":"maximization","chapter":"1 Consumer and Producer Theory","heading":"Maximization","text":"Definition 1.63  (Barrier Cone) non-empty \\(S \\subseteq \\mathbb{R}^n\\), barrier cone \\(S\\) defined \n\\[\nb(S) := \\left\\{ p \\\\mathbb{R}^n : \\max_{x \\S}\\ \\text{well-defined} \\right\\}.\n\\]Definition 1.64  (Support Function) non-empty \\(S \\subseteq \\mathbb{R}^n\\), support function \\(S\\) function \\(\\sigma_S : b(S) \\\\mathbb{R}\\) \n\\[\n\\sigma_S (p) := \\max_{x \\S} p \\cdot x.\n\\]Remark (Cone). \\(p \\b(S)\\) \\(\\lambda \\\\mathbb{R}_+\\), \\(\\lambda p \\b(S)\\) well. Hence \\(b(S)\\) cone , particular, \\(0 \\b(S)\\).Proposition 1.16  \\(b(S)\\) need convex even \\(S\\) .Theorem 1.63  Let \\(S \\subseteq \\mathbb{R}^n\\) non-empty.\\(x_1\\) solves \\(\\displaystyle \\max_{x \\S} p_1 \\cdot x\\) \\(x_2\\) solves \\(\\displaystyle \\max_{x \\S} p_2 \\cdot x\\), \\((x_1 - x_2)\\cdot(p_1 - p_2) \\geq 0\\). known monotonicity solutions.\\(\\sigma_S(tp) = t\\sigma_S(p)\\) every \\(p \\b(S)\\) \\(t \\geq 0\\). Namely, homogeneity degree 1.\\(b(S)\\) convex, \\(\\sigma_S\\) convex.Suppose \\(S \\subseteq \\mathbb{R}^n\\) closed convex, \\(p \\b(S)\\). \\(x_p\\) solves \\(\\displaystyle \\max_{x \\S} p \\cdot x\\) \\(x_p\\) subgradient \\(\\sigma_S\\) \\(p\\).Suppose \\(S \\subseteq \\mathbb{R}^n\\) closed convex, \\(p \\\\text{int}(b(S))\\) \\(\\sigma_S\\) differentiable \\(p\\). \\(x_p\\) solves \\(\\displaystyle \\max_{x \\S} p \\cdot x\\) \\(x_p = \\nabla \\sigma_S (p)\\). Also known Hotelling’s Lemma profit maximization.","code":""},{"path":"consumer-and-producer-theory.html","id":"minimization","chapter":"1 Consumer and Producer Theory","heading":"Minimization","text":"Definition 1.65  (Barrier Cone) non-empty \\(S \\subseteq \\mathbb{R}^n\\), barrier cone \\(S\\) defined \n\\[\nb^-(S) := \\left\\{ p \\\\mathbb{R}^n : \\min_{x \\S}\\ \\text{well-defined} \\right\\}.\n\\]Definition 1.66  (Support Function) non-empty \\(S \\subseteq \\mathbb{R}^n\\), support function \\(S\\) function \\(\\tau_S : b^-(S) \\\\mathbb{R}\\) \n\\[\n\\tau_S (p) := \\min_{x \\S} p \\cdot x.\n\\]Remark (Cone). Note \\(b^-(S) = -b(S) = \\left\\{ p \\\\mathbb{R}^n : -p \\b(S) \\right\\}\\). Furthermore, \\(\\tau_S (p) = -\\sigma_S (-p)\\).Theorem 1.64  Let \\(S \\subseteq \\mathbb{R}^n\\) non-empty.\\(x_1\\) solves \\(\\displaystyle \\min_{x \\S} p_1 \\cdot x\\) \\(x_2\\) solves \\(\\displaystyle \\min_{x \\S} p_2 \\cdot x\\), \\((x_1 - x_2)\\cdot(p_1 - p_2) \\leq 0\\). known monotonicity solutions.\\(\\tau_S(tp) = t\\tau_S(p)\\) every \\(p \\b^-(S)\\) \\(t \\geq 0\\). Namely, homogeneity degree 1.\\(b^-(S)\\) convex, \\(\\tau_S\\) concave.Suppose \\(S \\subseteq \\mathbb{R}^n\\) closed convex, \\(p \\b^-(S)\\). \\(x_p\\) solves \\(\\displaystyle \\min_{x \\S} p \\cdot x\\) \\(x_p\\) supergradient \\(\\tau_S\\) \\(p\\).Suppose \\(S \\subseteq \\mathbb{R}^n\\) closed convex, \\(p \\\\text{int}(b^-(S))\\) \\(\\tau_S\\) differentiable \\(p\\). \\(x_p\\) solves \\(\\displaystyle \\min_{x \\S} p \\cdot x\\) \\(x_p = \\nabla \\tau_S (p)\\). Also known Shephard’s Lemma cost minimization.","code":""},{"path":"consumer-and-producer-theory.html","id":"nonlinear-programming","chapter":"1 Consumer and Producer Theory","heading":"1.7 Nonlinear Programming","text":"","code":""},{"path":"economics-2.html","id":"economics-2","chapter":"2 Economics 2","heading":"2 Economics 2","text":"section still development.wish contribute section, feel free email carloselezamaj@gmail.com.","code":""},{"path":"economics-3.html","id":"economics-3","chapter":"3 Economics 3","heading":"3 Economics 3","text":"can find cheat sheet pdf file Spanish.\nJust click image .","code":""},{"path":"economics-4.html","id":"economics-4","chapter":"4 Economics 4","heading":"4 Economics 4","text":"section still development.","code":""},{"path":"economics-4.html","id":"general-equilibrium","chapter":"4 Economics 4","heading":"4.1 General Equilibrium","text":"","code":""},{"path":"economics-4.html","id":"pure-exchange-economies","chapter":"4 Economics 4","heading":"Pure Exchange Economies","text":"","code":""},{"path":"economics-4.html","id":"initial-assumptions","chapter":"4 Economics 4","heading":"Initial Assumptions","text":"\\(m\\) consumers \\(\\mathscr{} = \\{1, \\dots, m\\}\\).\\(n\\) goods \\(\\mathscr{L}= \\{1, \\dots, n\\}\\).utility function \\(u_i : \\mathbb{R}^\\mathscr{L}\\\\mathbb{R}\\) represents preferences \\(\\)th consumer.consumer can consume goods \\(x_i \\\\mathbb{R}^\\mathscr{L}_+\\).consumer initial endowment \\(w_i \\\\mathbb{R}^\\mathscr{L}_+\\).ordered pair \\((u_i, w_i)\\) describes consumer.utility functions represent neoclassical preferences.Proposition 4.1  \\(x \\succ_i \\mathrm{y}\\), \\(u_i(x) > u_i(\\mathrm{y})\\).Definition 4.1  (Exchange Economy) pure exchange economy \n\\[\n\\mathscr{E} = \\left\\langle \\mathscr{}, (u_i, w_i)_{\\\\mathscr{}} \\right\\rangle,\n\\]\n\\(\\mathscr{}\\) set agents; \\(u_i\\) \\(w_i\\) utility function initial endowment \\(\\)th consumer, respectively.Definition 4.2  (Total Endowment) \\[\n\\Omega = \\sum_{\\\\mathscr{}} w_i.\n\\]Definition 4.3  (Resource Allocation) resource allocation denoted \\(X = (x_1, x_2, \\dots, x_m)\\), \\(x_i \\\\mathbb{R}^\\mathscr{L}_+\\).Definition 4.4  (Feasible Allocation) feasible allocation \\(\\mathscr{F}\\) economy \\(\\mathscr{E}\\) defined \n\\[\n\\mathscr{F}= \\left\\{ X = (x_1, x_2, \\dots, x_m)\\ :\\ x_i \\\\mathbb{R}^\\mathscr{L}_i,\\ \\sum_{\\\\mathscr{}} x_i = \\sum_{\\\\mathscr{}} w_i \\right\\}.\n\\]Definition 4.5  (Pareto Efficiency) Let \\(\\mathscr{E}\\) exchange economy. feasible allocation resources \\(X\\) said Pareto efficient feasible allocation \\(\\hat{X}\\) , every agent \\(\\mathscr{}\\), \\(u_i(\\hat{x}_i) \\geq u_i(x_i)\\) , least one agent \\(j\\), \\(u_j(\\hat{x}_j) > u_j(x_j)\\).Definition 4.6  (Pareto Dominance) Let \\(X\\) \\(\\hat{X}\\) two feasible allocations. say \\(\\hat{X}\\) Pareto dominates \\(X\\) \n\\[\nu_i(\\hat{x}_{,1}, \\dots, \\hat{x}_{,n}) \\geq u_i(x_{,1}, \\dots, x_{,n}), \\quad \\forall \\\\mathscr{},\n\\]\nleast one consumer \\(j\\) \n\\[\nu_j(\\hat{x}_{j,1}, \\dots, \\hat{x}_{j,n}) > u_i(x_{j,1}, \\dots, x_{j,n}).\n\\]Definition 4.7  (Contract Curve) set Pareto allocations known contract curve.\nFigure 4.1: Edgeworth Box\n","code":""},{"path":"economics-4.html","id":"general-case","chapter":"4 Economics 4","heading":"General Case","text":"\\[\n\\begin{align*}\n\\max_{ x_1,\\ \\dots\\ ,\\ x_m } \\quad & u_1(x_{1,1}, \\dots, x_{1,n}) \\\\\n\\text{subject } \\quad & u_2(x_{2,1}, \\dots, x_{2,n}) \\geq \\bar{u}_2, \\\\\n& \\quad \\vdots \\\\\n& u_m(x_{m,1}, \\dots, x_{m,n}) \\geq \\bar{u}_m; \\\\\n& \\sum_{\\\\mathscr{}} x_{,1} \\leq \\sum_{\\\\mathscr{}} w_{,1}, \\\\\n& \\quad \\vdots \\\\\n& \\sum_{\\\\mathscr{}} x_{,n} \\leq \\sum_{\\\\mathscr{}} w_{,n}.\n\\end{align*}\n\\]Theorem 4.1  Let utility functions strictly increasing quasiconcave, \\(\\hat{X}\\) feasible interior allocation. \\(\\hat{X}\\) Pareto efficient \\(\\hat{X}\\) exhausts resources , pairs goods \\((\\ell, \\ell')\\),\n\\[\n\\text{MRS}(\\ell, \\ell')(\\hat{x}_{1,1}, \\dots, \\hat{x}_{1,n}) = \\cdots = \\text{MRS}(\\ell, \\ell')(\\hat{x}_{m,1}, \\dots, \\hat{x}_{m,n}).\n\\]","code":""},{"path":"economics-4.html","id":"competitive-equilibrium","chapter":"4 Economics 4","heading":"Competitive Equilibrium","text":"","code":""},{"path":"economics-4.html","id":"initial-assumptions-1","chapter":"4 Economics 4","heading":"Initial Assumptions","text":"market good.Every agent can access market without cost.single price good.consumers know price.consumer can sell initial endowment market use income buy goods services.Consumers seek maximize utility given budget restriction, independently everyone else .centralized mechanism.People may know others’ preferences endowments.perfect competition (namely, everyone price-taker).Prices source information agents.Definition 4.8  (Competitive Equilibrium) ordered pair allocation price vector, \\(\\left( X^*, p = (p_1, \\dots, p_n) \\right)\\), called competitive equilibrium following conditions hold:\\(\\forall \\\\mathscr{}\\), \\(x_i^* = (x_{,1}^*, \\dots, x_{,n}^*)\\) solves following maximization problem:\n\\[\n\\begin{align*}\n\\max_{ x_i } \\quad & u_i(x_i) \\\\\n\\text{subject } \\quad & wh + \\langle p, x_i \\rangle \\leq \\langle p, w_i \\rangle = \\sum_{\\ell \\\\mathscr{L}} p_{\\ell} w_{,\\ell}.\n\\end{align*}\n\\]Markets clear, .e. \\(\\displaystyle \\sum_{\\\\mathscr{}} x_{,\\ell}^* = \\sum_{\\\\mathscr{}} w_{,\\ell}\\), \\(\\forall \\ell \\\\mathscr{L}\\).Proposition 4.2  Given least one consumer strictly monotonic preferences. , \\((X^*, p)\\) competitive equilibrium, \\(p_1, p_2, \\dots, p_n >0\\).Proposition 4.3  Given least one consumer weakly monotonic preferences. , \\((X^*, p)\\) competitive equilibrium, least one \\(\\ell\\), \\(p_\\ell > 0\\).Proposition 4.4  Let \\((X^*, p)\\) competitive equilibrium. \\((X^*, cp)\\) also competitive equilibrium, \\(\\forall c \\\\mathbb{R}_+\\).Theorem 4.2  (Walras's Law) consumer \\(\\) weakly monotonic preferences \\(\\hat{x}_i \\x_i^*(p)\\), \n\\[\n\\langle p, \\hat{x}_i \\rangle = \\sum_{\\ell \\\\mathscr{L}} p_{\\ell} \\hat{x}_{,\\ell} = \\sum_{\\ell \\\\mathscr{L}} p_{\\ell} w_{,\\ell} = \\langle p, w_i \\rangle.\n\\]Corollary 4.1  (Walras's Law) Given weakly monotonic utility functions \\(p = (p_1, \\dots, p_n)\\) \\(p_\\ell > 0\\). \\((X^*, p)\\) maximization condition holds, \\(\\forall \\\\mathscr{}\\), markets clear \\(\\forall \\ell = 1, 2, \\dots, n - 1\\), market clearing condition holds commodity \\(n\\) well.Theorem 4.3  (Fixed Point) continuous function \\(f : \\triangle^{n - 1} \\\\triangle^{n - 1}\\), exists point \\(p^* = (p_1^*, p_2^*, \\dots, p_n^*)\\) \\(f(p^*) = p^*\\), \n\\[\n\\triangle^{n - 1} = \\left\\{ (p_1, p_2, \\dots, p_n) \\\\mathbb{R}^\\mathscr{L}_+ : \\sum_{\\ell \\\\mathscr{L}} p_\\ell = 1 \\right\\}.\n\\]Definition 4.9  (Shortage) define shortage excess demand \n\\[\nZ(p) = (z_1(p), z_2(p), \\dots, z_n(p)) = \\sum_{\\\\mathscr{}} x^*_i (p) - \\sum_{\\\\mathscr{}} w_i.\n\\]Proposition 4.5  \\(p\\) competitive equilibrium \\(Z(p) = 0\\).Remark (Excess Demand Properties). Continuous \\(p\\).Zero degree homogeneity.\\(\\langle p, Z(p) \\rangle = 0\\).Proposition 4.6  equilibrium unique.Theorem 4.4  (Welfare ) Given pure exchange economy consumers weakly monotonic utility functions. \\((X^*, p)\\) competitive equilibrium, \\(X^*\\) Pareto efficient allocation.Theorem 4.5  (Welfare II) Given economy \\(\\displaystyle \\mathscr{E} = \\langle \\mathscr{}, (u_i, w_i)_{\\\\mathscr{}} \\rangle\\) consumers weakly monotonic quasiconcave utility functions. \\((x_1, x_2, \\dots, x_m)\\) Pareto optimal allocation, exists redistribution resources \\((\\hat{w}_1, \\hat{w}_2, \\dots, \\hat{w}_m)\\) prices \\(p = (p_1, p_2, \\dots, p_n)\\) :\\(\\displaystyle \\sum_{\\\\mathscr{}} \\hat{w}_i = \\sum_{\\\\mathscr{}} w_i\\).\\((p, (x_1, x_2, \\dots, x_m))\\) competitive equilibrium economy \\(\\mathscr{E}\\).","code":""},{"path":"economics-4.html","id":"production","chapter":"4 Economics 4","heading":"Production","text":"","code":""},{"path":"economics-4.html","id":"initial-assumptions-2","chapter":"4 Economics 4","heading":"Initial Assumptions","text":"\\(k\\) firms \\(\\mathscr{J} = \\{1, \\dots, k\\}\\).production firm \\(j\\) good \\(l\\) described function \\(f_{j,l}\\) \\(f_j(z_{\\ell,l}) = f_j(z_{1,l}, \\dots, z_{n,l})\\). Namely, firm \\(j\\) uses \\(z_\\ell\\) units commodities \\(\\ell \\\\mathscr{L}\\) produce commodity \\(l\\).firms owned consumers society.firms’ ownership exogenous.\\(\\theta_{,j}\\) represents fraction \\(j\\)th firm owned \\(\\)th consumer.firms endowments.Remark. \\[\n\\sum_{\\\\mathscr{}} \\theta_{,j} = \\theta_{1,j} + \\theta_{2,j} + \\cdots + \\theta_{m,j} = 1 .\n\\]Definition 4.10  (Competitive Equilibrium) \\(((X^*, Z^*), p)\\) called competitive equilibrium following conditions hold:\\(\\forall j \\\\mathscr{J}\\), \\(z^*_j = ((z_{j,1,1'}, \\dots, z_{j,1,n'}), \\dots, (z_{j,n,1'}, \\dots, z_{j,n,n'}))\\) solves following maximization problem:\n\\[\n\\pi^*_j := \\max_{ z_j }\\ p_l f_{j,l} (z_\\ell) - \\sum_{\\ell \\\\mathscr{L}} p_\\ell z_{j, \\ell}.\n\\]\\(\\forall \\\\mathscr{}\\), \\(x_i^* = (x_{,1}^*, \\dots, x_{,n}^*)\\) solves following maximization problem:\n\\[\n\\begin{align*}\n\\max_{ x_i } \\quad & u_i(x_i) \\\\\n\\text{subject } \\quad & \\langle p, x_i \\rangle \\leq \\langle p, w_i \\rangle + \\sum_{j \\\\mathscr{J}} \\theta_{,j} \\pi^*_j.\n\\end{align*}\n\\]Markets clear, .e. \\(\\displaystyle \\sum_{\\\\mathscr{}} x_{,\\ell}^* + \\sum_{j \\\\mathscr{J}} \\sum_{\\ell \\\\mathscr{L}} z^*_{j,l} = \\sum_{\\\\mathscr{}} w_{,\\ell} + \\sum_{j \\\\mathscr{J}} f_{j,l} (z^*_\\ell)\\), \\(\\forall \\ell \\\\mathscr{L}\\).Proposition 4.7  Walras’s law welfare theorems hold.Proposition 4.8  Let \\(((X^*, Z^*), p)\\) competitive equilibrium. \\(((X^*, Z^*), cp)\\) also competitive equilibrium, \\(\\forall c \\\\mathbb{R}_+\\).production, Edgeworth box diagrams longer helpful.","code":""},{"path":"economics-4.html","id":"general-case-1","chapter":"4 Economics 4","heading":"General Case","text":"\\[\n\\begin{align*}\n\\max_{ x_i, z_j } \\quad & u_1(x_1) \\\\\n\\text{subject } \\quad & u_2(x_2) \\geq \\bar{u}_2, \\\\\n& \\quad \\vdots \\\\\n& u_m(x_m) \\geq \\bar{u}_m; \\\\\n& \\sum_{\\\\mathscr{}} x_{,1} + \\sum_{j \\\\mathscr{J}} z_{j,1} \\leq \\sum_{j \\\\mathscr{J}} f_{j,1} (z_\\ell) + \\sum_{\\\\mathscr{}} w_{,1}, \\\\\n& \\quad \\vdots \\\\\n& \\sum_{\\\\mathscr{}} x_{,n} + \\sum_{j \\\\mathscr{J}} z_{j,n} \\leq \\sum_{j \\\\mathscr{J}} f_{j,n} (z_\\ell) + \\sum_{\\\\mathscr{}} w_{,n}.\n\\end{align*}\n\\]Theorem 4.6  Let utility functions quasiconcave strictly increasing, \\((X,Z)\\) feasible interior allocation. \\((X, Z)\\) Pareto efficient following equalities hold pairs goods \\((\\ell, \\ell')\\):Marginal rates substitution equal across consumers,\n\\[\n\\text{.e.}\\quad \\text{MRS}(\\ell,\\ell')(x_1) = \\cdots = \\text{MRS}(\\ell,\\ell')(x_m).\n\\]Marginal rates technical substitution equal across firms pair commodities,\n\\[\n\\text{.e.}\\quad \\text{MRTS}(\\ell,\\ell')(z_1) = \\cdots = \\text{MRTS}(\\ell,\\ell')(z_k).\n\\]Marginal rates transformation equal marginal rates substitution,\n\\[\n\\text{.e.}\\quad \\text{MRT}(j,j')(z_{\\ell''}) = \\text{MRS}(\\ell,\\ell')(x_i), \\quad \\forall \\\\mathscr{}.\n\\]Definition 4.11  define production possibility set set non-negative outputs goods firms can produce using economy’s available factor inputs. output combinations frontier set correspond Pareto efficient allocation factor inputs, .e. allocation possible, given total factor endowment, increase production one good without decreasing production good.Remark (Friendly reminder marginal rates). Marginal Rate Substitution (\\(\\text{MRS}\\))rate consumer willing trade one good another maintain constant level utility.slope indifference curve.focuses demand side consumer theory.Marginal Rate Transformation (\\(\\text{MRT}\\))amount one good must given produce additional unit another good.slope production possibility frontier.focuses supply side commodity.Marginal Rate Technical Substitution (\\(\\text{MRTS}\\))amount quantity one input reduced order use another input.slope isoquant curve.focuses production side economic theory.","code":""},{"path":"economics-4.html","id":"monopoly-and-monopsony","chapter":"4 Economics 4","heading":"4.2 Monopoly and Monopsony","text":"","code":""},{"path":"economics-4.html","id":"game-theory","chapter":"4 Economics 4","heading":"4.3 Game Theory","text":"","code":""},{"path":"probability.html","id":"probability","chapter":"5 Probability","heading":"5 Probability","text":"basic course probability (EST-11101), may want skip notes Measure Theory.","code":""},{"path":"probability.html","id":"basics-and-combinatorics","chapter":"5 Probability","heading":"5.1 Basics and Combinatorics","text":"Definition 5.1  (Sample Space) sample space \\(\\Omega \\neq \\varnothing\\) set possible outcomes experiment. can finite infinite.Definition 5.2  (Event) event \\(\\) subset sample space \\(\\subseteq \\Omega\\), element power set sample space \\(\\displaystyle \\2^\\Omega\\).Definition 5.3  (Observable Event Set) set observable events denoted \\(\\mathscr{F}\\), \\(\\displaystyle \\mathscr{F}\\subseteq 2^\\Omega\\).Usually, \\(\\Omega\\) countable, \\(\\mathscr{F}= 2^\\Omega\\). However, sometimes many events excluded \\(\\mathscr{F}\\) since possible happen.Definition 5.4  (σ-Algebra) set \\(\\mathscr{F}\\) called \\(\\sigma\\)-algebra :\\(\\Omega \\\\mathscr{F}\\);\\(\\forall \\subseteq \\Omega\\), \\(\\\\mathscr{F}\\), \\(^C \\\\mathscr{F}\\); \\(\\forall (A_n)_{n \\\\mathbb{N}}\\), \\(A_n \\\\mathscr{F}\\), \\(\\displaystyle \\bigcup_{n = 1}^\\infty A_n \\\\mathscr{F}\\).Definition 5.5  (Probability Measure) \\(P : \\mathscr{F}\\[0, 1]\\) probability measure satisfies following three axioms:\\(\\forall \\\\mathscr{F}: P() \\geq 0\\),\\(P(\\Omega) = 1\\), \\(\\displaystyle P\\left( \\bigcup_{n=1}^\\infty A_n \\right) = \\sum_{n = 1}^\\infty P\\left(A_n\\right)\\),\\(A_n\\) disjunct.Remark. \\(\\displaystyle P\\left(^C\\right) = 1 - P()\\),\\(P(\\varnothing) = 0\\),\\(\\subseteq B\\), \\(P() \\leq P(B)\\), \\(P(\\cup B) = P() + P(B) - P(\\cap B)\\).Proposition 5.1  (De Morgan's Laws) Let \\(A_1, \\dots, A_n\\) set events.\n\\[\n\\left( \\bigcup_{=1}^n A_i \\right)^C = \\bigcap_{=1}^n A_i^C \\qquad \\left( \\bigcap_{=1}^n A_i \\right)^C = \\bigcup_{=1}^n A_i^C\n\\]Theorem 5.1  (Inclusion-Exclusion Principle) Let \\(A_1, \\dots, A_n\\) set events, \n\\[\nP\\left( \\bigcup_{=1}^n A_i \\right) = \\sum_{k = 1}^n (-1)^{k-1} S_k,\n\\]\n\n\\[\nS_k = \\sum_{\\subseteq \\{1, \\dots, n\\} \\\\ \\quad \\mid \\mid = k} P\\left( \\bigcap_{\\} A_i \\right).\n\\]Definition 5.6  (Laplace Space) \\(\\Omega = \\{\\omega_1, \\dots, \\omega_N\\}\\) \\(\\mid \\Omega \\mid = N\\) \\(\\omega_i\\) probability \\(p_i = \\frac{1}{N}\\), \\(\\Omega\\) called Laplace space \\(P\\) discrete uniform distribution. event \\(\\), \n\\[\nP() = \\frac{\\lvert \\rvert}{\\lvert\\Omega\\rvert}.\n\\]discrete uniform distribution exists \\(\\Omega\\) finite.Definition 5.7  (Conditional Probability) Given two events \\(\\) \\(B\\) \\(P() > 0\\), probability \\(B\\) given \\(\\) defined \n\\[\nP(B \\mid ) := \\frac{P(B \\cap )}{P()}.\n\\]Theorem 5.2  (Total Probability) Let \\(A_1, \\dots, A_n\\) set disjunct events \\(\\forall \\neq j : A_i \\cap A_j = \\varnothing\\) \\(\\displaystyle \\bigcup_{=1}^n A_i = \\Omega\\), , event \\(B \\subseteq \\Omega\\),\n\\[\nP(B) = \\sum_{= 1}^n P(B \\mid A_i) P(A_i).\n\\]Definition 5.8  (Bayes' Rule) Let \\(A_1, \\dots, A_n\\) set disjunct event \\(\\forall \\neq j : A_i \\cap A_j = \\varnothing\\) \\(\\displaystyle \\bigcup_{=1}^n A_i = \\Omega\\) \\(P(A_i) > 0\\) \\(= 1, \\dots, n\\), , event \\(B \\subseteq \\Omega\\) \\(P(B) > 0\\), \n\\[\nP(A_k \\mid B) = \\frac{P(B \\mid A_k) P(A_k)}{\\displaystyle\\sum_{=1}^n P(B \\mid A_i) P(A_i)}.\n\\]Definition 5.9  (Independence) set events \\(A_1, \\dots, A_n\\) independent , \\(m \\\\mathbb{N}\\) \\(\\{k_1, \\dots, k_m\\} \\subseteq 1, \\dots, n\\), \n\\[\nP\\left( \\bigcap_{=1}^m A_{k_i} \\right) = \\prod_{=1}^m P\\left(A_{k_i}\\right).\n\\]Definition 5.10  (Factorial) factorial function defined product\n\\[\nn! = \\prod_{=1}^n = n \\cdot (n - 1)!\n\\]\ninteger \\(n \\geq 1\\).Definition 5.11  (Gamma Function) Let \\(z \\\\mathbb{C}\\) \\(\\Re(z) > 0\\), gamma function defined via following convergent improper integral:\n\\[\n\\Gamma(z) = \\int_0^\\infty t^{z - 1} e^{-t} dt.\n\\]Remark (Gamma Function Properties). \\(\\displaystyle \\Gamma(1/2) = \\sqrt{\\pi}\\).\\(\\displaystyle \\Gamma(1) = \\Gamma(2) = 1\\).\\(\\displaystyle \\Gamma(z) = (z - 1)\\Gamma(z - 1)\\).\\(\\displaystyle \\Gamma(n) = (n - 1)!\\), \\(\\forall n \\\\mathbb{N}\\).Definition 5.12  (Permutation) Let \\(n\\) number total objects \\(k\\) number objects want select. permutation arrangement elements care ordering.Repetition allowed:\n\\[\nP_n(k) = \\frac{n!}{(n - k)!}.\n\\]Repetition allowed:\n\\[\nP_n(k) = n^k.\n\\]Definition 5.13  (Combination) Let \\(n\\) number total objects \\(k\\) number objects want select. combination arrangement elements care ordering.Repetition allowed:\n\\[\nC_n(k) = \\binom{n}{k} = \\frac{P_n(k)}{k!} = \\frac{n!}{k!(n - k)!}.\n\\]Repetition allowed:\n\\[\nC_n(k) = \\binom{n + k -1}{k}.\n\\]Repetition replacement.Remark (Binomial Coefficient Properties). \\(0! = 1\\),\\(\\displaystyle \\binom{n}{0} = \\binom{n}{n} = 1\\),\\(\\displaystyle \\binom{n}{1} = \\binom{n}{n - 1} = n\\),\\(\\displaystyle \\binom{n}{k} = \\binom{n}{n - k}\\),\\(\\displaystyle \\binom{n}{k} = \\binom{n - 1}{k - 1} + \\binom{n - 1}{k}\\), \\(\\displaystyle \\sum_{k = 0}^n \\binom{n}{k} = 2^n\\).Remark (Sum Properties). Let \\(x, y \\\\mathbb{R}^n\\), \\(c \\\\mathbb{R}\\) \\(k \\neq 1\\).\\(\\displaystyle \\sum_i x_i y_i \\neq \\sum_i x_i \\sum y_i\\).\\(\\displaystyle \\sum_i x_i^k \\neq \\left( \\sum_i x_i \\right)^k\\).\\(\\displaystyle \\sum_{=1}^n c = nc\\).\\(\\displaystyle \\sum_{=1}^n cx_i = n\\sum_{=1}^n x_i\\).\\(\\displaystyle \\sum_i (x_i + y_i) = \\sum_i x_i + \\sum_i y_i\\).Theorem 5.3  (Binomial Expansion) \\[\n(x+y)^n = \\sum_{k=0}^n {n \\choose k}x^{n-k}y^k = \\sum_{k=0}^n {n \\choose k}  x^k y^{n-k}\n\\]","code":""},{"path":"probability.html","id":"random-variables","chapter":"5 Probability","heading":"5.2 Random Variables","text":"Definition 5.14  (Random Variable) Let \\((\\Omega, \\mathscr{F}, P)\\) probability space. random variable \\(\\Omega\\) function\n\\[\nX : \\Omega \\\\mathscr{W}(X) \\subseteq \\mathbb{R}.\n\\]\nimage \\(\\mathscr{W}(X)\\) countable, \\(X\\) called discrete random variable, otherwise called continuous random variable.Definition 5.15  (Probability Density Function / Probability Mass Function) probability density function (PDF) \\(f_X : \\mathbb{R}\\\\mathbb{R}\\) random variable \\(X\\) function defined \n\\[\nf_X(x) := P(X = x) := P\\left(\\{\\omega \\mid X(\\omega) = x\\}\\right).\n\\]\n\\(X\\) discrete, called probability mass function.Remark. \\(X\\) discrete random variable, \\(\\displaystyle \\sum_i f_X(u_i) = 1\\).\\(X\\) continuous random variable, \\(\\displaystyle \\int_{-\\infty}^{\\infty} f_X(t)dt = 1\\).Definition 5.16  (Cumulative Distribution) cumulative distribution function (CDF) \\(F_X : \\mathbb{R}\\[0,1]\\) random variable \\(X\\) function defined \n\\[\nF_X(x) := P(X \\leq x) := P\\left(\\{\\omega \\mid X(\\omega) \\leq x\\}\\right).\n\\]\nPDF given, CDF can expressed \n\\[\nF_X(x) =\n\\begin{cases}\n\\displaystyle \\sum_{x_i \\leq x} f_X(x_i), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^x f_X(t)dt,  & \\text{continuous}\n\\end{cases}\n\\]Remark (Cumulative Distribution Properties). \\(t \\leq s\\), \\(F_X(t) \\leq F_X(s)\\) (monotonicity).\\(t > s\\), \\(\\displaystyle \\lim_{t \\s} F_X(t) = F_X(s)\\).\\(\\displaystyle \\lim_{t \\-\\infty} F_X(t) = 0\\) \\(\\displaystyle \\lim_{t \\\\infty} F_X(t) = 1\\).\\(\\displaystyle P(\\leq X \\leq b) = F_X(b) - F_X() = \\int_{}^{b} f_X(t)dt\\).\\(P(X > t) = 1 - P(X \\leq t) = 1 - F_X(t)\\).\\(\\displaystyle \\frac{d}{dx}F_X(x) = f_X(x)\\).Definition 5.17  (Expected Value) Let \\(X\\) random variable. expected value defined \n\\[\n\\mathbb{E}[X] :=\n\\begin{cases}\n\\displaystyle \\sum_{x_k \\\\mathscr{W}(X)} x_k \\cdot f_X(x_k), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^\\infty x \\cdot f_X(x)dx,  & \\text{continuous}\n\\end{cases}\n\\]Remark (Expected Value Properties). \\(\\mathbb{E}[X] \\leq \\mathbb{E}[Y]\\) \\(\\forall \\omega : X(\\omega) \\leq Y(\\omega)\\),\\(\\displaystyle \\mathbb{E}\\left[ \\sum_{=0}^n a_i X_i \\right] = \\sum_{=0}^n a_i \\mathbb{E}\\left[ X_i \\right]\\),\\(\\displaystyle \\mathbb{E}[X] = \\sum_{j=1}^\\infty P[X \\geq j]\\) \\(\\mathscr{W}(X) \\subseteq \\mathbb{N}_0\\),\\(\\displaystyle \\mathbb{E}\\left[ \\sum_{=0}^\\infty X_i \\right] \\neq \\sum_{=0}^\\infty \\mathbb{E}\\left[ X_i \\right]\\),\\(\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X]\\),\\(\\mathbb{E}[XY]^2 \\leq E[X^2]E[Y^2]\\), \\(\\displaystyle \\mathbb{E}\\left[ \\prod_{=0}^n X_i \\right] = \\prod_{=0}^n \\mathbb{E}[X_i]\\) independent \\(X_1, \\dots, X_n\\).expected value linear operator.Definition 5.18  (Raw Moment / Central Moment) Let \\(n \\\\mathbb{N}\\). \\(n\\)th (raw) moment defined \n\\[\n\\mu_n' = \\mathbb{E}\\left[X^n\\right].\n\\]\n\\(n\\)th central moment defined \n\\[\n\\mu_n = \\mathbb{E}\\left[\\left( X - \\mathbb{E}[X] \\right)^n \\right].\n\\]Definition 5.19  (Expected Value Functions) Let \\(X\\) random variable \\(Y = g(X)\\) \\(g : \\mathbb{R}\\\\mathbb{R}\\), \n\\[\n\\mathbb{E}[Y] :=\n\\begin{cases}\n\\displaystyle \\sum_{x_k \\\\mathscr{W}(X)} g(x_k) \\cdot f_X(x_k), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^\\infty g(x) \\cdot f_X(x)dx,  & \\text{continuous}\n\\end{cases}\n\\]Definition 5.20  (Moment-Generating Function) Let \\(X\\) random variable. moment-generating function \\(X\\) defined \n\\[\nM_X(t) := \\mathbb{E}\\left[ e^{tX} \\right].\n\\]Definition 5.21  (Characteristic Function) Let \\(X\\) random variable. characteristic function \\(X\\) defined \n\\[\n\\varphi_X(t) := \\mathbb{E}\\left[ e^{itX} \\right]\n\\]\n\\(= \\sqrt{-1} \\\\mathbb{C}\\).Definition 5.22  (Variance) Let \\(X\\) random variable \\(\\mathbb{E}[X^2] < \\infty\\). variance \\(X\\) defined \n\\[\n\\text{Var}[X] := \\mathbb{E}\\left[ (X - \\mathbb{E}[X])^2 \\right].\n\\]Remark (Variance Properties). \\(0 \\leq \\text{Var}[X] \\leq \\mathbb{E}[X^2]\\),\\(\\text{Var}[X] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]\\),\\(\\text{Var}[aX + b] = ^2\\text{Var}[X]\\),\\(\\text{Var}[X] = \\text{Cov}(X,X)\\),\\(\\displaystyle \\text{Var}\\left[ \\sum_{=1}^n a_i X_i \\right] = \\sum_{= 1}^n a_i^2 \\text{Var}[X_i] + 2 \\sum_{1 \\leq < j \\leq n} a_i a_j \\text{Cov}(X_i, X_j)\\), \\(\\displaystyle \\text{Var}\\left[ \\sum_{=1}^n X_i \\right] = \\sum_{= 1}^n \\text{Var}[X_i]\\) \\(\\text{Cov}(X_i, X_j) = 0\\), \\(\\forall \\neq j\\).Definition 5.23  (Standard Deviation) Let \\(X\\) random variable \\(\\mathbb{E}[X^2] < \\infty\\). standard deviation \\(X\\) defined \n\\[\n\\sigma(X) = \\text{sd}(X) := \\sqrt{\\text{Var}[X]}.\n\\]Definition 5.24  (Covariance) Let \\(X\\) \\(Y\\) random variables finite expected value. covariance \\(X\\) \\(Y\\) defined \n\\[\n\\begin{align*}\n\\text{Cov}(X, Y) :&=\\ \\mathbb{E}\\left[ (X - \\mathbb{E}[X]) (Y - \\mathbb{E}[Y]) \\right] \\\\\n            &=\\ \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n\\end{align*}\n\\]covariance measure correlation two random variables. \\(\\text{Cov}(X,Y)>0\\) \\(Y\\) tends increase \\(X\\) increases. \\(\\text{Cov}(X,Y)<0\\) \\(Y\\) tends decrease \\(X\\) increases. \\(\\text{Cov}(X,Y)=0\\), \\(X\\) \\(Y\\) uncorrelated.Remark (Covariance Properties). \\(\\text{Cov}(aX,) = ab\\text{Cov}(X,Y)\\),\\(\\text{Cov}(X + , Y + b) = \\text{Cov}(X,Y)\\), \\(\\text{Cov}(aX_1 + bX_2, cY_1 + dY_2)\\)\n\\(= ac\\text{Cov}(X_1,Y_1) + ad\\text{Cov}(X_1,Y_2) + bc\\text{Cov}(X_2,Y_1) + bd\\text{Cov}(X_2,Y_2)\\).Definition 5.25  (Correlation) Let \\(X\\) \\(Y\\) random variables finite expected value. correlation \\(X\\) \\(Y\\) defined \n\\[\n\\text{Corr}(X,Y) := \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}[X] \\cdot \\text{Var}[Y]}}.\n\\]Correlation covariance normalized values \\(-1\\) \\(1\\).Definition 5.26  (Coefficient Variation) coefficient variation defined ratio standard deviation mean.\n\\[\n\\text{.e.}\\quad c_V = \\frac{\\sigma}{\\mu}.\n\\]Definition 5.27  (Indicator Function) indicator function \\(\\mathbb{1}_A : \\Omega \\\\{0, 1\\}\\) set (event) \\(\\) defined \n\\[\n\\mathbb{1}_A (\\omega) :=\n\\begin{cases}\n1, & \\omega \\\\\\\n0, & \\omega \\^C\n\\end{cases}\n\\]Definition 5.28  (Survival Function) Let \\(X\\) continuous random variable cumulative distribution function \\(F(x)\\) interval \\([0, \\infty)\\). survival function reliability function defined \n\\[\nS(x) = P[X > x] = \\int_x^\\infty f(t)dt = 1 - F(x).\n\\]Definition 5.29  (Memorylessness) Suppose \\(X\\) non-negative random variable. probability distribution \\(X\\) memoryless \\(s, t \\geq 0\\), \n\\[\nP[X > s + t \\mid X > t] = P[X > s].\n\\]Theorem 5.4  (Markov's Inequality) Let \\(X\\) random variable \\(g : \\mathscr{W}(X) \\[0, \\infty)\\) increasing function, , \\(c\\) \\(g(c) > 0\\), \n\\[\nP[X \\geq c] \\leq \\frac{\\mathbb{E}[g(X)]}{g(x)}.\n\\]practical uses usually \\(g(x) = x\\).Theorem 5.5  (Chebyshev's Inequality) Let \\(X\\) random variable \\(\\text{Var}[X] < \\infty\\), , \\(k > 0\\),\n\\[\nP\\left[ \\lvert X - \\mathbb{E}[X] \\rvert \\geq k \\right] \\leq \\frac{\\text{Var}[X]}{k^2}.\n\\]Theorem 5.6  (Jensen's Inequality) \\(X\\) random variable \\(\\varphi\\) convex function, \n\\[\n\\varphi\\left(\\mathbb{E}[X]\\right) \\leq \\mathbb{E}\\left[\\varphi(X)\\right].\n\\]","code":""},{"path":"probability.html","id":"multivariate-distributions","chapter":"5 Probability","heading":"5.3 Multivariate Distributions","text":"Definition 5.30  (Joint Probability Density Function) joint probability density function \\(f_X : \\mathbb{R}^n \\[0, 1]\\) \\(X = (X_1, \\dots, X_n)\\) function defined \n\\[\nf_X(x_1, \\dots, x_n) := P[X_1 = x_1, \\dots, X_n = x_n].\n\\]Definition 5.31  (Joint Cumulative Distribution Function) joint cumulative distribution function \\(F_X : \\mathbb{R}^n \\[0, 1]\\) \\(X = (X_1, \\dots, X_n)\\) function defined \n\\[\nf_X(x_1, \\dots, x_n) := P[X_1 \\leq x_1, \\dots, X_n \\leq x_n].\n\\]\njoint PDF given, can expressed \n\\[\nF_X(x) =\n\\begin{cases}\n\\displaystyle \\sum_{t_1 \\leq x_1} \\cdots \\sum_{t_n \\leq x_n} f_X(t), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^{x_1} \\cdots \\int_{-\\infty}^{x_n} f_X(t)dt, & \\text{continuous}\n\\end{cases}\n\\]\n\\(t = (t_1, \\dots, t_n)\\) \\(x = (x_1, \\dots, x_n)\\).Remark. \\[\n\\frac{\\partial F_X(x_1, \\dots, x_n)}{\\partial x_1, \\dots, x_n} = f_X(x_1, \\dots, x_n)\n\\]Definition 5.32  (Marginal Probability Density Function) marginal probability density function \\(f_{X_i} : \\mathbb{R}\\[0,1]\\) \\(X_i\\) given joint PDF \\(f_X(x_1, \\dots, x_n)\\) defined \n\\[\nf_{X_i}(t_i) =\n\\begin{cases}\n\\displaystyle \\sum_{t_1} \\cdots \\sum_{t_{-1}} \\sum_{t_{+1}} \\cdots \\sum_{t_n} f_X(t), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} f_X(t)d\\tilde{t}, & \\text{continuous}\n\\end{cases}\n\\]\n\\(\\tilde{t} = (t_1, \\dots, t_{-1}, t_{+1}, \\dots, t_n)\\), discrete case \\(t_k \\\\mathscr{W}(X_k)\\).idea marginal probability ignore random variables consider one ’re interested .Definition 5.33  (Marginal Cumulative Distribution Function) marginal cumulative distribution function \\(F_{X_i} : \\mathbb{R}\\[0,1]\\) \\(X_i\\) given joint CDF \\(F_X(x_1, \\dots, x_n)\\) defined \n\\[\nF_{X_i}(x_i) := \\lim_{x_{j\\neq } \\\\infty} F_X(x_1, \\dots, x_n).\n\\]Definition 5.34  (Conditional Distribution) conditional distribution \\(f_{X \\mid Y} : \\mathbb{R}\\[0,1]\\) defined \n\\[\n\\begin{align*}\nf_{X\\mid Y} (x \\mid y) :&=\\ P[X = x \\mid Y = y] \\\\\n                        &=\\ \\frac{P[X = x, Y = y]}{P[Y = y]} \\\\\n                        &=\\ \\frac{\\text{Joint PDF}}{\\text{Marginal PDF}}\n\\end{align*}\n\\]Definition 5.35  (Independence) random variables \\(X_1, \\dots, X_n\\) independent \n\\[\nF_{X_1, \\dots, X_n} (x_1, \\dots, x_n) = \\prod_{=1}^n F_{X_i}(x_i).\n\\]\nSimilarly, PDF absolutely continuous, independent \n\\[\nf_{X_1, \\dots, X_n} (x_1, \\dots, x_n) = \\prod_{=1}^n f_{X_i}(x_i).\n\\]Theorem 5.7  (Function Independence) random variables \\(X_1, \\dots, X_n\\) independent \\(f_i : \\mathbb{R}\\\\mathbb{R}\\) function \\(Y_i := f_i(X_i)\\), also \\(Y_1, \\dots, Y_n\\) independent.Theorem 5.8  random variables \\(X_1, \\dots, X_n\\) independent , \\(\\forall B_i \\subseteq \\mathscr{W}(X_i)\\), \n\\[\nP[X_1 \\B_1, \\dots, X_n \\B_n] = \\prod_{=1}^n P[X_i \\B_i].\n\\]Definition 5.36  (Joint Expected Value) joint expected value random variable \\(Y = g(X_1, \\dots, X_n) = g(X)\\) defined \n\\[\n\\mathbb{E}[Y] =\n\\begin{cases}\n\\displaystyle \\sum_{t_1} \\cdots \\sum_{t_n} g(t) f_X(t), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} g(t)f_X(t)dt, & \\text{continuous}\n\\end{cases}\n\\]\n\\(t = (t_1, \\dots, t_n)\\), discrete case \\(t_k \\\\mathscr{W}(X_k)\\).Definition 5.37  (Conditional Expected Value) conditional expected value random variables \\(X\\) \\(Y\\) defined \n\\[\n\\mathbb{E}[X\\mid Y] :=\n\\begin{cases}\n\\displaystyle \\sum_{x \\\\mathbb{R}} x \\cdot f_{X\\mid Y}(x\\mid y), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^\\infty x \\cdot f_{X\\mid Y}(x\\mid y)dx,  & \\text{continuous}\n\\end{cases}\n\\]Remark. \\(\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X\\mid Y]]\\),\\(\\mathbb{E}[X\\mid Y] = \\mathbb{E}[X]\\) \\(X\\) \\(Y\\) independent, \\(\\text{Var}[X] = \\mathbb{E}\\left[\\text{Var}[X \\mid Y]\\right] + \\text{Var}\\left[\\mathbb{E}[X \\mid Y]\\right]\\).Definition 5.38  Let \\(Y = g(X_1, \\dots, X_n) = g(X)\\).\n\\[\nP[Y \\C] = \\int_{A_C} f_X(t)dt\n\\]\n\\(A_C = \\{ x = (x_1, \\dots, x_n) \\\\mathbb{R}^n : g(x) \\C \\}\\) \\(t = (t_1, \\dots, t_n)\\).Theorem 5.9  (Transformation) \\(g(\\cdot)\\) strictly increasing function, \n\\[\n\\begin{align}\nF_Y(y) &=\\ \\int_{-\\infty}^{g^{-1}(y)} f_X(x)dx \\\\\n       &=\\ F_X(g^{-1}(y)) \\\\\nf_Y(y) &=\\ f_X\\left( g^{-1}(y) \\right) \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align}\n\\]\n\\(g(\\cdot)\\) strictly decreasing function, \n\\[\n\\begin{align}\nF_Y(y) &=\\ \\int_{g^{-1}(y)}^{\\infty} f_X(x)dx \\\\\n       &=\\ 1 - F_X(g^{-1}(y)) \\\\\nf_Y(y) &= - f_X\\left( g^{-1}(y) \\right) \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align}\n\\]\nEquivalently, \\(\\displaystyle f_X(x) = f_Y(g(x)) \\left\\lvert \\frac{\\partial g(x)}{\\partial x} \\right\\rvert\\).higher dimensions, derivative generalizes determinant Jacobian matrix — matrix \\(\\displaystyle \\mathscr{J}_{ij} = \\frac{\\partial x_i}{\\partial y_j}\\). Thus, real-valued vector \\(x\\) \\(y\\),\n\\[\nf_X(x) = f_Y(g(x)) \\left\\lvert\\det \\left( \\frac{\\partial g(x)}{\\partial x} \\right) \\right\\rvert.\n\\]Theorem 5.10  (Transformation) Let \\(X\\) continuous CDF \\(F_X(\\cdot)\\) define \\(Y = F_X(X)\\). \\(Y \\sim \\mathscr{U}[0,1]\\), .e., \\(F_Y(y) = y\\) \\(y \\[0,1]\\).Theorem 5.11  \\(X \\sim F_X\\) \\(Y \\sim F_Y\\), \\(M_X\\) \\(M_Y\\) exist, \\(M_X(t) = M_Y(t)\\) \\(t\\) neighbourhood zero, \\(F_X(u) = F_Y(u)\\) \\(u\\).Remark (Monte Carlo Integration). Let \\(\\displaystyle = \\int_a^b g(x)dx\\) integral function hard evaluate, \n\\[\n\\begin{align*}\n&=\\ \\int_a^b g(x)dx \\\\\n  &=\\ (b - ) \\int_a^b g(x) \\frac{1}{b - } dx \\\\\n  &=\\ (b - ) \\int_{-\\infty}^{\\infty} g(x) f_{\\mathscr{U}}(x) dx \\\\\n  &=\\ (b - ) \\cdot \\mathbb{E}[g(\\mathscr{U})]\n\\end{align*}\n\\]\n\\(\\mathscr{U}(,b)\\) uniformly distributed. , Law Large Numbers, know can approximate \\(\\mathbb{E}[g(\\mathscr{U})]\\) randomly sampling \\(u_1, u_2, \\dots\\) \\(\\mathscr{U}(,b)\\).\n\\[\n\\frac{b - }{n} \\sum_{=1}^n g(u_i) \\underset{n\\\\infty}{\\longrightarrow} (b - ) \\cdot \\mathbb{E}[g(\\mathscr{U})].\n\\]Remark (Sum). Let \\(X_1, \\dots, X_n\\) independent random variables, sum \\(Z = X_1 + \\cdots + X_n\\) PDF \\(f_Z(z)\\) evaluated convolution PDFs\n\\[\nf_Z(z) = (f_{X_1}(x_1) * \\cdots * f_{X_n}(x_n))(z).\n\\]\nspecial case \\(Z = X + Y\\), \n\\[\nf_Z(z) =\n\\begin{cases}\n\\displaystyle \\sum_{x_k \\\\mathscr{W}(X)} f_X(x_k) f_Y(z - x_k), & \\text{discrete} \\\\\n\\displaystyle \\int_{-\\infty}^\\infty f_X(t) f_Y(z - t)dt, & \\text{continuous}\n\\end{cases}\n\\]Often much easier use properties random variables find sum instead evaluating convolution.Remark (Product). Let \\(X\\) \\(Y\\) independent random variables. evaluate PDF CDF \\(Z = XY\\), proceed \n\\[\n\\begin{align}\nF_Z(z) &=\\ P[XY \\leq z] \\\\\n       &=\\ P\\left[ X \\geq \\frac{z}{Y}, Y < 0 \\right] \\\\\n       &+\\ P\\left[ X \\leq \\frac{z}{Y}, Y > 0 \\right] \\\\\n       &=\\ \\int_{-\\infty}^0 \\left[ \\int_{\\frac{z}{y}}^\\infty f_X(x)dx \\right] f_Y(y)dy \\\\\n       &+\\ \\int_0^\\infty \\left[ \\int_{-\\infty}^{\\frac{z}{y}} f_X(x)dx \\right] f_Y(y)dy\n\\end{align}\n\\]\nPDF \n\\[\nf_Z(z) = \\frac{dF_Z}{dz}(z) = \\int_{-\\infty}^\\infty f_Y(y)f_X\\left(\\frac{z}{y}\\right) \\frac{1}{\\lvert y \\rvert} dy.\n\\]Remark (Quotient). Let \\(X\\) \\(Y\\) independent random variables. evaluate PDF CDF \\(\\displaystyle Z = \\frac{X}{Y}\\), proceed \n\\[\n\\begin{align}\nF_Z(z) &=\\ P\\left[\\frac{X}{Y} \\leq z\\right] \\\\\n       &=\\ P\\left[ X \\geq zY, Y < 0 \\right] \\\\\n       &+\\ P\\left[ X \\leq zY, Y > 0 \\right] \\\\\n       &=\\ \\int_{-\\infty}^0 \\left[ \\int_{yz}^\\infty f_X(x)dx \\right] f_Y(y)dy \\\\\n       &+\\ \\int_0^\\infty \\left[ \\int_{-\\infty}^{yz} f_X(x)dx \\right] f_Y(y)dy\n\\end{align}\n\\]\nPDF \n\\[\nf_Z(z) = \\frac{dF_Z}{dz}(z) = \\int_{-\\infty}^\\infty \\lvert y\\rvert\\ f_X\\left(yz\\right) f_Y(y) dy.\n\\]Definition 5.39  (Covariance Matrix / Correlation Matrix) Let \\(X_1, \\dots, X_n\\) random variables. Let \\(\\sigma_{ij} = \\text{Cov}(X_i, X_j)\\) \\(\\rho_{ij} = \\text{Corr}(X_i, X_j)\\) every \\(, j = 1, 2, \\dots, n\\). covariance correlation matrices defined respectively \n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1n} \\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2n} \\\\\n\\vdots      & \\vdots      & \\ddots & \\vdots \\\\\n\\sigma_{n1} & \\sigma_{n2} & \\cdots & \\sigma_{nn}\n\\end{bmatrix}_{n\\times n} \\\\ \\\\\nP =\n\\begin{bmatrix}\n1         & \\rho_{12} & \\cdots & \\rho_{1n} \\\\\n\\rho_{21} & 1         & \\cdots & \\rho_{2n} \\\\\n\\vdots    & \\vdots    & \\ddots & \\vdots \\\\\n\\rho_{n1} & \\rho_{n2} & \\cdots & 1\n\\end{bmatrix}_{n\\times n}\n\\]Theorem 5.12  Let \\(X\\) \\(Y\\) random variables. \\(\\lvert\\rho_{X,Y}\\rvert = 1\\) exist \\(\\alpha, \\beta \\\\mathbb{R}\\) \\(Y = \\alpha + \\beta X\\).","code":""},{"path":"statistical-inference.html","id":"statistical-inference","chapter":"6 Statistical Inference","heading":"6 Statistical Inference","text":"section still development.Definition 6.1  (Population) population set similar items events interest question experiment.Definition 6.2  (Sample) sample set individuals objects collected selected statistical population defined procedure.","code":""},{"path":"statistical-inference.html","id":"finite-sample-distributions","chapter":"6 Statistical Inference","heading":"6.1 Finite Sample Distributions","text":"Definition 6.3  (Convergence Distribution) sequence \\(\\displaystyle X_n\\) random variables said converge distribution random variable \\(X\\) \n\\[\n\\lim_{n\\\\infty} F_{X_n}(x) = F_X(x)\n\\]\nevery \\(x \\\\mathbb{R}\\).random vectors \\(\\left\\{ X_1, X_2, \\dots \\right\\} \\subset \\mathbb{R}^k\\), say sequence converges distribution random \\(k\\)-vector \\(X\\) \n\\[\n\\lim_{n\\\\infty} P(X_n\\) = P(X\\)\n\\]\nevery \\(\\subset \\mathbb{R}^k\\) continuity set \\(X\\).Definition 6.4  (Convergence Probability) sequence \\(\\displaystyle X_n\\) random variables converges probability towards random variable \\(X\\) \n\\[\n\\lim_{n\\\\infty} P\\left( \\left\\lvert X_n - X \\right\\rvert > \\varepsilon \\right) = 0\n\\]\n\\(\\varepsilon > 0\\).Definition 6.5  (Convergence Mean) Given real number \\(r \\geq 1\\), say sequence \\(X_n\\) converges \\(r\\)th mean \\(L^r\\) norm towards random variable \\(X\\) \\(r\\)th absolute moments \\(X_n\\) \\(X\\) exist, \n\\[\n\\lim_{n\\\\infty} \\mathbb{E}\\left( \\left\\lvert X_n - X \\right\\rvert^r \\right) = 0.\n\\]Theorem 6.1  (Law Large Numbers) Let \\(X_1, X_2, \\dots\\) independent identically distributed random variables finite mean \\(\\mu\\). Let \\(\\bar{X}_n\\) average first \\(n\\) variables, law large numbers establishes :Weak\n\\[\n\\bar{X}_n = \\frac{1}{n} \\sum_{=1}^n X_i \\underset{n\\\\infty}{\\longrightarrow} \\mu\n\\]Weak\n\\[\nP\\left[ \\left\\lvert \\bar{X}_n - \\mu \\right\\rvert > \\varepsilon \\right] \\underset{n\\\\infty}{\\longrightarrow} 0, \\quad \\forall \\varepsilon > 0\n\\]Weak\n\\[\nP\\left[ \\left\\lvert \\bar{X}_n - \\mu \\right\\rvert < \\varepsilon \\right] \\underset{n\\\\infty}{\\longrightarrow} 1, \\quad \\forall \\varepsilon > 0\n\\]Strong\n\\[\nP\\left[ \\left\\{ \\omega \\\\Omega : \\bar{X}_n(\\omega) \\underset{n\\\\infty}{\\longrightarrow} \\mu \\right\\} \\right] = 1\n\\]Theorem 6.2  (Central Limit Theorem) Suppose \\(\\left\\{X_1, \\dots, X_n\\right\\}\\) sequence independent identically distributed random variables \\(\\mathbb{E}[X_i] = \\mu\\) \\(\\text{Var}[X_i] = \\sigma^2 < \\infty\\). , \\(n\\) approaches infinity, random variables \\(\\sqrt{n}\\left( \\bar{X}_n - \\mu \\right)\\) converge distribution normal \\(\\mathscr{N}(0, \\sigma^2)\\).\n\\[\n\\text{.e.}\\quad \\sqrt{n}\\left( \\bar{X}_n - \\mu \\right) \\overset{d}{\\longrightarrow}\\mathscr{N}(0, \\sigma^2)\n\\]Let \\(X_1, \\dots, X_n\\) independent identically distributed random variables drawn according distribution \\(P_\\theta\\) parametrized \\(\\theta = (\\theta_1, \\dots, \\theta_m) \\\\Theta\\), \\(\\Theta\\) set possible parameters selected distribution. goal find best estimator \\(\\hat{\\theta} \\\\Theta\\) \\(\\hat{\\theta} \\approx \\theta\\) since real \\(\\theta\\) known exactly finite sample.Definition 6.6  (Estimator) estimator \\(\\hat{\\theta}_j\\) parameter \\(\\theta_j\\) random variable \\(\\hat{\\theta}_j(X_1, \\dots, X_n)\\) symbolized function observed data.Definition 6.7  (Estimate) estimate \\(\\hat{\\theta}_j(x_1, \\dots, x_n)\\) realization estimator. real value estimated parameter.Definition 6.8  (Bias) bias estimator \\(\\hat{\\theta}\\) defined \n\\[\n\\text{Bias}(\\hat{\\theta}, \\theta) := \\mathbb{E}_\\theta [\\hat{\\theta} - \\theta].\n\\]\nsay estimator unbiased \\(\\text{Bias}_\\theta [\\hat{\\theta}] = 0\\) \\(\\mathbb{E}_\\theta [\\hat{\\theta}] = \\theta\\).Definition 6.9  (Mean Squared Error) mean squared error estimator \\(\\hat{\\theta}\\) defined \n\\[\n\\text{MSE}_\\theta[\\hat{\\theta}] := \\mathbb{E}\\left[ \\left( \\hat{\\theta} - \\theta \\right)^2 \\right] = \\text{Var}_\\theta[\\hat{\\theta}] + \\text{Bias}^2 (\\hat{\\theta}, \\theta)\n\\]Definition 6.10  (Consistency) sequence estimators \\(\\hat{\\theta}^{(n)}\\) parameter \\(\\theta\\) called consistent , \\(\\varepsilon > 0\\),\n\\[\nP_\\theta \\left[ \\left\\lvert \\hat{\\theta}^{(n)} - \\theta \\right\\rvert > \\varepsilon \\right] \\underset{n\\\\infty}{\\longrightarrow} 0.\n\\]estimator consistent , sample data increases, estimator approaches real parameter.Definition 6.11  (Relative Efficiency) relative efficiency two estimators defined \n\\[\ne\\left(\\hat{\\theta}_1, \\hat{\\theta}_2\\right) = \\frac{\\text{Var}\\left[\\hat{\\theta}_2\\right]}{\\text{Var}\\left[\\hat{\\theta}_1\\right]}.\n\\]\nsay \\(\\hat{\\theta}\\) preferable \\(\\text{Var}\\left[\\hat{\\theta}_1\\right] < \\text{Var}\\left[\\hat{\\theta}_2\\right]\\).","code":""},{"path":"statistical-inference.html","id":"point-estimation","chapter":"6 Statistical Inference","heading":"6.2 Point Estimation","text":"Definition 6.12  (Likelihood Function) likelihood function \\(\\mathscr{L}\\) defined \n\\[\n\\mathscr{L}(\\theta;\\ x_1, \\dots, x_n) = f(x_1, \\dots, x_n;\\ \\theta).\n\\]\nAssuming \\(x_i \\perp x_j\\), \\(\\forall \\neq j\\),\n\\[\n\\mathscr{L}(\\theta;\\ x_1, \\dots, x_n) = \\prod_{=1}^n f(x_i;\\ \\theta).\n\\]practical purposes, often use log-likelihood function \\(\\ell(\\theta;\\ x_1, \\dots, x_n) = \\log\\mathscr{L}(\\theta;\\ x_1, \\dots, x_n)\\) since much easier differentiate afterwards, maximum \\(\\mathscr{L}\\) preserved \\(\\theta_j\\).Definition 6.13  (Maximum Likelihood Estimator) maximum likelihood estimator \\(\\hat{\\theta}\\) \\(\\theta\\) defined \n\\[\n\\hat{\\theta} \\\\left\\{ \\mathop{\\mathrm{\\arg\\!\\max}}_{\\theta \\\\Theta} \\mathscr{L}(\\theta;\\ X_1, \\dots, X_n) \\right\\}\n\\]Definition 6.14  (Score) score gradient natural logarithm likelihood function respect \\(m\\)-dimensional parameter vector \\(\\theta\\).\n\\[\ns(\\theta) := \\frac{\\partial \\log \\mathscr{L}(\\theta)}{\\partial\\theta}.\n\\]score indicates steepness log-likelihood function thereby sensitivity infinitesimal changes parameter values.Definition 6.15  (Fisher Information) Let \\(f(X;\\ \\theta)\\) probability density function probability mass function \\(X\\) conditioned value \\(\\theta\\). define Fisher information \n\\[\n\\mathscr{}(\\theta) := \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial\\theta}\\log f(X;\\ \\theta)\\right)^2\\right] = - \\mathbb{E}\\left[\\frac{\\partial^2}{\\partial\\theta^2}\\log f(X;\\ \\theta)\\right].\n\\]Fisher information way measuring amount information observable random variable \\(X\\) carries unknown parameter \\(\\theta\\) upon probability \\(X\\) depends.Definition 6.16  (Cramér–Rao Bound) Suppose \\(\\theta\\) unknown deterministic parameter estimated \\(n\\) independent observations \\(x\\), distribution according probability function \\(f(X;\\ \\theta)\\). variance unbiased estimator \\(\\hat{\\theta}\\) \\(\\theta\\) bounded reciprocal Fisher information \\(\\mathscr{}(\\theta)\\). Namely,\n\\[\n\\text{Var}\\left[\\hat{\\theta}\\right] \\geq \\frac{1}{n\\mathscr{}(\\theta)}.\n\\]Definition 6.17  (Efficiency) efficiency unbiased estimator \\(\\hat{\\theta}\\) parameter \\(\\theta\\) defined \n\\[\ne\\left( \\hat{\\theta} \\right) = \\frac{1 / \\mathscr{}(\\theta)}{\\text{Var}\\left[ \\hat{\\theta} \\right]}\n\\]\n\\(\\mathscr{}(\\theta)\\) Fisher information sample.Proposition 6.1  \\[\ne\\left( \\hat{\\theta} \\right) \\leq 1\n\\]Remark (Maximum Likelihood Estimator Properties). Asymptotically unbiased. Namely, \\(\\displaystyle \\lim_{n\\\\infty} \\text{Bias}\\left( \\hat{\\theta}_n, \\theta \\right) = 0\\).Asymptotically efficient. Namely, \\(\\displaystyle \\lim_{n\\\\infty} \\text{Var}\\left[ \\hat{\\theta} \\right] = \\frac{1}{n\\mathscr{}(\\theta)}\\).Consistency.\\(\\displaystyle \\hat{\\theta}_n \\overset{d}{\\longrightarrow}\\mathscr{N}\\left( \\theta, \\frac{1}{n\\mathscr{}(\\theta)} \\right)\\).","code":""},{"path":"statistical-inference.html","id":"interval-estimation","chapter":"6 Statistical Inference","heading":"6.3 Interval Estimation","text":"","code":""},{"path":"statistical-inference.html","id":"parametric-hypothesis-testing","chapter":"6 Statistical Inference","heading":"6.4 Parametric Hypothesis Testing","text":"","code":""},{"path":"statistical-inference.html","id":"normality-tests","chapter":"6 Statistical Inference","heading":"6.5 Normality Tests","text":"","code":""},{"path":"fundamentals-of-econometrics.html","id":"fundamentals-of-econometrics","chapter":"7 Fundamentals of Econometrics","heading":"7 Fundamentals of Econometrics","text":"section still development.","code":""},{"path":"fundamentals-of-econometrics.html","id":"statistical-foundations-of-econometric-modelling","chapter":"7 Fundamentals of Econometrics","heading":"7.1 Statistical Foundations of Econometric Modelling","text":"","code":""},{"path":"fundamentals-of-econometrics.html","id":"estimation-theory","chapter":"7 Fundamentals of Econometrics","heading":"7.2 Estimation Theory","text":"","code":""},{"path":"fundamentals-of-econometrics.html","id":"asymptotic-theory","chapter":"7 Fundamentals of Econometrics","heading":"7.3 Asymptotic Theory","text":"","code":""},{"path":"fundamentals-of-econometrics.html","id":"inference-for-linear-regression","chapter":"7 Fundamentals of Econometrics","heading":"7.4 Inference for Linear Regression","text":"","code":""},{"path":"notation.html","id":"notation","chapter":"A Notation","heading":"A Notation","text":"","code":""},{"path":"notation.html","id":"linear-algebra-1","chapter":"A Notation","heading":"Linear Algebra","text":"","code":""},{"path":"notation.html","id":"real-analysis-and-topology","chapter":"A Notation","heading":"Real Analysis and Topology","text":"","code":""},{"path":"notation.html","id":"statistics","chapter":"A Notation","heading":"Statistics","text":"","code":""},{"path":"probability-distributions.html","id":"probability-distributions","chapter":"B Probability Distributions","heading":"B Probability Distributions","text":"section still development.","code":""},{"path":"probability-distributions.html","id":"discrete","chapter":"B Probability Distributions","heading":"Discrete","text":"","code":""},{"path":"probability-distributions.html","id":"bernoulli","chapter":"B Probability Distributions","heading":"Bernoulli","text":"","code":""},{"path":"probability-distributions.html","id":"binomial","chapter":"B Probability Distributions","heading":"Binomial","text":"","code":""},{"path":"probability-distributions.html","id":"geometric","chapter":"B Probability Distributions","heading":"Geometric","text":"","code":""},{"path":"probability-distributions.html","id":"hypergeometric","chapter":"B Probability Distributions","heading":"Hypergeometric","text":"","code":""},{"path":"probability-distributions.html","id":"negative-binomial","chapter":"B Probability Distributions","heading":"Negative Binomial","text":"","code":""},{"path":"probability-distributions.html","id":"poisson","chapter":"B Probability Distributions","heading":"Poisson","text":"","code":""},{"path":"probability-distributions.html","id":"uniform","chapter":"B Probability Distributions","heading":"Uniform","text":"","code":""},{"path":"probability-distributions.html","id":"continuous","chapter":"B Probability Distributions","heading":"Continuous","text":"","code":""},{"path":"probability-distributions.html","id":"beta","chapter":"B Probability Distributions","heading":"Beta","text":"","code":""},{"path":"probability-distributions.html","id":"cauchy","chapter":"B Probability Distributions","heading":"Cauchy","text":"","code":""},{"path":"probability-distributions.html","id":"chi-squared","chapter":"B Probability Distributions","heading":"Chi Squared","text":"","code":""},{"path":"probability-distributions.html","id":"dirac","chapter":"B Probability Distributions","heading":"Dirac","text":"","code":""},{"path":"probability-distributions.html","id":"exponential","chapter":"B Probability Distributions","heading":"Exponential","text":"","code":""},{"path":"probability-distributions.html","id":"f","chapter":"B Probability Distributions","heading":"F","text":"","code":""},{"path":"probability-distributions.html","id":"gamma","chapter":"B Probability Distributions","heading":"Gamma","text":"","code":""},{"path":"probability-distributions.html","id":"laplace","chapter":"B Probability Distributions","heading":"Laplace","text":"","code":""},{"path":"probability-distributions.html","id":"normal","chapter":"B Probability Distributions","heading":"Normal","text":"","code":""},{"path":"probability-distributions.html","id":"student-t","chapter":"B Probability Distributions","heading":"Student T","text":"","code":""},{"path":"probability-distributions.html","id":"uniform-1","chapter":"B Probability Distributions","heading":"Uniform","text":"","code":""},{"path":"probability-distributions.html","id":"weibull","chapter":"B Probability Distributions","heading":"Weibull","text":"","code":""},{"path":"estimators.html","id":"estimators","chapter":"C Estimators","heading":"C Estimators","text":"section still development.","code":""}]
