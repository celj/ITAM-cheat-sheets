<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>5 Probability | My cheat sheets</title>
<meta name="author" content="Carlos Lezama">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="5 Probability | My cheat sheets">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="5 Probability | My cheat sheets">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- JavaScript Imports --><!-- D3 --><script src="https://d3js.org/d3.v6.min.js"></script><!-- Jquery --><script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script><script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script><!-- jStat --><script src="https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js"></script><!-- MathJax settings --><script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        "HTML-CSS": {
          fonts: ["Neo-Euler"],
          scale: 90
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      });
    </script><script src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_HTML"></script><!-- Popper --><script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="templates/bs4_style.css">
<link rel="stylesheet" href="templates/corrections.css">
<meta name="description" content="For a basic course in probability (such as EST-11101), you may want to skip some notes on Measure Theory.  5.1 Basics and Combinatorics  Definition 5.1 (Sample Space) The sample space \(\Omega...">
<meta property="og:description" content="For a basic course in probability (such as EST-11101), you may want to skip some notes on Measure Theory.  5.1 Basics and Combinatorics  Definition 5.1 (Sample Space) The sample space \(\Omega...">
<meta name="twitter:description" content="For a basic course in probability (such as EST-11101), you may want to skip some notes on Measure Theory.  5.1 Basics and Combinatorics  Definition 5.1 (Sample Space) The sample space \(\Omega...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">My cheat sheets</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="consumer-and-producer-theory.html"><span class="header-section-number">1</span> Consumer and Producer Theory</a></li>
<li><a class="" href="economics-2.html"><span class="header-section-number">2</span> Economics 2</a></li>
<li><a class="" href="economics-3.html"><span class="header-section-number">3</span> Economics 3</a></li>
<li><a class="" href="economics-4.html"><span class="header-section-number">4</span> Economics 4</a></li>
<li><a class="active" href="probability.html"><span class="header-section-number">5</span> Probability</a></li>
<li><a class="" href="statistical-inference.html"><span class="header-section-number">6</span> Statistical Inference</a></li>
<li><a class="" href="fundamentals-of-econometrics.html"><span class="header-section-number">7</span> Fundamentals of Econometrics</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="notation.html"><span class="header-section-number">A</span> Notation</a></li>
<li><a class="" href="probability-distributions.html"><span class="header-section-number">B</span> Probability Distributions</a></li>
<li><a class="" href="estimators.html"><span class="header-section-number">C</span> Estimators</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/celj/itam-cheat-sheets">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="probability" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Probability<a class="anchor" aria-label="anchor" href="#probability"><i class="fas fa-link"></i></a>
</h1>
<p>For a basic course in probability (such as <a href="http://estadistica.itam.mx/sites/default/files/u450/probabilidad.pdf">EST-11101</a>), you may want to skip some notes on Measure Theory.</p>
<div id="basics-and-combinatorics" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Basics and Combinatorics<a class="anchor" aria-label="anchor" href="#basics-and-combinatorics"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:unlabeled-div-198" class="definition"><strong>Definition 5.1  (Sample Space) </strong></span>The <em>sample space</em> <span class="math inline">\(\Omega \neq \varnothing\)</span> is the set of all possible outcomes of an experiment. It can be finite or infinite.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-199" class="definition"><strong>Definition 5.2  (Event) </strong></span>An <em>event</em> <span class="math inline">\(A\)</span> is a subset of the sample space <span class="math inline">\(A \subseteq \Omega\)</span>, or an element of the power set of the sample space <span class="math inline">\(\displaystyle A \in 2^\Omega\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-200" class="definition"><strong>Definition 5.3  (Observable Event Set) </strong></span>The set of all <em>observable events</em> is denoted by <span class="math inline">\(\mathscr{F}\)</span>, where <span class="math inline">\(\displaystyle \mathscr{F}\subseteq 2^\Omega\)</span>.</p>
</div>
<blockquote>
<p>Usually, if <span class="math inline">\(\Omega\)</span> is countable, <span class="math inline">\(\mathscr{F}= 2^\Omega\)</span>. However, sometimes many events are excluded from <span class="math inline">\(\mathscr{F}\)</span> since it is not possible for them to happen.</p>
</blockquote>
<div class="definition">
<p><span id="def:unlabeled-div-201" class="definition"><strong>Definition 5.4  (Ïƒ-Algebra) </strong></span>The set <span class="math inline">\(\mathscr{F}\)</span> is called a <em><span class="math inline">\(\sigma\)</span>-algebra</em> if:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\Omega \in \mathscr{F}\)</span>;</li>
<li>
<span class="math inline">\(\forall A \subseteq \Omega\)</span>, <span class="math inline">\(A \in \mathscr{F}\)</span>, then <span class="math inline">\(A^C \in \mathscr{F}\)</span>; and</li>
<li>
<span class="math inline">\(\forall (A_n)_{n \in \mathbb{N}}\)</span>, <span class="math inline">\(A_n \in \mathscr{F}\)</span>, then <span class="math inline">\(\displaystyle \bigcup_{n = 1}^\infty A_n \in \mathscr{F}\)</span>.</li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-202" class="definition"><strong>Definition 5.5  (Probability Measure) </strong></span><span class="math inline">\(P : \mathscr{F}\to [0, 1]\)</span> is a <em>probability measure</em> if it satisfies the following three axioms:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\forall A \in \mathscr{F}: P(A) \geq 0\)</span>,</li>
<li>
<span class="math inline">\(P(\Omega) = 1\)</span>, and</li>
<li>
<span class="math inline">\(\displaystyle P\left( \bigcup_{n=1}^\infty A_n \right) = \sum_{n = 1}^\infty P\left(A_n\right)\)</span>,</li>
</ol>
<p>where <span class="math inline">\(A_n\)</span> are disjunct.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-203" class="remark"><em>Remark</em>. </span><br></p>
<ul>
<li>
<span class="math inline">\(\displaystyle P\left(A^C\right) = 1 - P(A)\)</span>,</li>
<li>
<span class="math inline">\(P(\varnothing) = 0\)</span>,</li>
<li>if <span class="math inline">\(A \subseteq B\)</span>, then <span class="math inline">\(P(A) \leq P(B)\)</span>, and</li>
<li>
<span class="math inline">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span>.</li>
</ul>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-204" class="proposition"><strong>Proposition 5.1  (De Morgan's Laws) </strong></span>Let <span class="math inline">\(A_1, \dots, A_n\)</span> be a set of events.
<span class="math display">\[
\left( \bigcup_{i=1}^n A_i \right)^C = \bigcap_{i=1}^n A_i^C \qquad \left( \bigcap_{i=1}^n A_i \right)^C = \bigcup_{i=1}^n A_i^C
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-205" class="theorem"><strong>Theorem 5.1  (Inclusion-Exclusion Principle) </strong></span>Let <span class="math inline">\(A_1, \dots, A_n\)</span> be a set of events, then
<span class="math display">\[
P\left( \bigcup_{i=1}^n A_i \right) = \sum_{k = 1}^n (-1)^{k-1} S_k,
\]</span>
where
<span class="math display">\[
S_k = \sum_{I \subseteq \{1, \dots, n\} \\ \quad \mid I\mid = k} P\left( \bigcap_{i \in I} A_i \right).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-206" class="definition"><strong>Definition 5.6  (Laplace Space) </strong></span>If <span class="math inline">\(\Omega = \{\omega_1, \dots, \omega_N\}\)</span> with <span class="math inline">\(\mid \Omega \mid = N\)</span> where all <span class="math inline">\(\omega_i\)</span> have the same probability <span class="math inline">\(p_i = \frac{1}{N}\)</span>, <span class="math inline">\(\Omega\)</span> is called <em>Laplace space</em> and <span class="math inline">\(P\)</span> has a discrete uniform distribution. For some event <span class="math inline">\(A\)</span>, we have:
<span class="math display">\[
P(A) = \frac{\lvert A\rvert}{\lvert\Omega\rvert}.
\]</span></p>
</div>
<blockquote>
<p>The discrete uniform distribution exists only if <span class="math inline">\(\Omega\)</span> is finite.</p>
</blockquote>
<div class="definition">
<p><span id="def:unlabeled-div-207" class="definition"><strong>Definition 5.7  (Conditional Probability) </strong></span>Given two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> with <span class="math inline">\(P(A) &gt; 0\)</span>, the probability of <span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span> is defined as follows:
<span class="math display">\[
P(B \mid A) := \frac{P(B \cap A)}{P(A)}.
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-208" class="theorem"><strong>Theorem 5.2  (Total Probability) </strong></span>Let <span class="math inline">\(A_1, \dots, A_n\)</span> be a set of disjunct events <span class="math inline">\(\forall i \neq j : A_i \cap A_j = \varnothing\)</span> where <span class="math inline">\(\displaystyle \bigcup_{i=1}^n A_i = \Omega\)</span>, then, for any event <span class="math inline">\(B \subseteq \Omega\)</span>
<span class="math display">\[
P(B) = \sum_{i = 1}^n P(B \mid A_i) P(A_i).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-209" class="definition"><strong>Definition 5.8  (Bayes' Rule) </strong></span>Let <span class="math inline">\(A_1, \dots, A_n\)</span> be the set of disjunct event <span class="math inline">\(\forall i \neq j : A_i \cap A_j = \varnothing\)</span> where <span class="math inline">\(\displaystyle \bigcup_{i=1}^n A_i = \Omega\)</span> with <span class="math inline">\(P(A_i) &gt; 0\)</span> for all <span class="math inline">\(i = 1, \dots, n\)</span>, then, for an event <span class="math inline">\(B \subseteq \Omega\)</span> with <span class="math inline">\(P(B) &gt; 0\)</span>, we have
<span class="math display">\[
P(A_k \mid B) = \frac{P(B \mid A_k) P(A_k)}{\displaystyle\sum_{i=1}^n P(B \mid A_i) P(A_i)}.
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-210" class="definition"><strong>Definition 5.9  (Independence) </strong></span>A set of events <span class="math inline">\(A_1, \dots, A_n\)</span> are <em>independent</em> if, for all <span class="math inline">\(m \in \mathbb{N}\)</span> with <span class="math inline">\(\{k_1, \dots, k_m\} \subseteq 1, \dots, n\)</span>, we have
<span class="math display">\[
P\left( \bigcap_{i=1}^m A_{k_i} \right) = \prod_{i=1}^m P\left(A_{k_i}\right).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-211" class="definition"><strong>Definition 5.10  (Factorial) </strong></span>The <em>factorial</em> function is defined by the product
<span class="math display">\[
n! = \prod_{i=1}^n i = n \cdot (n - 1)!
\]</span>
for integer <span class="math inline">\(n \geq 1\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-212" class="definition"><strong>Definition 5.11  (Gamma Function) </strong></span>Let <span class="math inline">\(z \in \mathbb{C}\)</span> with <span class="math inline">\(\Re(z) &gt; 0\)</span>, the <em>gamma function</em> is defined via the following convergent improper integral:
<span class="math display">\[
\Gamma(z) = \int_0^\infty t^{z - 1} e^{-t} dt.
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-213" class="remark"><em>Remark</em> (Gamma Function Properties). </span><br></p>
<ul>
<li>
<span class="math inline">\(\displaystyle \Gamma(1/2) = \sqrt{\pi}\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \Gamma(1) = \Gamma(2) = 1\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \Gamma(z) = (z - 1)\Gamma(z - 1)\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \Gamma(n) = (n - 1)!\)</span>, <span class="math inline">\(\forall n \in \mathbb{N}\)</span>.</li>
</ul>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-214" class="definition"><strong>Definition 5.12  (Permutation) </strong></span>Let <span class="math inline">\(n\)</span> be the number of total objects and <span class="math inline">\(k\)</span> be the number of objects we want to select. A <em>permutation</em> is an arrangement of elements where we care about ordering.</p>
<ol style="list-style-type: decimal">
<li>Repetition not allowed:
<span class="math display">\[
P_n(k) = \frac{n!}{(n - k)!}.
\]</span>
</li>
<li>Repetition allowed:
<span class="math display">\[
P_n(k) = n^k.
\]</span>
</li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-215" class="definition"><strong>Definition 5.13  (Combination) </strong></span>Let <span class="math inline">\(n\)</span> be the number of total objects and <span class="math inline">\(k\)</span> be the number of objects we want to select. A <em>combination</em> is an arrangement of elements where we do not care about ordering.</p>
<ol style="list-style-type: decimal">
<li>Repetition not allowed:
<span class="math display">\[
C_n(k) = \binom{n}{k} = \frac{P_n(k)}{k!} = \frac{n!}{k!(n - k)!}.
\]</span>
</li>
<li>Repetition allowed:
<span class="math display">\[
C_n(k) = \binom{n + k -1}{k}.
\]</span>
</li>
</ol>
</div>
<blockquote>
<p>Repetition is the same as replacement.</p>
</blockquote>
<div class="remark">
<p><span id="unlabeled-div-216" class="remark"><em>Remark</em> (Binomial Coefficient Properties). </span><br></p>
<ul>
<li>
<span class="math inline">\(0! = 1\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \binom{n}{0} = \binom{n}{n} = 1\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \binom{n}{1} = \binom{n}{n - 1} = n\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \binom{n}{k} = \binom{n}{n - k}\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \binom{n}{k} = \binom{n - 1}{k - 1} + \binom{n - 1}{k}\)</span>, and</li>
<li>
<span class="math inline">\(\displaystyle \sum_{k = 0}^n \binom{n}{k} = 2^n\)</span>.</li>
</ul>
</div>
<div class="remark">
<p><span id="unlabeled-div-217" class="remark"><em>Remark</em> (Sum Properties). </span>Let <span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, <span class="math inline">\(c \in \mathbb{R}\)</span> and <span class="math inline">\(k \neq 1\)</span>.</p>
<ul>
<li>
<span class="math inline">\(\displaystyle \sum_i x_i y_i \neq \sum_i x_i \sum y_i\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \sum_i x_i^k \neq \left( \sum_i x_i \right)^k\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \sum_{i=1}^n c = nc\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \sum_{i=1}^n cx_i = n\sum_{i=1}^n x_i\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \sum_i (x_i + y_i) = \sum_i x_i + \sum_i y_i\)</span>.</li>
</ul>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-218" class="theorem"><strong>Theorem 5.3  (Binomial Expansion) </strong></span><span class="math display">\[
(x+y)^n = \sum_{k=0}^n {n \choose k}x^{n-k}y^k = \sum_{k=0}^n {n \choose k}  x^k y^{n-k}
\]</span></p>
</div>
</div>
<div id="random-variables" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Random Variables<a class="anchor" aria-label="anchor" href="#random-variables"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:unlabeled-div-219" class="definition"><strong>Definition 5.14  (Random Variable) </strong></span>Let <span class="math inline">\((\Omega, \mathscr{F}, P)\)</span> be a probability space. A <em>random variable</em> on <span class="math inline">\(\Omega\)</span> is a function
<span class="math display">\[
X : \Omega \to \mathscr{W}(X) \subseteq \mathbb{R}.
\]</span>
If the image <span class="math inline">\(\mathscr{W}(X)\)</span> is countable, <span class="math inline">\(X\)</span> is called a <em>discrete random variable</em>, otherwise it is called a <em>continuous random variable</em>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-220" class="definition"><strong>Definition 5.15  (Probability Density Function / Probability Mass Function) </strong></span>The <em>probability density function</em> (PDF) <span class="math inline">\(f_X : \mathbb{R}\to \mathbb{R}\)</span> of a random variable <span class="math inline">\(X\)</span> is a function defined as follows:
<span class="math display">\[
f_X(x) := P(X = x) := P\left(\{\omega \mid X(\omega) = x\}\right).
\]</span>
With <span class="math inline">\(X\)</span> discrete, it is called <em>probability mass function</em>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-221" class="remark"><em>Remark</em>. </span><br></p>
<ul>
<li>If <span class="math inline">\(X\)</span> is a discrete random variable, then <span class="math inline">\(\displaystyle \sum_i f_X(u_i) = 1\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> is a continuous random variable, then <span class="math inline">\(\displaystyle \int_{-\infty}^{\infty} f_X(t)dt = 1\)</span>.</li>
</ul>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-222" class="definition"><strong>Definition 5.16  (Cumulative Distribution) </strong></span>The <em>cumulative distribution function</em> (CDF) <span class="math inline">\(F_X : \mathbb{R}\to [0,1]\)</span> of a random variable <span class="math inline">\(X\)</span> is a function defined as follows:
<span class="math display">\[
F_X(x) := P(X \leq x) := P\left(\{\omega \mid X(\omega) \leq x\}\right).
\]</span>
If the PDF is given, the CDF can be expressed with:
<span class="math display">\[
F_X(x) =
\begin{cases}
\displaystyle \sum_{x_i \leq x} f_X(x_i), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^x f_X(t)dt,  &amp; \text{continuous}
\end{cases}
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-223" class="remark"><em>Remark</em> (Cumulative Distribution Properties). </span><br></p>
<ul>
<li>If <span class="math inline">\(t \leq s\)</span>, then <span class="math inline">\(F_X(t) \leq F_X(s)\)</span> (<em>monotonicity</em>).</li>
<li>If <span class="math inline">\(t &gt; s\)</span>, then <span class="math inline">\(\displaystyle \lim_{t \to s} F_X(t) = F_X(s)\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \lim_{t \to -\infty} F_X(t) = 0\)</span> and <span class="math inline">\(\displaystyle \lim_{t \to \infty} F_X(t) = 1\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle P(a \leq X \leq b) = F_X(b) - F_X(a) = \int_{a}^{b} f_X(t)dt\)</span>.</li>
<li>
<span class="math inline">\(P(X &gt; t) = 1 - P(X \leq t) = 1 - F_X(t)\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \frac{d}{dx}F_X(x) = f_X(x)\)</span>.</li>
</ul>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-224" class="definition"><strong>Definition 5.17  (Expected Value) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable. The <em>expected value</em> is defined as follows:
<span class="math display">\[
\mathop{\mathrm{\mathbb{E}}}[X] :=
\begin{cases}
\displaystyle \sum_{x_k \in \mathscr{W}(X)} x_k \cdot f_X(x_k), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^\infty x \cdot f_X(x)dx,  &amp; \text{continuous}
\end{cases}
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-225" class="remark"><em>Remark</em> (Expected Value Properties). </span><br></p>
<ul>
<li>
<span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[X] \leq \mathop{\mathrm{\mathbb{E}}}[Y]\)</span> if <span class="math inline">\(\forall \omega : X(\omega) \leq Y(\omega)\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \mathop{\mathrm{\mathbb{E}}}\left[ \sum_{i=0}^n a_i X_i \right] = \sum_{i=0}^n a_i \mathop{\mathrm{\mathbb{E}}}\left[ X_i \right]\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \mathop{\mathrm{\mathbb{E}}}[X] = \sum_{j=1}^\infty P[X \geq j]\)</span> if <span class="math inline">\(\mathscr{W}(X) \subseteq \mathbb{N}_0\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \mathop{\mathrm{\mathbb{E}}}\left[ \sum_{i=0}^\infty X_i \right] \neq \sum_{i=0}^\infty \mathop{\mathrm{\mathbb{E}}}\left[ X_i \right]\)</span>,</li>
<li>
<span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[\mathop{\mathrm{\mathbb{E}}}[X]] = \mathop{\mathrm{\mathbb{E}}}[X]\)</span>,</li>
<li>
<span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[XY]^2 \leq E[X^2]E[Y^2]\)</span>, and</li>
<li>
<span class="math inline">\(\displaystyle \mathop{\mathrm{\mathbb{E}}}\left[ \prod_{i=0}^n X_i \right] = \prod_{i=0}^n \mathop{\mathrm{\mathbb{E}}}[X_i]\)</span> for independent <span class="math inline">\(X_1, \dots, X_n\)</span>.</li>
</ul>
</div>
<blockquote>
<p>The expected value is a linear operator.</p>
</blockquote>
<div class="definition">
<p><span id="def:unlabeled-div-226" class="definition"><strong>Definition 5.18  (Raw Moment / Central Moment) </strong></span>Let <span class="math inline">\(n \in \mathbb{N}\)</span>. The <em><span class="math inline">\(n\)</span>th (raw) moment</em> is defined as follows:
<span class="math display">\[
\mu_n' = \mathop{\mathrm{\mathbb{E}}}\left[X^n\right].
\]</span>
The <em><span class="math inline">\(n\)</span>th central moment</em> is defined as follows:
<span class="math display">\[
\mu_n = \mathop{\mathrm{\mathbb{E}}}\left[\left( X - \mathop{\mathrm{\mathbb{E}}}[X] \right)^n \right].
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-227" class="definition"><strong>Definition 5.19  (Expected Value of Functions) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(Y = g(X)\)</span> with <span class="math inline">\(g : \mathbb{R}\to \mathbb{R}\)</span>, then
<span class="math display">\[
\mathop{\mathrm{\mathbb{E}}}[Y] :=
\begin{cases}
\displaystyle \sum_{x_k \in \mathscr{W}(X)} g(x_k) \cdot f_X(x_k), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^\infty g(x) \cdot f_X(x)dx,  &amp; \text{continuous}
\end{cases}
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-228" class="definition"><strong>Definition 5.20  (Moment-Generating Function) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable. The <em>moment-generating function</em> of <span class="math inline">\(X\)</span> is defined as follows:
<span class="math display">\[
M_X(t) := \mathop{\mathrm{\mathbb{E}}}\left[ e^{tX} \right].
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-229" class="definition"><strong>Definition 5.21  (Characteristic Function) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable. The <em>characteristic function</em> of <span class="math inline">\(X\)</span> is defined as follows:
<span class="math display">\[
\varphi_X(t) := \mathop{\mathrm{\mathbb{E}}}\left[ e^{itX} \right]
\]</span>
where <span class="math inline">\(i = \sqrt{-1} \in \mathbb{C}\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-230" class="definition"><strong>Definition 5.22  (Variance) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[X^2] &lt; \infty\)</span>. The <em>variance</em> of <span class="math inline">\(X\)</span> is defined as follows:
<span class="math display">\[
\text{Var}[X] := \mathop{\mathrm{\mathbb{E}}}\left[ (X - \mathop{\mathrm{\mathbb{E}}}[X])^2 \right].
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-231" class="remark"><em>Remark</em> (Variance Properties). </span><br></p>
<ul>
<li>
<span class="math inline">\(0 \leq \text{Var}[X] \leq \mathop{\mathrm{\mathbb{E}}}[X^2]\)</span>,</li>
<li>
<span class="math inline">\(\text{Var}[X] = \mathop{\mathrm{\mathbb{E}}}[X^2] - \mathop{\mathrm{\mathbb{E}}}^2[X]\)</span>,</li>
<li>
<span class="math inline">\(\text{Var}[aX + b] = a^2\text{Var}[X]\)</span>,</li>
<li>
<span class="math inline">\(\text{Var}[X] = \text{Cov}(X,X)\)</span>,</li>
<li>
<span class="math inline">\(\displaystyle \text{Var}\left[ \sum_{i=1}^n a_i X_i \right] = \sum_{i = 1}^n a_i^2 \text{Var}[X_i] + 2 \sum_{1 \leq i &lt; j \leq n} a_i a_j \text{Cov}(X_i, X_j)\)</span>, and</li>
<li>
<span class="math inline">\(\displaystyle \text{Var}\left[ \sum_{i=1}^n X_i \right] = \sum_{i = 1}^n \text{Var}[X_i]\)</span> if <span class="math inline">\(\text{Cov}(X_i, X_j) = 0\)</span>, <span class="math inline">\(\forall i \neq j\)</span>.</li>
</ul>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-232" class="definition"><strong>Definition 5.23  (Standard Deviation) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[X^2] &lt; \infty\)</span>. The <em>standard deviation</em> of <span class="math inline">\(X\)</span> is defined as follows:
<span class="math display">\[
\sigma(X) = \text{sd}(X) := \sqrt{\text{Var}[X]}.
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-233" class="definition"><strong>Definition 5.24  (Covariance) </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables with finite expected value. The <em>covariance</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as follows:
<span class="math display">\[
\begin{align*}
\text{Cov}(X, Y) :&amp;=\ \mathop{\mathrm{\mathbb{E}}}\left[ (X - \mathop{\mathrm{\mathbb{E}}}[X]) (Y - \mathop{\mathrm{\mathbb{E}}}[Y]) \right] \\
            &amp;=\ \mathop{\mathrm{\mathbb{E}}}[XY] - \mathop{\mathrm{\mathbb{E}}}[X]\mathop{\mathrm{\mathbb{E}}}[Y]
\end{align*}
\]</span></p>
</div>
<blockquote>
<p>The covariance is a measure of correlation between two random variables. <span class="math inline">\(\text{Cov}(X,Y)&gt;0\)</span> if <span class="math inline">\(Y\)</span> tends to increase as <span class="math inline">\(X\)</span> increases. <span class="math inline">\(\text{Cov}(X,Y)&lt;0\)</span> if <span class="math inline">\(Y\)</span> tends to decrease as <span class="math inline">\(X\)</span> increases. If <span class="math inline">\(\text{Cov}(X,Y)=0\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated.</p>
</blockquote>
<div class="remark">
<p><span id="unlabeled-div-234" class="remark"><em>Remark</em> (Covariance Properties). </span><br></p>
<ul>
<li>
<span class="math inline">\(\text{Cov}(aX,bY) = ab\text{Cov}(X,Y)\)</span>,</li>
<li>
<span class="math inline">\(\text{Cov}(X + a, Y + b) = \text{Cov}(X,Y)\)</span>, and</li>
<li>
<span class="math inline">\(\text{Cov}(aX_1 + bX_2, cY_1 + dY_2)\)</span>
<span class="math inline">\(= ac\text{Cov}(X_1,Y_1) + ad\text{Cov}(X_1,Y_2) + bc\text{Cov}(X_2,Y_1) + bd\text{Cov}(X_2,Y_2)\)</span>.</li>
</ul>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-235" class="definition"><strong>Definition 5.25  (Correlation) </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables with finite expected value. The <em>correlation</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as follows:
<span class="math display">\[
\text{Corr}(X,Y) := \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}[X] \cdot \text{Var}[Y]}}.
\]</span></p>
</div>
<blockquote>
<p>Correlation is the same as covariance but normalized with values between -1 and 1.</p>
</blockquote>
<div class="definition">
<p><span id="def:unlabeled-div-236" class="definition"><strong>Definition 5.26  (Coefficient of Variation) </strong></span>The <em>coefficient of variation</em> is defined as the ratio of the standard deviation to the mean.
<span class="math display">\[
\text{i.e.}\quad c_V = \frac{\sigma}{\mu}.
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-237" class="definition"><strong>Definition 5.27  (Indicator Function) </strong></span>The <em>indicator function</em> <span class="math inline">\(\mathbb{1}_A : \Omega \to \{0, 1\}\)</span> for a set (event) <span class="math inline">\(A\)</span> is defined as follows:
<span class="math display">\[
\mathbb{1}_A (\omega) :=
\begin{cases}
1, &amp; \omega \in A \\
0, &amp; \omega \in A^C
\end{cases}
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-238" class="definition"><strong>Definition 5.28  (Survival Function) </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable with cumulative distribution function <span class="math inline">\(F(x)\)</span> on the interval <span class="math inline">\([0, \infty)\)</span>. Its <em>survival function</em> or <em>reliability function</em> is defined as follows:
<span class="math display">\[
S(x) = P[X &gt; x] = \int_x^\infty f(t)dt = 1 - F(x).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-239" class="definition"><strong>Definition 5.29  (Memorylessness) </strong></span>Suppose <span class="math inline">\(X\)</span> is a non-negative random variable. The probability distribution of <span class="math inline">\(X\)</span> is <em>memoryless</em> if for any <span class="math inline">\(s, t \geq 0\)</span>, we have
<span class="math display">\[
P[X &gt; s + t \mid X &gt; t] = P[X &gt; s].
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-240" class="theorem"><strong>Theorem 5.4  (Markov's Inequality) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(g : \mathscr{W}(X) \to [0, \infty)\)</span> be an increasing function, then, for all <span class="math inline">\(c\)</span> with <span class="math inline">\(g(c) &gt; 0\)</span>, we have
<span class="math display">\[
P[X \geq c] \leq \frac{\mathop{\mathrm{\mathbb{E}}}[g(X)]}{g(x)}.
\]</span></p>
<blockquote>
<p>For practical uses usually <span class="math inline">\(g(x) = x\)</span>.</p>
</blockquote>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-241" class="theorem"><strong>Theorem 5.5  (Chebyshev's Inequality) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\text{Var}[X] &lt; \infty\)</span>, then, if <span class="math inline">\(b &gt; 0\)</span>,
<span class="math display">\[
P\left[ \lvert X - \mathop{\mathrm{\mathbb{E}}}[X] \rvert \geq b \right] \leq \frac{\text{Var}[X]}{b^2}.
\]</span></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="29%">
<col width="35%">
<col width="35%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Lower Bound</th>
<th>Upper Bound</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<span class="math inline">\(b\)</span> units</td>
<td><span class="math inline">\(\displaystyle P\left[ \left\lvert X - \mathop{\mathrm{\mathbb{E}}}[X] \right\rvert &lt; b \right] \geq 1 - \frac{\text{Var}[X]}{b^2}\)</span></td>
<td><span class="math inline">\(\displaystyle P\left[ \left\lvert X - \mathop{\mathrm{\mathbb{E}}}[X] \right\rvert \geq b \right] \leq \frac{\text{Var}[X]}{b^2}\)</span></td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(r\)</span> standard deviations</td>
<td><span class="math inline">\(\displaystyle P\left[ \left\lvert X - \mathop{\mathrm{\mathbb{E}}}[X] \right\rvert &lt; r\  \text{sd}(X) \right] \geq 1 - \frac{1}{r^2} \quad\)</span></td>
<td><span class="math inline">\(\displaystyle P\left[ \left\lvert X - \mathop{\mathrm{\mathbb{E}}}[X] \right\rvert \geq r\ \text{sd}(X) \right] \leq \frac{1}{r^2}\)</span></td>
</tr>
</tbody>
</table></div>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-242" class="theorem"><strong>Theorem 5.6  (Jensen's Inequality) </strong></span>If <span class="math inline">\(X\)</span> is a random variable and <span class="math inline">\(\varphi\)</span> is a convex function, then
<span class="math display">\[
\varphi\left(\mathop{\mathrm{\mathbb{E}}}[X]\right) \leq \mathop{\mathrm{\mathbb{E}}}\left[\varphi(X)\right].
\]</span></p>
</div>
</div>
<div id="multivariate-distributions" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Multivariate Distributions<a class="anchor" aria-label="anchor" href="#multivariate-distributions"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:unlabeled-div-243" class="definition"><strong>Definition 5.30  (Joint Probability Density Function) </strong></span>The <em>joint probability density function</em> <span class="math inline">\(f_X : \mathbb{R}^n \to [0, 1]\)</span> with <span class="math inline">\(X = (X_1, \dots, X_n)\)</span> is a function defined as follows:
<span class="math display">\[
f_X(x_1, \dots, x_n) := P[X_1 = x_1, \dots, X_n = x_n].
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-244" class="definition"><strong>Definition 5.31  (Joint Cumulative Distribution Function) </strong></span>The <em>joint cumulative distribution function</em> <span class="math inline">\(F_X : \mathbb{R}^n \to [0, 1]\)</span> with <span class="math inline">\(X = (X_1, \dots, X_n)\)</span> is a function defined as follows:
<span class="math display">\[
f_X(x_1, \dots, x_n) := P[X_1 \leq x_1, \dots, X_n \leq x_n].
\]</span>
If the joint PDF is given, it can be expressed with:
<span class="math display">\[
F_X(x) =
\begin{cases}
\displaystyle \sum_{t_1 \leq x_1} \cdots \sum_{t_n \leq x_n} f_X(t), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_n} f_X(t)dt, &amp; \text{continuous}
\end{cases}
\]</span>
where <span class="math inline">\(t = (t_1, \dots, t_n)\)</span> and <span class="math inline">\(x = (x_1, \dots, x_n)\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-245" class="remark"><em>Remark</em>. </span><span class="math display">\[
\frac{\partial F_X(x_1, \dots, x_n)}{\partial x_1, \dots, x_n} = f_X(x_1, \dots, x_n)
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-246" class="definition"><strong>Definition 5.32  (Marginal Probability Density Function) </strong></span>The <em>marginal probability density function</em> <span class="math inline">\(f_{X_i} : \mathbb{R}\to [0,1]\)</span> of <span class="math inline">\(X_i\)</span> given a joint PDF <span class="math inline">\(f_X(x_1, \dots, x_n)\)</span> is defined as follows:
<span class="math display">\[
f_{X_i}(t_i) =
\begin{cases}
\displaystyle \sum_{t_1} \cdots \sum_{t_{i-1}} \sum_{t_{i+1}} \cdots \sum_{t_n} f_X(t), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f_X(t)d\tilde{t}, &amp; \text{continuous}
\end{cases}
\]</span>
where <span class="math inline">\(\tilde{t} = (t_1, \dots, t_{i-1}, t_{i+1}, \dots, t_n)\)</span>, and in the discrete case <span class="math inline">\(t_k \in \mathscr{W}(X_k)\)</span>.</p>
</div>
<blockquote>
<p>The idea of the marginal probability is to ignore all other random variables and consider only the one weâ€™re interested in.</p>
</blockquote>
<div class="definition">
<p><span id="def:unlabeled-div-247" class="definition"><strong>Definition 5.33  (Marginal Cumulative Distribution Function) </strong></span>The <em>marginal cumulative distribution function</em> <span class="math inline">\(F_{X_i} : \mathbb{R}\to [0,1]\)</span> of <span class="math inline">\(X_i\)</span> given a joint CDF <span class="math inline">\(F_X(x_1, \dots, x_n)\)</span> is defined as follows:
<span class="math display">\[
F_{X_i}(x_i) := \lim_{x_{j\neq i} \to \infty} F_X(x_1, \dots, x_n).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-248" class="definition"><strong>Definition 5.34  (Conditional Distribution) </strong></span>The <em>conditional distribution</em> <span class="math inline">\(f_{X \mid Y} : \mathbb{R}\to [0,1]\)</span> is defined as follows:
<span class="math display">\[
\begin{align*}
f_{X\mid Y} (x \mid y) :&amp;=\ P[X = x \mid Y = y] \\
                        &amp;=\ \frac{P[X = x, Y = y]}{P[Y = y]} \\
                        &amp;=\ \frac{\text{Joint PDF}}{\text{Marginal PDF}}
\end{align*}
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-249" class="definition"><strong>Definition 5.35  (Independence) </strong></span>The random variables <span class="math inline">\(X_1, \dots, X_n\)</span> are <em>independent</em> if
<span class="math display">\[
F_{X_1, \dots, X_n} (x_1, \dots, x_n) = \prod_{i=1}^n F_{X_i}(x_i).
\]</span>
Similarly, if their PDF is absolutely continuous, they are <em>independent</em> if
<span class="math display">\[
f_{X_1, \dots, X_n} (x_1, \dots, x_n) = \prod_{i=1}^n f_{X_i}(x_i).
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-250" class="theorem"><strong>Theorem 5.7  (Function Independence) </strong></span>If the random variables <span class="math inline">\(X_1, \dots, X_n\)</span> are independent and <span class="math inline">\(f_i : \mathbb{R}\to \mathbb{R}\)</span> is a function with <span class="math inline">\(Y_i := f_i(X_i)\)</span>, then also <span class="math inline">\(Y_1, \dots, Y_n\)</span> are independent.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-251" class="theorem"><strong>Theorem 5.8  </strong></span>The random variables <span class="math inline">\(X_1, \dots, X_n\)</span> are independent if and only if, <span class="math inline">\(\forall B_i \subseteq \mathscr{W}(X_i)\)</span>, we have
<span class="math display">\[
P[X_1 \in B_1, \dots, X_n \in B_n] = \prod_{i=1}^n P[X_i \in B_i].
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-252" class="definition"><strong>Definition 5.36  (Joint Expected Value) </strong></span>The <em>joint expected value</em> of a random variable <span class="math inline">\(Y = g(X_1, \dots, X_n) = g(X)\)</span> is defined as follows:
<span class="math display">\[
\mathop{\mathrm{\mathbb{E}}}[Y] =
\begin{cases}
\displaystyle \sum_{t_1} \cdots \sum_{t_n} g(t) f_X(t), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} g(t)f_X(t)dt, &amp; \text{continuous}
\end{cases}
\]</span>
where <span class="math inline">\(t = (t_1, \dots, t_n)\)</span>, and in the discrete case <span class="math inline">\(t_k \in \mathscr{W}(X_k)\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-253" class="definition"><strong>Definition 5.37  (Conditional Expected Value) </strong></span>The <em>conditional expected value</em> of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as follows:
<span class="math display">\[
\mathop{\mathrm{\mathbb{E}}}[X\mid Y] :=
\begin{cases}
\displaystyle \sum_{x \in \mathbb{R}} x \cdot f_{X\mid Y}(x\mid y), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^\infty x \cdot f_{X\mid Y}(x\mid y)dx,  &amp; \text{continuous}
\end{cases}
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-254" class="remark"><em>Remark</em>. </span><br></p>
<ul>
<li>
<span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[X] = \mathop{\mathrm{\mathbb{E}}}[\mathop{\mathrm{\mathbb{E}}}[X\mid Y]]\)</span>,</li>
<li>
<span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[X\mid Y] = \mathop{\mathrm{\mathbb{E}}}[X]\)</span> if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, and</li>
<li>
<span class="math inline">\(\text{Var}[X] = \mathop{\mathrm{\mathbb{E}}}\left[\text{Var}[X \mid Y]\right] + \text{Var}\left[\mathop{\mathrm{\mathbb{E}}}[X \mid Y]\right]\)</span>.</li>
</ul>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-255" class="definition"><strong>Definition 5.38  </strong></span>Let <span class="math inline">\(Y = g(X_1, \dots, X_n) = g(X)\)</span>.
<span class="math display">\[
P[Y \in C] = \int_{A_C} f_X(t)dt
\]</span>
where <span class="math inline">\(A_C = \{ x = (x_1, \dots, x_n) \in \mathbb{R}^n : g(x) \in C \}\)</span> and <span class="math inline">\(t = (t_1, \dots, t_n)\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-256" class="theorem"><strong>Theorem 5.9  (Transformation) </strong></span>Let <span class="math inline">\(F\)</span> be a continuous and strictly increasing CDF and let <span class="math inline">\(X \sim \mathscr{U}(0, 1)\)</span>.
<span class="math display">\[
Y = F^{-1}(X) \implies F_Y = F.
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-257" class="remark"><em>Remark</em> (Monte Carlo Integration). </span>Let <span class="math inline">\(\displaystyle I = \int_a^b g(x)dx\)</span> be the integral of a function that is hard to evaluate, then
<span class="math display">\[
\begin{align*}
I &amp;=\ \int_a^b g(x)dx \\
  &amp;=\ (b - a) \int_a^b g(x) \frac{1}{b - a} dx \\
  &amp;=\ (b - a) \int_{-\infty}^{\infty} g(x) f_{\mathscr{U}}(x) dx \\
  &amp;=\ (b - a) \cdot \mathop{\mathrm{\mathbb{E}}}[g(\mathscr{U})]
\end{align*}
\]</span>
where <span class="math inline">\(\mathscr{U}(a,b)\)</span> is uniformly distributed. Then, by the <strong>Law of Large Numbers</strong>, we know that we can approximate <span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[g(\mathscr{U})]\)</span> by randomly sampling <span class="math inline">\(u_1, u_2, \dots\)</span> from <span class="math inline">\(\mathscr{U}(a,b)\)</span>.
<span class="math display">\[
\frac{b - a}{n} \sum_{i=1}^n g(u_i) \underset{n\to\infty}{\longrightarrow} (b - a) \cdot \mathop{\mathrm{\mathbb{E}}}[g(\mathscr{U})].
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-258" class="remark"><em>Remark</em> (Transformation). </span>If we have a random variable <span class="math inline">\(X\)</span> with known CDF (strictly increasing) and <span class="math inline">\(Y = g(X)\)</span>. To evaluate <span class="math inline">\(F_Y\)</span> and <span class="math inline">\(f_Y\)</span>, we proceed as follows:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\displaystyle F_Y(t) = P[g(X) \leq t] = \int_{A_g} f_X(s)ds\)</span></li>
<li><span class="math inline">\(\displaystyle f_Y(t) = \frac{dF_Y}{dt}(t)\)</span></li>
</ol>
<p>where <span class="math inline">\(A_g = \{ s \in \mathbb{R}: g(s) \leq t \}\)</span>.</p>
</div>
<div class="remark">
<span id="unlabeled-div-259" class="remark"><em>Remark</em> (Sum). </span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be independent random variables, then the sum <span class="math inline">\(Z = X_1 + \cdots + X_n\)</span> has a PDF <span class="math inline">\(f_Z(z)\)</span> evaluated with a convolution between all PDFs
<span class="math display">\[
f_Z(z) = (f_{X_1}(x_1) * \cdots * f_{X_n}(x_n))(z).
\]</span>
In the special case where <span class="math inline">\(Z = X + Y\)</span>, we have
$$
f_Z(z) =
<span class="math display">\[\begin{cases}
\displaystyle \sum_{x_k \in \mathscr{W}(X)} f_X(x_k) f_Y(z - x_k), &amp; \text{discrete} \\
\displaystyle \int_{-\infty}^\infty f_X(t) f_Y(z - t)dt, &amp; \text{continuous}
\end{cases}\]</span>
<p>$$</p>
</div>
<blockquote>
<p>Often it is much easier to use properties of the random variables to find the sum instead of evaluating the convolution.</p>
</blockquote>
<div class="remark">
<p><span id="unlabeled-div-260" class="remark"><em>Remark</em> (Product). </span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables. To evaluate the PDF and the CDF of <span class="math inline">\(Z = XY\)</span>, we proceed as follows:
<span class="math display">\[
\begin{align}
F_Z(z) &amp;=\ P[XY \leq z] \\
       &amp;=\ P\left[ X \geq \frac{z}{Y}, Y &lt; 0 \right] \\
       &amp;+\ P\left[ X \leq \frac{z}{Y}, Y &gt; 0 \right] \\
       &amp;=\ \int_{-\infty}^0 \left[ \int_{\frac{z}{y}}^\infty f_X(x)dx \right] f_Y(y)dy \\
       &amp;+\ \int_0^\infty \left[ \int_{-\infty}^{\frac{z}{y}} f_X(x)dx \right] f_Y(y)dy
\end{align}
\]</span>
where the PDF is
<span class="math display">\[
f_Z(z) = \frac{dF_Z}{dz}(z) = \int_{-\infty}^\infty f_Y(y)f_X\left(\frac{z}{y}\right) \frac{1}{\lvert y \rvert} dy.
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-261" class="remark"><em>Remark</em> (Quotient). </span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables. To evaluate the PDF and the CDF of <span class="math inline">\(\displaystyle Z = \frac{X}{Y}\)</span>, we proceed as follows:
<span class="math display">\[
\begin{align}
F_Z(z) &amp;=\ P\left[\frac{X}{Y} \leq z\right] \\
       &amp;=\ P\left[ X \geq zY, Y &lt; 0 \right] \\
       &amp;+\ P\left[ X \leq zY, Y &gt; 0 \right] \\
       &amp;=\ \int_{-\infty}^0 \left[ \int_{yz}^\infty f_X(x)dx \right] f_Y(y)dy \\
       &amp;+\ \int_0^\infty \left[ \int_{-\infty}^{yz} f_X(x)dx \right] f_Y(y)dy
\end{align}
\]</span>
where the PDF is
<span class="math display">\[
f_Z(z) = \frac{dF_Z}{dz}(z) = \int_{-\infty}^\infty \lvert y\rvert\ f_X\left(yz\right) f_Y(y) dy.
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-262" class="definition"><strong>Definition 5.39  (Covariance Matrix / Correlation Matrix) </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be random variables. Let <span class="math inline">\(\sigma_{ij} = \text{Cov}(X_i, X_j)\)</span> and <span class="math inline">\(\rho_{ij} = \text{Corr}(X_i, X_j)\)</span> for every <span class="math inline">\(i, j = 1, 2, \dots, n\)</span>. The <em>covariance</em> and <em>correlation matrices</em> as defined respectively as follows:
<span class="math display">\[
\Sigma =
\begin{bmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1n} \\
\sigma_{21} &amp; \sigma_{22} &amp; \cdots &amp; \sigma_{2n} \\
\vdots      &amp; \vdots      &amp; \ddots &amp; \vdots \\
\sigma_{n1} &amp; \sigma_{n2} &amp; \cdots &amp; \sigma_{nn}
\end{bmatrix}_{n\times n} \\ \\
P =
\begin{bmatrix}
1         &amp; \rho_{12} &amp; \cdots &amp; \rho_{1n} \\
\rho_{21} &amp; 1         &amp; \cdots &amp; \rho_{2n} \\
\vdots    &amp; \vdots    &amp; \ddots &amp; \vdots \\
\rho_{n1} &amp; \rho_{n2} &amp; \cdots &amp; 1
\end{bmatrix}_{n\times n}
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-263" class="theorem"><strong>Theorem 5.10  </strong></span><span class="math inline">\(\lvert\rho_{X,Y}\rvert = 1\)</span> if and only if there exist <span class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span> such that <span class="math inline">\(Y = \alpha + \beta X\)</span>.</p>
</div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="economics-4.html"><span class="header-section-number">4</span> Economics 4</a></div>
<div class="next"><a href="statistical-inference.html"><span class="header-section-number">6</span> Statistical Inference</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#probability"><span class="header-section-number">5</span> Probability</a></li>
<li><a class="nav-link" href="#basics-and-combinatorics"><span class="header-section-number">5.1</span> Basics and Combinatorics</a></li>
<li><a class="nav-link" href="#random-variables"><span class="header-section-number">5.2</span> Random Variables</a></li>
<li><a class="nav-link" href="#multivariate-distributions"><span class="header-section-number">5.3</span> Multivariate Distributions</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/celj/itam-cheat-sheets/blob/master/04-probability.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/celj/itam-cheat-sheets/edit/master/04-probability.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>My cheat sheets</strong>" was written by Carlos Lezama. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
